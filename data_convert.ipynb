{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import pypdf\n",
    "\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader, PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 0\tFile: 311.Dialog2Flow Pre-training Soft-Contrastive Action-Driven Sentence Embeddings for Automatic Dialog Flow Extraction.pdf\n",
      "Idx: 1\tFile: 1180.Generative Subgraph Retrieval for Knowledge Graph–Grounded Dialog Generation.pdf\n",
      "Idx: 2\tFile: 481.CliMedBench A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models in Clinical Scenarios.pdf\n",
      "Idx: 3\tFile: 462.Data Advisor Dynamic Data Curation for Safety Alignment of Large Language Models.pdf\n",
      "Idx: 4\tFile: 1184.GottBERT a pure German Language Model.pdf\n",
      "Idx: 5\tFile: 317.Predicting Rewards Alongside Tokens Non-disruptive Parameter Insertion for Efficient Inference Intervention in Large Language Model.pdf\n",
      "Idx: 6\tFile: 242.StyleRemix Interpretable Authorship Obfuscation via Distillation and Perturbation of Style Elements.pdf\n",
      "Idx: 7\tFile: 665.Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training.pdf\n",
      "Idx: 8\tFile: 421.Unveiling Factual Recall Behaviors of Large Language Models through Knowledge Neurons.pdf\n",
      "Idx: 9\tFile: 82.A New Pipeline for Knowledge Graph Reasoning Enhanced by Large Language Models Without Fine-Tuning.pdf\n",
      "Idx: 10\tFile: 295.Beyond Reference Evaluating High Quality Translations Better than Human References.pdf\n",
      "Idx: 11\tFile: 1052.Fool Me Once Contrasting Textual and Visual Explanations in a Clinical Decision-Support Setting.pdf\n",
      "Idx: 12\tFile: 833.MoCoKGC Momentum Contrast Entity Encoding for Knowledge Graph Completion.pdf\n",
      "Idx: 13\tFile: 171.DGLF A Dual Graph-based Learning Framework for Multi-modal Sarcasm Detection.pdf\n",
      "Idx: 14\tFile: 352.Distilling Knowledge from Text-to-Image Generative Models Improves Visio-Linguistic Reasoning in CLIP.pdf\n",
      "Idx: 15\tFile: 1036.Who is better at math, Jenny or Jingzhen Uncovering Stereotypes in Large Language Models.pdf\n",
      "Idx: 16\tFile: 1027.CasiMedicos-Arg A Medical Question Answering Dataset Annotated with Explanatory Argumentative Structures.pdf\n",
      "Idx: 17\tFile: 33.On Sensitivity of Learning with Limited Labelled Data to the Effects of Randomness Impact of Interactions and Systematic Choices.pdf\n",
      "Idx: 18\tFile: 1252.ZEBRA Zero-Shot Example-Based Retrieval Augmentation for Commonsense Question Answering.pdf\n",
      "Idx: 19\tFile: 713.DKEC Domain Knowledge Enhanced Multi-Label Classification for Diagnosis Prediction.pdf\n",
      "Idx: 20\tFile: 605.Breaking the Curse of Multilinguality with Cross-lingual Expert Language Models.pdf\n",
      "Idx: 21\tFile: 110.An Effective Deployment of Diffusion LM for Data Augmentation in Low-Resource Sentiment Classification.pdf\n",
      "Idx: 22\tFile: 882.Global Reward to Local Rewards Multimodal-Guided Decomposition for Improving Dialogue Agents.pdf\n",
      "Idx: 23\tFile: 878.BaitAttack Alleviating Intention Shift in Jailbreak Attacks via Adaptive Bait Crafting.pdf\n",
      "Idx: 24\tFile: 335.Pretraining Language Models Using Translationese.pdf\n",
      "Idx: 25\tFile: 538.Attribute Diversity Determines the Systematicity Gap in VQA.pdf\n",
      "Idx: 26\tFile: 1050.Are Data Augmentation Methods in Named Entity Recognition Applicable for Uncertainty Estimation.pdf\n",
      "Idx: 27\tFile: 418.Position Engineering Boosting Large Language Models through Positional Information Manipulation.pdf\n",
      "Idx: 28\tFile: 804.Modeling User Preferences with Automatic Metrics Creating a High-Quality Preference Dataset for Machine Translation.pdf\n",
      "Idx: 29\tFile: 532.Enhancing Systematic Decompositional Natural Language Inference Using Informal Logic.pdf\n",
      "Idx: 30\tFile: 970.ClimRetrieve A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures.pdf\n",
      "Idx: 31\tFile: 331.Methods of Automatic Matrix Language Determination for Code-Switched Speech.pdf\n",
      "Idx: 32\tFile: 1228.The Death and Life of Great Prompts Analyzing the Evolution of LLM Prompts from the Structural Perspective.pdf\n",
      "Idx: 33\tFile: 492.FIRST Faster Improved Listwise Reranking with Single Token Decoding.pdf\n",
      "Idx: 34\tFile: 100.MatchTime Towards Automatic Soccer Game Commentary Generation.pdf\n",
      "Idx: 35\tFile: 807.SecCoder Towards Generalizable and Robust Secure Code Generation.pdf\n",
      "Idx: 36\tFile: 35.A Study of Nationality Bias in Names and Perplexity using Off-the-Shelf Affect-related Tweet Classifiers.pdf\n",
      "Idx: 37\tFile: 889.A Comparison of Language Modeling and Translation as Multilingual Pretraining Objectives.pdf\n",
      "Idx: 38\tFile: 881.SciAgent Tool-augmented Language Models for Scientific Reasoning.pdf\n",
      "Idx: 39\tFile: 877.Revisiting Supervised Contrastive Learning for Microblog Classification.pdf\n",
      "Idx: 40\tFile: 1071.BLSP-Emo Towards Empathetic Large Speech-Language Models.pdf\n",
      "Idx: 41\tFile: 964.Varying Sentence Representations via Condition-Specified Routers.pdf\n",
      "Idx: 42\tFile: 688.Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently.pdf\n",
      "Idx: 43\tFile: 960.MemeCLIP Leveraging CLIP Representations for Multimodal Meme Classification.pdf\n",
      "Idx: 44\tFile: 1017.Investigating and Mitigating Object Hallucinations in Pretrained Vision-Language (CLIP) Models.pdf\n",
      "Idx: 45\tFile: 1025.Language models and brains align due to more than next-word prediction and word-level information.pdf\n",
      "Idx: 46\tFile: 863.Effective Synthetic Data and Test-Time Adaptation for OCR Correction.pdf\n",
      "Idx: 47\tFile: 187.Cross-lingual Transfer for Automatic Question Generation by Learning Interrogative Structures in Target Languages.pdf\n",
      "Idx: 48\tFile: 744.1+12 Can Large Language Models Serve as Cross-Lingual Knowledge Aggregators.pdf\n",
      "Idx: 49\tFile: 593.The Lou Dataset - Exploring the Impact of Gender-Fair Language in German Text Classification.pdf\n",
      "Idx: 50\tFile: 9.When LLMs Meets Acoustic Landmarks An Efficient Approach to Integrate Speech into Large Language Models for Depression Detection.pdf\n",
      "Idx: 51\tFile: 152.Surveying the Dead Minds Historical-Psychological Text Analysis with Contextualized Construct Representation (CCR) for Classical Chinese.pdf\n",
      "Idx: 52\tFile: 1168.TokenVerse Towards Unifying Speech and NLP Tasks via Transducer-based ASR.pdf\n",
      "Idx: 53\tFile: 139.Direct Multi-Turn Preference Optimization for Language Agents.pdf\n",
      "Idx: 54\tFile: 979.Are Large Language Models Capable of Generating Human-Level Narratives.pdf\n",
      "Idx: 55\tFile: 708.“Global is Good, Local is Bad” Understanding Brand Bias in LLMs.pdf\n",
      "Idx: 56\tFile: 669.Multi-Level Cross-Modal Alignment for Speech Relation Extraction.pdf\n",
      "Idx: 57\tFile: 1156.Let’s discuss! Quality Dimensions and Annotated Datasets for Computational Argument Quality Assessment.pdf\n",
      "Idx: 58\tFile: 510.Can Active Label Correction Improve LLM-based Modular AI Systems.pdf\n",
      "Idx: 59\tFile: 1256.Improving Minimum Bayes Risk Decoding with Multi-Prompt.pdf\n",
      "Idx: 60\tFile: 772.MOSEL 950,000 Hours of Speech Data for Open-Source Speech Foundation Model Training on EU Languages.pdf\n",
      "Idx: 61\tFile: 11.Hateful Word in Context Classification.pdf\n",
      "Idx: 62\tFile: 427.Multi-Dialect Vietnamese Task, Dataset, Baseline Models and Challenges.pdf\n",
      "Idx: 63\tFile: 154.QUITE Quantifying Uncertainty in Natural Language Text in Bayesian Reasoning Scenarios.pdf\n",
      "Idx: 64\tFile: 984.The Zeno’s Paradox of ‘Low-Resource’ Languages.pdf\n",
      "Idx: 65\tFile: 1262.Knowledge Graph Enhanced Large Language Model Editing.pdf\n",
      "Idx: 66\tFile: 968.Representational Analysis of Binding in Language Models.pdf\n",
      "Idx: 67\tFile: 629.Do LLMs suffer from Multi-Party Hangover A Diagnostic Approach to Addressee Recognition and Response Selection in Conversations.pdf\n",
      "Idx: 68\tFile: 439.KnowledgeSG Privacy-Preserving Synthetic Text Generation with Knowledge Distillation from Server.pdf\n",
      "Idx: 69\tFile: 487.Knowledge Conflicts for LLMs A Survey.pdf\n",
      "Idx: 70\tFile: 1255.Coffee-Gym An Environment for Evaluating and Improving Natural Language Feedback on Erroneous Code.pdf\n",
      "Idx: 71\tFile: 654.Enhancing High-order Interaction Awareness in LLM-based Recommender Model.pdf\n",
      "Idx: 72\tFile: 673.Subword Segmentation in LLMs Looking at Inflection and Consistency.pdf\n",
      "Idx: 73\tFile: 720.Target-Aware Language Modeling via Granular Data Sampling.pdf\n",
      "Idx: 74\tFile: 1203.Memory-Efficient Fine-Tuning of Transformers via Token Selection.pdf\n",
      "Idx: 75\tFile: 684.GlossLM A Massively Multilingual Corpus and Pretrained Model for Interlinear Glossed Text.pdf\n",
      "Idx: 76\tFile: 413.Aligning Large Language Models with Diverse Political Viewpoints.pdf\n",
      "Idx: 77\tFile: 44.Parameter-Efficient Sparsity Crafting from Dense to Mixture-of-Experts for Instruction Tuning on General Tasks.pdf\n",
      "Idx: 78\tFile: 867.CONTESTS a Framework for Consistency Testing of Span Probabilities in Language Models.pdf\n",
      "Idx: 79\tFile: 1081.Investigating the Role of Instruction Variety and Task Difficulty in Robotic Manipulation Tasks.pdf\n",
      "Idx: 80\tFile: 1019.ToolPlanner A Tool Augmented LLM for Multi Granularity Instructions with Path Planning and Feedback.pdf\n",
      "Idx: 81\tFile: 231.Where is the signal in tokenization space.pdf\n",
      "Idx: 82\tFile: 122.GLaPE Gold Label-agnostic Prompt Evaluation for Large Language Models.pdf\n",
      "Idx: 83\tFile: 478.MetaReflection Learning Instructions for Language Agents using Past Reflections.pdf\n",
      "Idx: 84\tFile: 511.Statistical Uncertainty in Word Embeddings GloVe-V.pdf\n",
      "Idx: 85\tFile: 1077.VerifyMatch A Semi-Supervised Learning Paradigm for Natural Language Inference with Confidence-Aware MixUp.pdf\n",
      "Idx: 86\tFile: 415.Extending Context Window of Large Language Models from a Distributional Perspective.pdf\n",
      "Idx: 87\tFile: 384.InterIntent Investigating Social Intelligence of LLMs via Intention Understanding in an Interactive Game Context.pdf\n",
      "Idx: 88\tFile: 996.PepRec Progressive Enhancement of Prompting for Recommendation.pdf\n",
      "Idx: 89\tFile: 407.Learning to Retrieve Iteratively for In-Context Learning.pdf\n",
      "Idx: 90\tFile: 409.Python is Not Always the Best Choice Embracing Multilingual Program of Thoughts.pdf\n",
      "Idx: 91\tFile: 374.Unifying Multimodal Retrieval via Document Screenshot Embedding.pdf\n",
      "Idx: 92\tFile: 1175.Language is Scary when Over-Analyzed Unpacking Implied Misogynistic Reasoning with Argumentation Theory-Driven Prompts.pdf\n",
      "Idx: 93\tFile: 1063.Preserving Multi-Modal Capabilities of Pre-trained VLMs for Improving Vision-Linguistic Compositionality.pdf\n",
      "Idx: 94\tFile: 268.How Do Humans Write Code Large Models Do It the Same Way Too.pdf\n",
      "Idx: 95\tFile: 698.Neeko Leveraging Dynamic LoRA for Efficient Multi-Character Role-Playing Agent.pdf\n",
      "Idx: 96\tFile: 245.Hidden Persuaders LLMs’ Political Leaning and Their Influence on Voters.pdf\n",
      "Idx: 97\tFile: 710.ACE A LLM-based Negotiation Coaching System.pdf\n",
      "Idx: 98\tFile: 155.African or European Swallow Benchmarking Large Vision-Language Models for Fine-Grained Object Classification.pdf\n",
      "Idx: 99\tFile: 1135.“They are uncultured” Unveiling Covert Harms and Social Threats in LLM Generated Conversations.pdf\n",
      "Idx: 100\tFile: 1089.Factuality of Large Language Models A Survey.pdf\n",
      "Idx: 101\tFile: 3.Multi-News+ Cost-efficient Dataset Cleansing via LLM-based Data Annotation.pdf\n",
      "Idx: 102\tFile: 1163.Task Oriented In-Domain Data Augmentation.pdf\n",
      "Idx: 103\tFile: 2.UniGen Universal Domain Generalization for Sentiment Classification via Zero-shot Dataset Generation.pdf\n",
      "Idx: 104\tFile: 1072.SynthesizRR Generating Diverse Datasets with Retrieval Augmentation.pdf\n",
      "Idx: 105\tFile: 199.Fine-Grained Prediction of Reading Comprehension from Eye Movements.pdf\n",
      "Idx: 106\tFile: 254.Order of Magnitude Speedups for LLM Membership Inference.pdf\n",
      "Idx: 107\tFile: 222.PTD-SQL Partitioning and Targeted Drilling with LLMs in Text-to-SQL.pdf\n",
      "Idx: 108\tFile: 151.Collaborative Performance Prediction for Large Language Models.pdf\n",
      "Idx: 109\tFile: 747.Adaptive Query Rewriting Aligning Rewriters through Marginal Probability of Conversational Answers.pdf\n",
      "Idx: 110\tFile: 291.FOOL ME IF YOU CAN! An Adversarial Dataset to Investigate the Robustness of LMs in Word Sense Disambiguation.pdf\n",
      "Idx: 111\tFile: 917.Scaling Laws for Linear Complexity Language Models.pdf\n",
      "Idx: 112\tFile: 241.Modular Pluralism Pluralistic Alignment via Multi-LLM Collaboration.pdf\n",
      "Idx: 113\tFile: 272.CoEvol Constructing Better Responses for Instruction Finetuning through Multi-Agent Cooperation.pdf\n",
      "Idx: 114\tFile: 175.The Accuracy Paradox in RLHF When Better Reward Models Don’t Yield Better Language Models.pdf\n",
      "Idx: 115\tFile: 824.Bridging Local Details and Global Context in Text-Attributed Graphs.pdf\n",
      "Idx: 116\tFile: 567.Fine-grained Pluggable Gradient Ascent for Knowledge Unlearning in Language Models.pdf\n",
      "Idx: 117\tFile: 441.Unlocking the Future Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models.pdf\n",
      "Idx: 118\tFile: 1123.Detecting Online Community Practices with Large Language Models A Case Study of Pro-Ukrainian Publics on Twitter.pdf\n",
      "Idx: 119\tFile: 584.Enhancing Language Model Factuality via Activation-Based Confidence Calibration and Guided Decoding.pdf\n",
      "Idx: 120\tFile: 178.CUTE Measuring LLMs’ Understanding of Their Tokens.pdf\n",
      "Idx: 121\tFile: 282.Crafting Personalized Agents through Retrieval-Augmented Generation on Editable Memory Graphs.pdf\n",
      "Idx: 122\tFile: 1261.Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models.pdf\n",
      "Idx: 123\tFile: 618.Paraphrase Types Elicit Prompt Engineering Capabilities.pdf\n",
      "Idx: 124\tFile: 1104.MIPD Exploring Manipulation and Intention In a Novel Corpus of Polish Disinformation.pdf\n",
      "Idx: 125\tFile: 1119.CodeJudge Evaluating Code Generation with Large Language Models.pdf\n",
      "Idx: 126\tFile: 812.LLM Task Interference An Initial Study on the Impact of Task-Switch in Conversational History.pdf\n",
      "Idx: 127\tFile: 1199.De-Identification of Sensitive Personal Data in Datasets Derived from IIT-CDIP.pdf\n",
      "Idx: 128\tFile: 40.CoCoLoFa A Dataset of News Comments with Common Logical Fallacies Written by LLM-Assisted Crowds.pdf\n",
      "Idx: 129\tFile: 270.Forgetting Curve A Reliable Method for Evaluating Memorization Capability for Long-Context Models.pdf\n",
      "Idx: 130\tFile: 611.ATM Adversarial Tuning Multi-agent System Makes a Robust Retrieval-Augmented Generator.pdf\n",
      "Idx: 131\tFile: 27.Strength Lies in Differences! Improving Strategy Planning for Non-collaborative Dialogues via Diversified User Simulation.pdf\n",
      "Idx: 132\tFile: 1028.A Simple and Effective L_2 Norm-Based Strategy for KV Cache Compression.pdf\n",
      "Idx: 133\tFile: 126.Advancing Process Verification for Large Language Models via Tree-Based Preference Learning.pdf\n",
      "Idx: 134\tFile: 444.Can Large Language Models Faithfully Express Their Intrinsic Uncertainty in Words.pdf\n",
      "Idx: 135\tFile: 355.On the Reliability of Psychological Scales on Large Language Models.pdf\n",
      "Idx: 136\tFile: 1244.M3D MultiModal MultiDocument Fine-Grained Inconsistency Detection.pdf\n",
      "Idx: 137\tFile: 1150.Interventional Speech Noise Injection for ASR Generalizable Spoken Language Understanding.pdf\n",
      "Idx: 138\tFile: 278.KidLM Advancing Language Models for Children – Early Insights and Future Directions.pdf\n",
      "Idx: 139\tFile: 1124.Multilingual Topic Classification in X Dataset and Analysis.pdf\n",
      "Idx: 140\tFile: 912.Finding Blind Spots in Evaluator LLMs with Interpretable Checklists.pdf\n",
      "Idx: 141\tFile: 558.STORYSUMM Evaluating Faithfulness in Story Summarization.pdf\n",
      "Idx: 142\tFile: 459.AdaSwitch Adaptive Switching between Small and Large Agents for Effective Cloud-Local Collaborative Learning.pdf\n",
      "Idx: 143\tFile: 1082.GPT vs RETRO Exploring the Intersection of Retrieval and Parameter-Efficient Fine-Tuning.pdf\n",
      "Idx: 144\tFile: 502.MOSEL Inference Serving Using Dynamic Modality Selection.pdf\n",
      "Idx: 145\tFile: 846.Dense X Retrieval What Retrieval Granularity Should We Use.pdf\n",
      "Idx: 146\tFile: 59.BlendFilter Advancing Retrieval-Augmented Large Language Models via Query Generation Blending and Knowledge Filtering.pdf\n",
      "Idx: 147\tFile: 1198.Local Contrastive Editing of Gender Stereotypes.pdf\n",
      "Idx: 148\tFile: 310.Relevance Is a Guiding Light Relevance-aware Adaptive Learning for End-to-end Task-oriented Dialogue System.pdf\n",
      "Idx: 149\tFile: 856.Efficient Overshadowed Entity Disambiguation by Mitigating Shortcut Learning.pdf\n",
      "Idx: 150\tFile: 896.MLLM-Protector Ensuring MLLM’s Safety without Hurting Performance.pdf\n",
      "Idx: 151\tFile: 647.Puzzle Solving using Reasoning of Large Language Models A Survey.pdf\n",
      "Idx: 152\tFile: 125.Towards Difficulty-Agnostic Efficient Transfer Learning for Vision-Language Models.pdf\n",
      "Idx: 153\tFile: 193.How do Large Language Models Learn In-Context Query and Key Matrices of In-Context Heads are Two Towers for Metric Learning.pdf\n",
      "Idx: 154\tFile: 869.DogeRM Equipping Reward Models with Domain Knowledge through Model Merging.pdf\n",
      "Idx: 155\tFile: 353.Learning from Natural Language Explanations for Generalizable Entity Matching.pdf\n",
      "Idx: 156\tFile: 52.In-context Contrastive Learning for Event Causality Identification.pdf\n",
      "Idx: 157\tFile: 500.MiniCheck Efficient Fact-Checking of LLMs on Grounding Documents.pdf\n",
      "Idx: 158\tFile: 389.Learning to Extract Structured Entities Using Language Models.pdf\n",
      "Idx: 159\tFile: 277.RWKV-CLIP A Robust Vision-Language Representation Learner.pdf\n",
      "Idx: 160\tFile: 760.Large Language Models Are Poor Clinical Decision-Makers A Comprehensive Benchmark.pdf\n",
      "Idx: 161\tFile: 399.RepEval Effective Text Evaluation with LLM Representation.pdf\n",
      "Idx: 162\tFile: 105.Retrieved Sequence Augmentation for Protein Representation Learning.pdf\n",
      "Idx: 163\tFile: 148.Seemingly Plausible Distractors in Multi-Hop Reasoning Are Large Language Models Attentive Readers.pdf\n",
      "Idx: 164\tFile: 729.Detecting Errors through Ensembling Prompts (DEEP) An End-to-End LLM Framework for Detecting Factual Errors.pdf\n",
      "Idx: 165\tFile: 721.SPEED++ A Multilingual Event Extraction Framework for Epidemic Prediction and Preparedness.pdf\n",
      "Idx: 166\tFile: 913.LONGAGENT Achieving Question Answering for 128k-Token-Long Documents through Multi-Agent Collaboration.pdf\n",
      "Idx: 167\tFile: 228.Empowering Large Language Model for Continual Video Question Answering with Collaborative Prompting.pdf\n",
      "Idx: 168\tFile: 728.Analysis of Plan-based Retrieval for Grounded Text Generation.pdf\n",
      "Idx: 169\tFile: 656.MARE Multi-Aspect Rationale Extractor on Unsupervised Rationale Extraction.pdf\n",
      "Idx: 170\tFile: 826.RepMatch Quantifying Cross-Instance Similarities in Representation Space.pdf\n",
      "Idx: 171\tFile: 1240.Is This a Bad Table A Closer Look at the Evaluation of Table Generation from Text.pdf\n",
      "Idx: 172\tFile: 1034.Enhanced Hallucination Detection in Neural Machine Translation through Simple Detector Aggregation.pdf\n",
      "Idx: 173\tFile: 894.Generating Demonstrations for In-Context Compositional Generalization in Grounded Language Learning.pdf\n",
      "Idx: 174\tFile: 624.BPO Staying Close to the Behavior LLM Creates Better Online LLM Alignment.pdf\n",
      "Idx: 175\tFile: 633.CodeAgent Autonomous Communicative Agents for Code Review.pdf\n",
      "Idx: 176\tFile: 1147.Can Language Models Induce Grammatical Knowledge from Indirect Evidence.pdf\n",
      "Idx: 177\tFile: 971.Context-Aware Adapter Tuning for Few-Shot Relation Learning in Knowledge Graphs.pdf\n",
      "Idx: 178\tFile: 1141.You Make me Feel like a Natural Question Training QA Systems on Transformed Trivia Questions.pdf\n",
      "Idx: 179\tFile: 608.What is lost in Normalization Exploring Pitfalls in Multilingual ASR Model Evaluations.pdf\n",
      "Idx: 180\tFile: 420.ADELIE Aligning Large Language Models on Information Extraction.pdf\n",
      "Idx: 181\tFile: 903.Free your mouse! Command Large Language Models to Generate Code to Format Word Documents.pdf\n",
      "Idx: 182\tFile: 731.Boosting Logical Fallacy Reasoning in LLMs via Logical Structure Tree.pdf\n",
      "Idx: 183\tFile: 303.Improving Spoken Language Modeling with Phoneme Classification A Simple Fine-tuning Approach.pdf\n",
      "Idx: 184\tFile: 425.Gold Panning in Vocabulary An Adaptive Method for Vocabulary Expansion of Domain-Specific LLMs.pdf\n",
      "Idx: 185\tFile: 461.mDPO Conditional Preference Optimization for Multimodal Large Language Models.pdf\n",
      "Idx: 186\tFile: 308.EXPLORA Efficient Exemplar Subset Selection for Complex Reasoning.pdf\n",
      "Idx: 187\tFile: 892.Themis A Reference-free NLG Evaluation Language Model with Flexibility and Interpretability.pdf\n",
      "Idx: 188\tFile: 181.BC-Prover Backward Chaining Prover for Formal Theorem Proving.pdf\n",
      "Idx: 189\tFile: 356.Contrastive Entity Coreference and Disambiguation for Historical Texts.pdf\n",
      "Idx: 190\tFile: 1075.DEM Distribution Edited Model for Training with Mixed Data Distributions.pdf\n",
      "Idx: 191\tFile: 55.Large Language Models for Data Annotation and Synthesis A Survey.pdf\n",
      "Idx: 192\tFile: 108.DA3 A Distribution-Aware Adversarial Attack against Language Models.pdf\n",
      "Idx: 193\tFile: 305.Formality is Favored Unraveling the Learning Preferences of Large Language Models on Data with Conflicting Knowledge.pdf\n",
      "Idx: 194\tFile: 1226.FairFlow Mitigating Dataset Biases through Undecided Learning for Natural Language Understanding.pdf\n",
      "Idx: 195\tFile: 966.Information Flow Routes Automatically Interpreting Language Models at Scale.pdf\n",
      "Idx: 196\tFile: 1172.I love pineapple on pizza != I hate pineapple on pizza Stance-Aware Sentence Transformers for Opinion Mining.pdf\n",
      "Idx: 197\tFile: 799.ECIS-VQG Generation of Entity-centric Information-seeking Questions from Videos.pdf\n",
      "Idx: 198\tFile: 1202.Do great minds think alike Investigating Human-AI Complementarity in Question Answering with CAIMIRA.pdf\n",
      "Idx: 199\tFile: 304.Safely Learning with Private Data A Federated Learning Framework for Large Language Model.pdf\n",
      "Idx: 200\tFile: 174.Calibrating the Confidence of Large Language Models by Eliciting Fidelity.pdf\n",
      "Idx: 201\tFile: 169.KB-Plugin A Plug-and-play Framework for Large Language Models to Induce Programs over Low-resourced Knowledge Bases.pdf\n",
      "Idx: 202\tFile: 661.NuNER Entity Recognition Encoder Pre-training via LLM-Annotated Data.pdf\n",
      "Idx: 203\tFile: 402.Towards Understanding Jailbreak Attacks in LLMs A Representation Space Analysis.pdf\n",
      "Idx: 204\tFile: 209.A Generic Method for Fine-grained Category Discovery in Natural Language Texts.pdf\n",
      "Idx: 205\tFile: 516.Enhancing Reinforcement Learning with Dense Rewards from Language Model Critic.pdf\n",
      "Idx: 206\tFile: 135.Prefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization.pdf\n",
      "Idx: 207\tFile: 572.I Learn Better If You Speak My Language Understanding the Superior Performance of Fine-Tuning Large Language Models with LLM-Generated Responses.pdf\n",
      "Idx: 208\tFile: 537.Precise Model Benchmarking with Only a Few Observations.pdf\n",
      "Idx: 209\tFile: 683.ReadMe++ Benchmarking Multilingual Language Models for Multi-Domain Readability Assessment.pdf\n",
      "Idx: 210\tFile: 606.More Insightful Feedback for Tutoring Enhancing Generation Mechanisms and Automatic Evaluation.pdf\n",
      "Idx: 211\tFile: 862.Re-ReST Reflection-Reinforced Self-Training for Language Agents.pdf\n",
      "Idx: 212\tFile: 957.Adaptive Question Answering Enhancing Language Model Proficiency for Addressing Knowledge Conflicts with Source Citations.pdf\n",
      "Idx: 213\tFile: 414.“You Gotta be a Doctor, Lin”  An Investigation of Name-Based Bias of Large Language Models in Employment Recommendations.pdf\n",
      "Idx: 214\tFile: 723.UNICORN A Unified Causal Video-Oriented Language-Modeling Framework for Temporal Video-Language Tasks.pdf\n",
      "Idx: 215\tFile: 167.Evaluating Large Language Models via Linguistic Profiling.pdf\n",
      "Idx: 216\tFile: 615.Towards Probing Speech-Specific Risks in Large Multimodal Models A Taxonomy, Benchmark, and Insights.pdf\n",
      "Idx: 217\tFile: 1263.‘Quis custodiet ipsos custodes’ Who will watch the watchmen On Detecting AI-generated peer-reviews.pdf\n",
      "Idx: 218\tFile: 737.Language Concept Erasure for Language-invariant Dense Retrieval.pdf\n",
      "Idx: 219\tFile: 338.Fine-Grained Detection of Solidarity for Women and Migrants in 155 Years of German Parliamentary Debates.pdf\n",
      "Idx: 220\tFile: 694.Decoding with Limited Teacher Supervision Requires Understanding When to Trust the Teacher.pdf\n",
      "Idx: 221\tFile: 201.Unsupervised Human Preference Learning.pdf\n",
      "Idx: 222\tFile: 738.Learning Personalized Alignment for Evaluating Open-ended Text Generation.pdf\n",
      "Idx: 223\tFile: 559.MMoE Enhancing Multimodal Models with Mixtures of Multimodal Interaction Experts.pdf\n",
      "Idx: 224\tFile: 1125.MT-Eval A Multi-Turn Capabilities Evaluation Benchmark for Large Language Models.pdf\n",
      "Idx: 225\tFile: 859.AudioVSR Enhancing Video Speech Recognition with Audio Data.pdf\n",
      "Idx: 226\tFile: 1058.Efficient Unseen Language Adaptation for Multilingual Pre-Trained Language Models.pdf\n",
      "Idx: 227\tFile: 296.Unveiling the Lexical Sensitivity of LLMs Combinatorial Optimization for Prompt Enhancement.pdf\n",
      "Idx: 228\tFile: 279.Using Language Models to Disambiguate Lexical Choices in Translation.pdf\n",
      "Idx: 229\tFile: 659.QGEval Benchmarking Multi-dimensional Evaluation for Question Generation.pdf\n",
      "Idx: 230\tFile: 1267.Filtered Direct Preference Optimization.pdf\n",
      "Idx: 231\tFile: 1074.DataNarrative Automated Data-Driven Storytelling with Visualizations and Texts.pdf\n",
      "Idx: 232\tFile: 791.Tools Fail Detecting Silent Errors in Faulty Tools.pdf\n",
      "Idx: 233\tFile: 991.Data Contamination Can Cross Language Barriers.pdf\n",
      "Idx: 234\tFile: 722.CoGen Learning from Feedback with Coupled Comprehension and Generation.pdf\n",
      "Idx: 235\tFile: 329.Consistent Bidirectional Language Modelling Expressive Power and Representational Conciseness.pdf\n",
      "Idx: 236\tFile: 992.Automated Essay Scoring A Reflection on the State of the Art.pdf\n",
      "Idx: 237\tFile: 137.Towards Low-Resource Harmful Meme Detection with LMM Agents.pdf\n",
      "Idx: 238\tFile: 1227.Style-Shifting Behaviour of the Manosphere on Reddit.pdf\n",
      "Idx: 239\tFile: 436.UNO Arena for Evaluating Sequential Decision-Making Capability of Large Language Models.pdf\n",
      "Idx: 240\tFile: 891.LLaMA-MoE Building Mixture-of-Experts from LLaMA with Continual Pre-Training.pdf\n",
      "Idx: 241\tFile: 931.Interpreting Context Look-ups in Transformers Investigating Attention-MLP Interactions.pdf\n",
      "Idx: 242\tFile: 1219.SpeechQE Estimating the Quality of Direct Speech Translation.pdf\n",
      "Idx: 243\tFile: 211.A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models.pdf\n",
      "Idx: 244\tFile: 21.Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing.pdf\n",
      "Idx: 245\tFile: 16.Fine-Tuning or Retrieval Comparing Knowledge Injection in LLMs.pdf\n",
      "Idx: 246\tFile: 607.Stable Language Model Pre-training by Reducing Embedding Variability.pdf\n",
      "Idx: 247\tFile: 842.Open-world Multi-label Text Classification with Extremely Weak Supervision.pdf\n",
      "Idx: 248\tFile: 1059.Prove Your Point! Bringing Proof-Enhancement Principles to Argumentative Essay Generation.pdf\n",
      "Idx: 249\tFile: 99.SHIELD Evaluation and Defense Strategies for Copyright Compliance in LLM Text Generation.pdf\n",
      "Idx: 250\tFile: 230.Dancing in Chains Reconciling Instruction Following and Faithfulness in Language Models.pdf\n",
      "Idx: 251\tFile: 486.Zero-shot Cross-domain Dialogue State Tracking via Context-aware Auto-prompting and Instruction-following Contrastive Decoding.pdf\n",
      "Idx: 252\tFile: 711.TransferTOD A Generalizable Chinese Multi-Domain Task-Oriented Dialogue System with Transfer Capabilities.pdf\n",
      "Idx: 253\tFile: 87.Mitigating Matthew Effect Multi-Hypergraph Boosted Multi-Interest Self-Supervised Learning for Conversational Recommendation.pdf\n",
      "Idx: 254\tFile: 1057.Link, Synthesize, Retrieve Universal Document Linking for Zero-Shot Information Retrieval.pdf\n",
      "Idx: 255\tFile: 91.Tracking the perspectives of interacting language models.pdf\n",
      "Idx: 256\tFile: 598.Fine-Tuning and Prompt Optimization Two Great Steps that Work Better Together.pdf\n",
      "Idx: 257\tFile: 1065.A Two-Step Approach for Data-Efficient French Pronunciation Learning.pdf\n",
      "Idx: 258\tFile: 666.Segment Any Text A Universal Approach for Robust, Efficient and Adaptable Sentence Segmentation.pdf\n",
      "Idx: 259\tFile: 98.Predicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement.pdf\n",
      "Idx: 260\tFile: 1122.Defending Jailbreak Prompts via In-Context Adversarial Game.pdf\n",
      "Idx: 261\tFile: 898.InfiniPot Infinite Context Processing on Memory-Constrained LLMs.pdf\n",
      "Idx: 262\tFile: 803.Can Automatic Metrics Assess High-Quality Translations.pdf\n",
      "Idx: 263\tFile: 564.Breaking ReLU Barrier Generalized MoEfication for Dense Pretrained Models.pdf\n",
      "Idx: 264\tFile: 42.FLIRT Feedback Loop In-context Red Teaming.pdf\n",
      "Idx: 265\tFile: 789.MolTRES Improving Chemical Language Representation Learning for Molecular Property Prediction.pdf\n",
      "Idx: 266\tFile: 214.VGBench Evaluating Large Language Models on Vector Graphics Understanding and Generation.pdf\n",
      "Idx: 267\tFile: 921.LM2 A Simple Society of Language Models Solves Complex Reasoning.pdf\n",
      "Idx: 268\tFile: 424.Do Text-to-Vis Benchmarks Test Real Use of Visualisations.pdf\n",
      "Idx: 269\tFile: 50.Overcome Noise and Bias Segmentation-Aided Multi-Granularity Denoising and Debiasing for Enhanced Quarduples Extraction in Dialogue.pdf\n",
      "Idx: 270\tFile: 499.A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery.pdf\n",
      "Idx: 271\tFile: 219.M2PT Multimodal Prompt Tuning for Zero-shot Instruction Learning.pdf\n",
      "Idx: 272\tFile: 568.ARM An Alignment-and-Replacement Module for Chinese Spelling Check Based on LLMs.pdf\n",
      "Idx: 273\tFile: 497.LIONs An Empirically Optimized Approach to Align Language Models.pdf\n",
      "Idx: 274\tFile: 1201.STAR SocioTechnical Approach to Red Teaming Language Models.pdf\n",
      "Idx: 275\tFile: 653.Major Entity Identification A Generalizable Alternative to Coreference Resolution.pdf\n",
      "Idx: 276\tFile: 138.VIVA A Benchmark for Vision-Grounded Decision-Making with Human Values.pdf\n",
      "Idx: 277\tFile: 133.Oddballs and Misfits Detecting Implicit Abuse in Which Identity Groups are Depicted as Deviating from the Norm.pdf\n",
      "Idx: 278\tFile: 689.Leveraging Estimated Transferability Over Human Intuition for Model Selection in Text Ranking.pdf\n",
      "Idx: 279\tFile: 1016.HiFT A Hierarchical Full Parameter Fine-Tuning Strategy.pdf\n",
      "Idx: 280\tFile: 419.Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale.pdf\n",
      "Idx: 281\tFile: 1250.Training-free Deep Concept Injection Enables Language Models for Video Question Answering.pdf\n",
      "Idx: 282\tFile: 773.Improving Knowledge Graph Completion with Structure-Aware Supervised Contrastive Learning.pdf\n",
      "Idx: 283\tFile: 1042.Semantics and Sentiment Cross-lingual Variations in Emoji Use.pdf\n",
      "Idx: 284\tFile: 828.A Closer Look at Multidimensional Online Political Incivility.pdf\n",
      "Idx: 285\tFile: 1010.Why do objects have many names A study on word informativeness in language use and lexical systems.pdf\n",
      "Idx: 286\tFile: 140.Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models.pdf\n",
      "Idx: 287\tFile: 822.VHASR A Multimodal Speech Recognition System With Vision Hotwords.pdf\n",
      "Idx: 288\tFile: 948.Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models.pdf\n",
      "Idx: 289\tFile: 66.DocHieNet A Large and Diverse Dataset for Document Hierarchy Parsing.pdf\n",
      "Idx: 290\tFile: 318.NLEBench+NorGLM A Comprehensive Empirical Analysis and Benchmark Dataset for Generative Language Models in Norwegian.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not convert string to float: b'0.00-44247785' : FloatObject (b'0.00-44247785') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 291\tFile: 85.Lookback Lens Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps.pdf\n",
      "Idx: 292\tFile: 806.KnowTuning Knowledge-aware Fine-tuning for Large Language Models.pdf\n",
      "Idx: 293\tFile: 1029.GOME Grounding-based Metaphor Binding With Conceptual Elaboration For Figurative Language Illustration.pdf\n",
      "Idx: 294\tFile: 1079.Mitigating the Impact of Reference Quality on Evaluation of Summarization Systems with Reference-Free Metrics.pdf\n",
      "Idx: 295\tFile: 819.FinDVer Explainable Claim Verification over Long and Hybrid-content Financial Documents.pdf\n",
      "Idx: 296\tFile: 994.Unveiling and Consulting Core Experts in Retrieval-Augmented MoE-based LLMs.pdf\n",
      "Idx: 297\tFile: 339.CItruS Chunked Instruction-aware State Eviction for Long Sequence Modeling.pdf\n",
      "Idx: 298\tFile: 963.MedCoT Medical Chain of Thought via Hierarchical Expert.pdf\n",
      "Idx: 299\tFile: 1098.Evaluating Diversity in Automatic Poetry Generation.pdf\n",
      "Idx: 300\tFile: 1266.ALVIN Active Learning Via INterpolation.pdf\n",
      "Idx: 301\tFile: 363.Verba volant, scripta volant Don’t worry! There are computational solutions for protoword reconstruction.pdf\n",
      "Idx: 302\tFile: 162.MoDULA Mixture of Domain-Specific and Universal LoRA for Multi-Task Learning.pdf\n",
      "Idx: 303\tFile: 488.MisinfoEval Generative AI in the Era of “Alternative Facts”.pdf\n",
      "Idx: 304\tFile: 1170.Memorize Step by Step Efficient Long-Context Prefilling with Incremental Memory and Decremental Chunk.pdf\n",
      "Idx: 305\tFile: 644.Repairs in a Block World A New Benchmark for Handling User Corrections with Multi-Modal Language Models.pdf\n",
      "Idx: 306\tFile: 1121.SYNFAC-EDIT Synthetic Imitation Edit Feedback for Factual Alignment in Clinical Summarization.pdf\n",
      "Idx: 307\tFile: 946.Community-Cross-Instruct Unsupervised Instruction Generation for Aligning Large Language Models to Online Communities.pdf\n",
      "Idx: 308\tFile: 681.ControlMath Controllable Data Generation Promotes Math Generalist Models.pdf\n",
      "Idx: 309\tFile: 576.Multimodal Clickbait Detection by De-confounding Biases Using Causal Representation Inference.pdf\n",
      "Idx: 310\tFile: 1021.How to Compute the Probability of a Word.pdf\n",
      "Idx: 311\tFile: 603.Towards Fast Multilingual LLM Inference Speculative Decoding and Specialized Drafters.pdf\n",
      "Idx: 312\tFile: 951.Do LLMs learn a true syntactic universal.pdf\n",
      "Idx: 313\tFile: 617.What are the Generator Preferences for End-to-end Task-Oriented Dialog System.pdf\n",
      "Idx: 314\tFile: 51.Integrating Plutchik’s Theory with Mixture of Experts for Enhancing Emotion Classification.pdf\n",
      "Idx: 315\tFile: 159.An Electoral Approach to Diversify LLM-based Multi-Agent Collective Decision-Making.pdf\n",
      "Idx: 316\tFile: 820.Extracting Prompts by Inverting LLM Outputs.pdf\n",
      "Idx: 317\tFile: 1043.The Emergence of Compositional Languages in Multi-entity Referential Games from Image to Graph Representations.pdf\n",
      "Idx: 318\tFile: 790.First Heuristic Then Rational Dynamic Use of Heuristics in Language Model Reasoning.pdf\n",
      "Idx: 319\tFile: 830.Applying Intrinsic Debiasing on Downstream Tasks Challenges and Considerations for Machine Translation.pdf\n",
      "Idx: 320\tFile: 411.Incomplete Utterance Rewriting with Editing Operation Guidance and Utterance Augmentation.pdf\n",
      "Idx: 321\tFile: 482.The Best Defense is Attack Repairing Semantics in Textual Adversarial Examples.pdf\n",
      "Idx: 322\tFile: 989.Embedded Named Entity Recognition using Probing Classifiers.pdf\n",
      "Idx: 323\tFile: 732.Chain and Causal Attention for Efficient Entity Tracking.pdf\n",
      "Idx: 324\tFile: 1264.Mitigating Open-Vocabulary Caption Hallucinations.pdf\n",
      "Idx: 325\tFile: 552.StablePrompt  Automatic Prompt Tuning using Reinforcement Learning for Large Language Model.pdf\n",
      "Idx: 326\tFile: 432.GENRA Enhancing Zero-shot Retrieval with Rank Aggregation.pdf\n",
      "Idx: 327\tFile: 145.Can visual language models resolve textual ambiguity with visual cues Let visual puns tell you!.pdf\n",
      "Idx: 328\tFile: 565.Detecting Subtle Differences between Human and Model Languages Using Spectrum of Relative Likelihood.pdf\n",
      "Idx: 329\tFile: 18.Studying and Mitigating Biases in Sign Language Understanding Models.pdf\n",
      "Idx: 330\tFile: 464.Attribute or Abstain Large Language Models as Long Document Assistants.pdf\n",
      "Idx: 331\tFile: 763.DynamicER Resolving Emerging Mentions to Dynamic Entities for RAG.pdf\n",
      "Idx: 332\tFile: 718.Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models.pdf\n",
      "Idx: 333\tFile: 952.GDPO Learning to Directly Align Language Models with Diversity Using GFlowNets.pdf\n",
      "Idx: 334\tFile: 246.SOUL Unlocking the Power of Second-Order Optimization for LLM Unlearning.pdf\n",
      "Idx: 335\tFile: 879.Images Speak Louder than Words Understanding and Mitigating Bias in Vision-Language Model from a Causal Mediation Perspective.pdf\n",
      "Idx: 336\tFile: 1127.CmdCaliper A Semantic-Aware Command-Line Embedding Model and Dataset for Security Research.pdf\n",
      "Idx: 337\tFile: 575.When Parts Are Greater Than Sums Individual LLM Components Can Outperform Full Models.pdf\n",
      "Idx: 338\tFile: 592.Roleplay-doh Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles.pdf\n",
      "Idx: 339\tFile: 746.Style-Specific Neurons for Steering LLMs in Text Style Transfer.pdf\n",
      "Idx: 340\tFile: 1013.On the Universal Truthfulness Hyperplane Inside LLMs.pdf\n",
      "Idx: 341\tFile: 1131.AMPO Automatic Multi-Branched Prompt Optimization.pdf\n",
      "Idx: 342\tFile: 1073.Multimodal Self-Instruct Synthetic Abstract Image and Visual Reasoning Instruction Using Language Model.pdf\n",
      "Idx: 343\tFile: 212.Decompose and Compare Consistency Measuring VLMs’ Answer Reliability via Task-Decomposition Consistency Comparison.pdf\n",
      "Idx: 344\tFile: 1100.Delving into Qualitative Implications of Synthetic Data for Hate Speech Detection.pdf\n",
      "Idx: 345\tFile: 1084.Sequential API Function Calling Using GraphQL Schema.pdf\n",
      "Idx: 346\tFile: 767.Topic-Oriented Open Relation Extraction with A Priori Seed Generation.pdf\n",
      "Idx: 347\tFile: 190.Improving Multi-party Dialogue Generation via Topic and Rhetorical Coherence.pdf\n",
      "Idx: 348\tFile: 927.SEGMENT+ Long Text Processing with Short-Context Language Models.pdf\n",
      "Idx: 349\tFile: 942.FFN-SkipLLM A Hidden Gem for Autoregressive Decoding with Adaptive Feed Forward Skipping.pdf\n",
      "Idx: 350\tFile: 188.ScalingFilter Assessing Data Quality through Inverse Utilization of Scaling Laws.pdf\n",
      "Idx: 351\tFile: 907.On the Robustness of Editing Large Language Models.pdf\n",
      "Idx: 352\tFile: 144.Selective Vision is the Challenge for Visual Reasoning A Benchmark for Visual Argument Understanding.pdf\n",
      "Idx: 353\tFile: 930.Small LLMs Are Weak Tool Learners A Multi-LLM Agent.pdf\n",
      "Idx: 354\tFile: 1154.IFCap Image-like Retrieval and Frequency-based Entity Filtering for Zero-shot Captioning.pdf\n",
      "Idx: 355\tFile: 236.Semantic Training Signals Promote Hierarchical Syntactic Generalization in Transformers.pdf\n",
      "Idx: 356\tFile: 652.Pragmatic Norms Are All You Need – Why The Symbol Grounding Problem Does Not Apply to LLMs.pdf\n",
      "Idx: 357\tFile: 78.Mitigating Language Bias of LMMs in Social Intelligence Understanding with Virtual Counterfactual Calibration.pdf\n",
      "Idx: 358\tFile: 26.Consolidating Ranking and Relevance Predictions of Large Language Models through Post-Processing.pdf\n",
      "Idx: 359\tFile: 205.From Bottom to Top Extending the Potential of Parameter Efficient Fine-Tuning.pdf\n",
      "Idx: 360\tFile: 725.OATH-Frames Characterizing Online Attitudes Towards Homelessness with LLM Assistants.pdf\n",
      "Idx: 361\tFile: 1008.Calibrating Language Models with Adaptive Temperature Scaling.pdf\n",
      "Idx: 362\tFile: 240.Teaching LLMs to Abstain across Languages via Multilingual Feedback.pdf\n",
      "Idx: 363\tFile: 143.Backward Lens Projecting Language Model Gradients into the Vocabulary Space.pdf\n",
      "Idx: 364\tFile: 1208.Metrics for What, Metrics for Whom Assessing Actionability of Bias Evaluation Metrics in NLP.pdf\n",
      "Idx: 365\tFile: 735.FAC2E Better Understanding Large Language Model Capabilities by Dissociating Language and Cognition.pdf\n",
      "Idx: 366\tFile: 845.CopyBench Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation.pdf\n",
      "Idx: 367\tFile: 1207.Preference-Guided Reflective Sampling for Aligning Language Models.pdf\n",
      "Idx: 368\tFile: 736.OpenSep Leveraging Large Language Models with Textual Inversion for Open World Audio Separation.pdf\n",
      "Idx: 369\tFile: 397.Revealing the Parallel Multilingual Learning within Large Language Models.pdf\n",
      "Idx: 370\tFile: 739.Large Language Models Are Involuntary Truth-Tellers Exploiting Fallacy Failure for Jailbreak Attacks.pdf\n",
      "Idx: 371\tFile: 1039.CHESS Optimizing LLM Inference via Channel-Wise Thresholding and Selective Sparsification.pdf\n",
      "Idx: 372\tFile: 536.Fast Forwarding Low-Rank Training.pdf\n",
      "Idx: 373\tFile: 586.InferAligner Inference-Time Alignment for Harmlessness through Cross-Model Guidance.pdf\n",
      "Idx: 374\tFile: 703.SUPER Evaluating Agents on Setting Up and Executing Tasks from Research Repositories.pdf\n",
      "Idx: 375\tFile: 97.Glue pizza and eat rocks - Exploiting Vulnerabilities in Retrieval-Augmented Generative Models.pdf\n",
      "Idx: 376\tFile: 924.Can We Trust the Performance Evaluation of Uncertainty Estimation Methods in Text Summarization.pdf\n",
      "Idx: 377\tFile: 1242.BMRetriever Tuning Large Language Models as Better Biomedical Text Retrievers.pdf\n",
      "Idx: 378\tFile: 6.Table Question Answering for Low-resourced Indic Languages.pdf\n",
      "Idx: 379\tFile: 1004.Seg2Act Global Context-aware Action Generation for Document Logical Structuring.pdf\n",
      "Idx: 380\tFile: 639.TEMA Token Embeddings Mapping for Enriching Low-Resource Language Models.pdf\n",
      "Idx: 381\tFile: 1158.One-to-Many Communication and Compositionality in Emergent Communication.pdf\n",
      "Idx: 382\tFile: 781.Cluster-Norm for Unsupervised Probing of Knowledge.pdf\n",
      "Idx: 383\tFile: 1011.Dual-Space Knowledge Distillation for Large Language Models.pdf\n",
      "Idx: 384\tFile: 1167.PREDICT Multi-Agent-based Debate Simulation for Generalized Hate Speech Detection.pdf\n",
      "Idx: 385\tFile: 372.Personalized Pieces Efficient Personalized Large Language Models through Collaborative Efforts.pdf\n",
      "Idx: 386\tFile: 208.D2R Dual-Branch Dynamic Routing Network for Multimodal Sentiment Detection.pdf\n",
      "Idx: 387\tFile: 65.A Survey on In-context Learning.pdf\n",
      "Idx: 388\tFile: 319.RSA-Control A Pragmatics-Grounded Lightweight Controllable Text Generation Framework.pdf\n",
      "Idx: 389\tFile: 489.MEANT Multimodal Encoder for Antecedent Information.pdf\n",
      "Idx: 390\tFile: 271.Retrieve-Plan-Generation An Iterative Planning and Answering Framework for Knowledge-Intensive LLM Generation.pdf\n",
      "Idx: 391\tFile: 1.Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing.pdf\n",
      "Idx: 392\tFile: 705.Domain adapted machine translation What does catastrophic forgetting forget and why.pdf\n",
      "Idx: 393\tFile: 1069.ShadowLLM Predictor-based Contextual Sparsity for Large Language Models.pdf\n",
      "Idx: 394\tFile: 875.Human-LLM Hybrid Text Answer Aggregation for Crowd Annotations.pdf\n",
      "Idx: 395\tFile: 944.Deciphering the Interplay of Parametric and Non-parametric Memory in Retrieval-augmented Language Models.pdf\n",
      "Idx: 396\tFile: 81.Large Language Models as Foundations for Next-Gen Dense Retrieval A Comprehensive Empirical Assessment.pdf\n",
      "Idx: 397\tFile: 926.BPE Gets Picky Efficient Vocabulary Refinement During Tokenizer Training.pdf\n",
      "Idx: 398\tFile: 622.Split and Merge Aligning Position Biases in LLM-based Evaluators.pdf\n",
      "Idx: 399\tFile: 457.Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works.pdf\n",
      "Idx: 400\tFile: 354.Do You Know What You Are Talking About Characterizing Query-Knowledge Relevance For Reliable Retrieval Augmented Generation.pdf\n",
      "Idx: 401\tFile: 702.Lifelong Event Detection via Optimal Transport.pdf\n",
      "Idx: 402\tFile: 549.Extract, Define, Canonicalize An LLM-based Framework for Knowledge Graph Construction.pdf\n",
      "Idx: 403\tFile: 641.Text2Chart31 Instruction Tuning for Chart Generation with Automatic Feedback.pdf\n",
      "Idx: 404\tFile: 965.Inductive-Deductive Strategy Reuse for Multi-Turn Instructional Dialogues.pdf\n",
      "Idx: 405\tFile: 93.Can Large Language Models Always Solve Easy Problems if They Can Solve Harder Ones.pdf\n",
      "Idx: 406\tFile: 977.Don’t Forget Your Reward Values Language Model Alignment via Value-based Calibration.pdf\n",
      "Idx: 407\tFile: 938.YesBut A High-Quality Annotated Multimodal Dataset for evaluating Satire Comprehension capability of Vision-Language Models.pdf\n",
      "Idx: 408\tFile: 995.CURE Context- and Uncertainty-Aware Mental Disorder Detection.pdf\n",
      "Idx: 409\tFile: 233.Estimating Knowledge in Large Language Models Without Generating a Single Token.pdf\n",
      "Idx: 410\tFile: 920.ATAP Automatic Template-Augmented Commonsense Knowledge Graph Completion via Pre-Trained Language Models.pdf\n",
      "Idx: 411\tFile: 63.RULE Reliable Multimodal RAG for Factuality in Medical Vision Language Models.pdf\n",
      "Idx: 412\tFile: 613.Unlabeled Debiasing in Downstream Tasks via Class-wise Low Variance Regularization.pdf\n",
      "Idx: 413\tFile: 561.Enhancing Pre-Trained Generative Language Models with Question Attended Span Extraction on Machine Reading Comprehension.pdf\n",
      "Idx: 414\tFile: 648.SciEx Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading.pdf\n",
      "Idx: 415\tFile: 880.Mitigating the Language Mismatch and Repetition Issues in LLM-based Machine Translation via Model Editing.pdf\n",
      "Idx: 416\tFile: 1166.No Culture Left Behind ArtELingo-28, a Benchmark of WikiArt with Captions in 28 Languages.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not convert string to float: b'0.000-15223097' : FloatObject (b'0.000-15223097') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 417\tFile: 1130.Towards Aligning Language Models with Textual Feedback.pdf\n",
      "Idx: 418\tFile: 75.Effective Demonstration Annotation for In-Context Learning via Language Model-Based Determinantal Point Process.pdf\n",
      "Idx: 419\tFile: 540.Development of Cognitive Intelligence in Pre-trained Language Models.pdf\n",
      "Idx: 420\tFile: 905.The Instinctive Bias Spurious Images lead to Illusion in MLLMs.pdf\n",
      "Idx: 421\tFile: 1015.User Inference Attacks on Large Language Models.pdf\n",
      "Idx: 422\tFile: 802.ASL STEM Wiki Dataset and Benchmark for Interpreting STEM Articles.pdf\n",
      "Idx: 423\tFile: 717.Defending Against Social Engineering Attacks in the Age of LLMs.pdf\n",
      "Idx: 424\tFile: 314.Adaption-of-Thought Learning Question Difficulty Improves Large Language Models for Reasoning.pdf\n",
      "Idx: 425\tFile: 300.LUQ Long-text Uncertainty Quantification for LLMs.pdf\n",
      "Idx: 426\tFile: 901.Defining Knowledge Bridging Epistemology and Large Language Models.pdf\n",
      "Idx: 427\tFile: 132.I Need Help! Evaluating LLM’s Ability to Ask for Users’ Support A Case Study on Text-to-SQL Generation.pdf\n",
      "Idx: 428\tFile: 466.Retrieved In-Context Principles from Previous Mistakes.pdf\n",
      "Idx: 429\tFile: 1068.Comparing a BERT Classifier and a GPT classifier for Detecting Connective Language Across Multiple Social Media.pdf\n",
      "Idx: 430\tFile: 872.Re-Reading Improves Reasoning in Large Language Models.pdf\n",
      "Idx: 431\tFile: 600.AmbigNLG Addressing Task Ambiguity in Instruction for NLG.pdf\n",
      "Idx: 432\tFile: 628.A Multi-Perspective Analysis of Memorization in Large Language Models.pdf\n",
      "Idx: 433\tFile: 367.EH-MAM Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning.pdf\n",
      "Idx: 434\tFile: 658.“A good pun is its own reword” Can Large Language Models Understand Puns.pdf\n",
      "Idx: 435\tFile: 1136.Multi-expert Prompting Improves Reliability, Safety and Usefulness of Large Language Models.pdf\n",
      "Idx: 436\tFile: 348.Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation.pdf\n",
      "Idx: 437\tFile: 701.Why Does New Knowledge Create Messy Ripple Effects in LLMs.pdf\n",
      "Idx: 438\tFile: 229.Dissecting Fine-Tuning Unlearning in Large Language Models.pdf\n",
      "Idx: 439\tFile: 508.Assessing “Implicit” Retrieval Robustness of Large Language Models.pdf\n",
      "Idx: 440\tFile: 61.Eliminating Biased Length Reliance of Direct Preference Optimization via Down-Sampled KL Divergence.pdf\n",
      "Idx: 441\tFile: 595.Evidence-Focused Fact Summarization for Knowledge-Augmented Zero-Shot Question Answering.pdf\n",
      "Idx: 442\tFile: 816.Revisiting Automated Evaluation for Long-form Table Question Answering.pdf\n",
      "Idx: 443\tFile: 456.Empowering Backbone Models for Visual Text Generation with Input Granularity Control and Glyph-Aware Training.pdf\n",
      "Idx: 444\tFile: 888.Temporally Consistent Factuality Probing for Large Language Models.pdf\n",
      "Idx: 445\tFile: 914.AutoPersuade A Framework for Evaluating and Explaining Persuasive Arguments.pdf\n",
      "Idx: 446\tFile: 674.Explicit, Implicit, and Scattered Revisiting Event Extraction to Capture Complex Arguments.pdf\n",
      "Idx: 447\tFile: 153.Knowledge Verification to Nip Hallucination in the Bud.pdf\n",
      "Idx: 448\tFile: 531.The effects of distance on NPI illusive effects in BERT.pdf\n",
      "Idx: 449\tFile: 89.Exploring Union and Intersection of Visual Regions for Generating Questions, Answers, and Distractors.pdf\n",
      "Idx: 450\tFile: 1035.Jailbreaking LLMs with Arabic Transliteration and Arabizi.pdf\n",
      "Idx: 451\tFile: 734.A Bayesian Approach to Harnessing the Power of LLMs in Authorship Attribution.pdf\n",
      "Idx: 452\tFile: 166.MiniConGTS A Near Ultimate Minimalist Contrastive Grid Tagging Scheme for Aspect Sentiment Triplet Extraction.pdf\n",
      "Idx: 453\tFile: 915.Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs.pdf\n",
      "Idx: 454\tFile: 958.Granular Privacy Control for Geolocation with Vision Language Models.pdf\n",
      "Idx: 455\tFile: 808.Nash CoT Multi-Path Inference with Preference Equilibrium.pdf\n",
      "Idx: 456\tFile: 1231.The LLM Effect Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead.pdf\n",
      "Idx: 457\tFile: 1001.Quantum Recurrent Architectures for Text Classification.pdf\n",
      "Idx: 458\tFile: 1143.Flee the Flaw Annotating the Underlying Logic of Fallacious Arguments Through Templates and Slot-filling.pdf\n",
      "Idx: 459\tFile: 1220.Assessing and Verifying Task Utility in LLM-Powered Applications.pdf\n",
      "Idx: 460\tFile: 911.WorryWords Norms of Anxiety Association for over 44k English Words.pdf\n",
      "Idx: 461\tFile: 380.Verifiable, Debuggable, and Repairable Commonsense Logical Reasoning via LLM-based Theory Resolution.pdf\n",
      "Idx: 462\tFile: 306.How Does the Textual Information Affect the Retrieval of Multimodal In-Context Learning.pdf\n",
      "Idx: 463\tFile: 4.FIZZ Factual Inconsistency Detection by Zoom-in Summary and Zoom-out Document.pdf\n",
      "Idx: 464\tFile: 545.TraveLER A Modular Multi-LMM Agent Framework for Video Question-Answering.pdf\n",
      "Idx: 465\tFile: 297.SEACrowd A Multilingual Multimodal Data Hub and Benchmark Suite for Southeast Asian Languages.pdf\n",
      "Idx: 466\tFile: 1269.Entity Insertion in Multilingual Linked Corpora The Case of Wikipedia.pdf\n",
      "Idx: 467\tFile: 577.Matryoshka-Adaptor Unsupervised and Supervised Tuning for Smaller Embedding Dimensions.pdf\n",
      "Idx: 468\tFile: 1120.Self-Training Large Language and Vision Assistant for Medical Question Answering.pdf\n",
      "Idx: 469\tFile: 670.Self-Training for Sample-Efficient Active Learning for Text Classification with Pre-Trained Language Models.pdf\n",
      "Idx: 470\tFile: 113.TinyChart Efficient Chart Understanding with Program-of-Thoughts Learning and Visual Token Merging.pdf\n",
      "Idx: 471\tFile: 832.SparseGrad A Selective Method for Efficient Fine-tuning of MLP Layers.pdf\n",
      "Idx: 472\tFile: 582.“In-Dialogues We Learn” Towards Personalized Dialogue Without Pre-defined Profiles through In-Dialogue Learning.pdf\n",
      "Idx: 473\tFile: 631.Unveiling the Role of Pretraining in Direct Speech Translation.pdf\n",
      "Idx: 474\tFile: 410.Advancing Adversarial Suffix Transfer Learning on Aligned Large Language Models.pdf\n",
      "Idx: 475\tFile: 1249.Simultaneous Interpretation Corpus Construction by Large Language Models in Distant Language Pair.pdf\n",
      "Idx: 476\tFile: 104.Event Causality Identification with Synthetic Control.pdf\n",
      "Idx: 477\tFile: 645.Beyond the Turn-Based Game Enabling Real-Time Conversations with Duplex Models.pdf\n",
      "Idx: 478\tFile: 1176.Thoughts to Target Enhance Planning for Target-driven Conversation.pdf\n",
      "Idx: 479\tFile: 646.Strengthening Structural Inductive Biases by Pre-training to Perform Syntactic Transformations.pdf\n",
      "Idx: 480\tFile: 902.TKGT Redefinition and A New Way of Text-to-Table Tasks Based on Real World Demands and Knowledge Graphs Augmented LLMs.pdf\n",
      "Idx: 481\tFile: 918.Autoregressive Multi-trait Essay Scoring via Reinforcement Learning with Scoring-aware Multiple Rewards.pdf\n",
      "Idx: 482\tFile: 679.Empowering Multi-step Reasoning across Languages via Program-Aided Language Models.pdf\n",
      "Idx: 483\tFile: 754.Zero-Shot Cross-Lingual NER Using Phonemic Representations for Low-Resource Languages.pdf\n",
      "Idx: 484\tFile: 866.Exploring the Learning Capabilities of Language Models using LEVERWORLDS.pdf\n",
      "Idx: 485\tFile: 583.Encourage or Inhibit Monosemanticity Revisit Monosemanticity from a Feature Decorrelation Perspective.pdf\n",
      "Idx: 486\tFile: 1003.What the Harm Quantifying the Tangible Impact of Gender Bias in Machine Translation with a Human-centered Study.pdf\n",
      "Idx: 487\tFile: 1179.Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction Value Also Matters.pdf\n",
      "Idx: 488\tFile: 685.GDTB Genre Diverse Data for English Shallow Discourse Parsing across Modalities, Text Types, and Domains.pdf\n",
      "Idx: 489\tFile: 1009.Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance.pdf\n",
      "Idx: 490\tFile: 529.Structure Guided Prompt Instructing Large Language Model in Multi-Step Reasoning by Exploring Graph Structure of the Text.pdf\n",
      "Idx: 491\tFile: 861.Ladder A Model-Agnostic Framework Boosting LLM-based Machine Translation to the Next Level.pdf\n",
      "Idx: 492\tFile: 406.Advancing Test-Time Adaptation in Wild Acoustic Test Settings.pdf\n",
      "Idx: 493\tFile: 578.KNN-Instruct Automatic Instruction Construction with K Nearest Neighbor Deduction.pdf\n",
      "Idx: 494\tFile: 1257.Deciphering Cognitive Distortions in Patient-Doctor Mental Health Conversations A Multimodal LLM-Based Detection and Reasoning Framework.pdf\n",
      "Idx: 495\tFile: 517.Words Matter Reducing Stigma in Online Conversations about Substance Use with Large Language Models.pdf\n",
      "Idx: 496\tFile: 890.Can LLMs replace Neil deGrasse Tyson Evaluating the Reliability of LLMs as Science Communicators.pdf\n",
      "Idx: 497\tFile: 266.World to Code Multi-modal Data Generation via Self-Instructed Compositional Captioning and Filtering.pdf\n",
      "Idx: 498\tFile: 1193.Beyond Turn-Based Interfaces Synchronous LLMs as Full-Duplex Dialogue Agents.pdf\n",
      "Idx: 499\tFile: 871.Unlocking Anticipatory Text Generation A Constrained Approach for Large Language Models Decoding.pdf\n",
      "Idx: 500\tFile: 588.Fisher Information-based Efficient Curriculum Federated Learning with Large Language Models.pdf\n",
      "Idx: 501\tFile: 574.An image speaks a thousand words, but can everyone listen On image transcreation for cultural relevance.pdf\n",
      "Idx: 502\tFile: 678.Medical Adaptation of Large Language and Vision-Language Models Are We Making Progress.pdf\n",
      "Idx: 503\tFile: 814.Chain-of-Note Enhancing Robustness in Retrieval-Augmented Language Models.pdf\n",
      "Idx: 504\tFile: 727.SciER An Entity and Relation Extraction Dataset for Datasets, Methods, and Tasks in Scientific Documents.pdf\n",
      "Idx: 505\tFile: 118.TCSinger Zero-Shot Singing Voice Synthesis with Style Transfer and Multi-Level Style Control.pdf\n",
      "Idx: 506\tFile: 111.Self-Bootstrapped Visual-Language Model for Knowledge Selection and Question Answering.pdf\n",
      "Idx: 507\tFile: 102.Triad A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering.pdf\n",
      "Idx: 508\tFile: 528.Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation.pdf\n",
      "Idx: 509\tFile: 460.CoBa Convergence Balancer for Multitask Finetuning of Large Language Models.pdf\n",
      "Idx: 510\tFile: 570.Atomic Inference for NLI with Generated Facts as Atoms.pdf\n",
      "Idx: 511\tFile: 850.Optimizing Chinese Lexical Simplification Across Word Types A Hybrid Approach.pdf\n",
      "Idx: 512\tFile: 256.F2RL Factuality and Faithfulness Reinforcement Learning Framework for Claim-Guided Evidence-Supported Counterspeech Generation.pdf\n",
      "Idx: 513\tFile: 103.MetaGPT Merging Large Language Models Using Model Exclusive Task Arithmetic.pdf\n",
      "Idx: 514\tFile: 67.AMR-Evol Adaptive Modular Response Evolution Elicits Better Knowledge Distillation for Large Language Models in Code Generation.pdf\n",
      "Idx: 515\tFile: 1041.DocCGen Document-based Controlled Code Generation.pdf\n",
      "Idx: 516\tFile: 929.Closing the Loop Learning to Generate Writing Feedback via Language Model Simulated Student Revisions.pdf\n",
      "Idx: 517\tFile: 1076.Altogether Image Captioning via Re-aligning Alt-text.pdf\n",
      "Idx: 518\tFile: 969.CoSafe Evaluating Large Language Model Safety in Multi-Turn Dialogue Coreference.pdf\n",
      "Idx: 519\tFile: 170.Understanding Higher-Order Correlations Among Semantic Components in Embeddings.pdf\n",
      "Idx: 520\tFile: 483.CSSL Contrastive Self-Supervised Learning for Dependency Parsing on Relatively Free Word Ordered and Morphologically Rich Low Resource Languages.pdf\n",
      "Idx: 521\tFile: 1216.Sprout Green Generative AI with Carbon-Efficient LLM Inference.pdf\n",
      "Idx: 522\tFile: 524.Text-Tuple-Table Towards Information Integration in Text-to-Table Generation via Global Tuple Extraction.pdf\n",
      "Idx: 523\tFile: 376.An Audit on the Perspectives and Challenges of Hallucinations in NLP.pdf\n",
      "Idx: 524\tFile: 1067.DetoxLLM A Framework for Detoxification with Explanations.pdf\n",
      "Idx: 525\tFile: 134.By My Eyes Grounding Multimodal Large Language Models with Sensor Data via Visual Prompting.pdf\n",
      "Idx: 526\tFile: 36.Mitigating the Alignment Tax of RLHF.pdf\n",
      "Idx: 527\tFile: 556.SCOI Syntax-augmented Coverage-based In-context Example Selection for Machine Translation.pdf\n",
      "Idx: 528\tFile: 350.Investigating Mysteries of CoT-Augmented Distillation.pdf\n",
      "Idx: 529\tFile: 625.One2Set + Large Language Model Best Partners for Keyphrase Generation.pdf\n",
      "Idx: 530\tFile: 818.Learn Beyond The Answer Training Language Models with Reflection for Mathematical Reasoning.pdf\n",
      "Idx: 531\tFile: 283.EVEDIT Event-based Knowledge Editing for Deterministic Knowledge Propagation.pdf\n",
      "Idx: 532\tFile: 38.Personality-aware Student Simulation for Conversational Intelligent Tutoring Systems.pdf\n",
      "Idx: 533\tFile: 548.If CLIP Could Talk Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions.pdf\n",
      "Idx: 534\tFile: 908.IM-BERT Enhancing Robustness of BERT through the Implicit Euler Method.pdf\n",
      "Idx: 535\tFile: 274.Bayesian Calibration of Win Rate Estimation with LLM Evaluators.pdf\n",
      "Idx: 536\tFile: 1237.RE-RAG Improving Open-Domain QA Performance and Interpretability with Relevance Estimator in Retrieval-Augmented Generation.pdf\n",
      "Idx: 537\tFile: 664.Advancing Semantic Textual Similarity Modeling A Regression Framework with Translated ReLU and Smooth K2 Loss.pdf\n",
      "Idx: 538\tFile: 136.CHIQ Contextual History Enhancement for Improving Query Rewriting in Conversational Search.pdf\n",
      "Idx: 539\tFile: 43.Successfully Guiding Humans with Imperfect Instructions by Highlighting Potential Errors and Suggesting Corrections.pdf\n",
      "Idx: 540\tFile: 449.“Image, Tell me your story!” Predicting the original meta-context of visual misinformation.pdf\n",
      "Idx: 541\tFile: 887.Hate Personified Investigating the role of LLMs in content moderation.pdf\n",
      "Idx: 542\tFile: 1192.Show and Guide Instructional-Plan Grounded Vision and Language Model.pdf\n",
      "Idx: 543\tFile: 1151.Rethinking the Role of Proxy Rewards in Language Model Alignment.pdf\n",
      "Idx: 544\tFile: 825.Building Resources for Emakhuwa Machine Translation and News Classification Benchmarks.pdf\n",
      "Idx: 545\tFile: 660.Dependency Graph Parsing as Sequence Labeling.pdf\n",
      "Idx: 546\tFile: 1088.A Fast and Sound Tagging Method for Discontinuous Named-Entity Recognition.pdf\n",
      "Idx: 547\tFile: 1183.Connecting the Dots Evaluating Abstract Reasoning Capabilities of LLMs Using the New York Times Connections Word Game.pdf\n",
      "Idx: 548\tFile: 852.Joint Pre-Encoding Representation and Structure Embedding for Efficient and Low-Resource Knowledge Graph Completion.pdf\n",
      "Idx: 549\tFile: 32.On Fake News Detection with LLM Enhanced Semantics Mining.pdf\n",
      "Idx: 550\tFile: 988.Pruning via Merging Compressing LLMs via Manifold Alignment Based Layer Merging.pdf\n",
      "Idx: 551\tFile: 741.Null-Shot Prompting Rethinking Prompting Large Language Models With Hallucination.pdf\n",
      "Idx: 552\tFile: 750.Leveraging Context-Aware Prompting for Commit Message Generation.pdf\n",
      "Idx: 553\tFile: 360.Focused Large Language Models are Stable Many-Shot Learners.pdf\n",
      "Idx: 554\tFile: 470.Towards Verifiable Text Generation with Evolving Memory and Self-Reflection.pdf\n",
      "Idx: 555\tFile: 854.RoCEL Advancing Table Entity Linking through Distinctive Row and Column Contexts.pdf\n",
      "Idx: 556\tFile: 396.Knowledge-Centric Hallucination Detection.pdf\n",
      "Idx: 557\tFile: 601.Distributional Properties of Subword Regularization.pdf\n",
      "Idx: 558\tFile: 377.Discovering Knowledge-Critical Subnetworks in Pretrained Language Models.pdf\n",
      "Idx: 559\tFile: 117.Embedding and Gradient Say Wrong A White-Box Method for Hallucination Detection.pdf\n",
      "Idx: 560\tFile: 7.ImageInWords Unlocking Hyper-Detailed Image Descriptions.pdf\n",
      "Idx: 561\tFile: 619.VLEU a Method for Automatic Evaluation for Generalizability of Text-to-Image Models.pdf\n",
      "Idx: 562\tFile: 774.Contribution of Linguistic Typology to Universal Dependency Parsing An Empirical Investigation.pdf\n",
      "Idx: 563\tFile: 1107.Towards Enhancing Coherence in Extractive Summarization Dataset and Experiments with LLMs.pdf\n",
      "Idx: 564\tFile: 757.OneNet A Fine-Tuning Free Framework for Few-Shot Entity Linking via Large Language Model Prompting.pdf\n",
      "Idx: 565\tFile: 604.GlobeSumm A Challenging Benchmark Towards Unifying Multi-lingual, Cross-lingual and Multi-document News Summarization.pdf\n",
      "Idx: 566\tFile: 202.Is Safer Better The Impact of Guardrails on the Argumentative Strength of LLMs in Hate Speech Countering.pdf\n",
      "Idx: 567\tFile: 1037.Instruction Matters A Simple yet Effective Task Selection for Optimized Instruction Tuning of Specific Tasks.pdf\n",
      "Idx: 568\tFile: 1051.NeuroTrialNER An Annotated Corpus for Neurological Diseases and Therapies in Clinical Trial Registries.pdf\n",
      "Idx: 569\tFile: 672.The Multilingual Alignment Prism Aligning Global and Local Preferences to Reduce Harm.pdf\n",
      "Idx: 570\tFile: 259.IDEAW Robust Neural Audio Watermarking with Invertible Dual-Embedding.pdf\n",
      "Idx: 571\tFile: 342.PSC Extending Context Window of Large Language Models via Phase Shift Calibration.pdf\n",
      "Idx: 572\tFile: 475.Humans or LLMs as the Judge A Study on Judgement Bias.pdf\n",
      "Idx: 573\tFile: 1265.Initialization of Large Language Models via Reparameterization to Mitigate Loss Spikes.pdf\n",
      "Idx: 574\tFile: 183.Autoregressive Pre-Training on Pixels and Texts.pdf\n",
      "Idx: 575\tFile: 430.More Than Catastrophic Forgetting Integrating General Capabilities For Domain-Specific LLMs.pdf\n",
      "Idx: 576\tFile: 829.Leveraging BERT and TFIDF Features for Short Text Clustering via Alignment-Promoting Co-Training.pdf\n",
      "Idx: 577\tFile: 543.Is It Good Data for Multilingual Instruction Tuning or Just Bad Multilingual Evaluation for Large Language Models.pdf\n",
      "Idx: 578\tFile: 1239.Simul-MuST-C Simultaneous Multilingual Speech Translation Corpus Using Large Language Model.pdf\n",
      "Idx: 579\tFile: 337.MTA4DPR Multi-Teaching-Assistants Based Iterative Knowledge Distillation for Dense Passage Retrieval.pdf\n",
      "Idx: 580\tFile: 1047.Collective Critics for Creative Story Generation.pdf\n",
      "Idx: 581\tFile: 916.Exploring the Compositional Deficiency of Large Language Models in Mathematical Reasoning Through Trap Problems.pdf\n",
      "Idx: 582\tFile: 925.Is It Really Long Context if All You Need Is Retrieval Towards Genuinely Difficult Long Context NLP.pdf\n",
      "Idx: 583\tFile: 1153.Beyond Correlation Interpretable Evaluation of Machine Translation Metrics.pdf\n",
      "Idx: 584\tFile: 562.CommonIT Commonality-Aware Instruction Tuning for Large Language Models via Data Partitions.pdf\n",
      "Idx: 585\tFile: 237.When Is Multilinguality a Curse Language Modeling for 250 High- and Low-Resource Languages.pdf\n",
      "Idx: 586\tFile: 220.Text Grafting Near-Distribution Weak Supervision for Minority Classes in Text Classification.pdf\n",
      "Idx: 587\tFile: 177.Zero-shot Cross-Lingual Transfer for Synthetic Data Generation in Grammatical Error Detection.pdf\n",
      "Idx: 588\tFile: 541.Modeling Layout Reading Order as Ordering Relations for Visually-rich Document Understanding.pdf\n",
      "Idx: 589\tFile: 900.CorrSynth - A Correlated Sampling Method for Diverse Dataset Generation from LLMs.pdf\n",
      "Idx: 590\tFile: 1066.Exploring Intra and Inter-language Consistency in Embeddings with ICA.pdf\n",
      "Idx: 591\tFile: 325.Thinking Outside of the Differential Privacy Box A Case Study in Text Privatization with Language Model Prompting.pdf\n",
      "Idx: 592\tFile: 509.On the Relationship between Truth and Political Bias in Language Models.pdf\n",
      "Idx: 593\tFile: 1155.Encoding Spreadsheets for Large Language Models.pdf\n",
      "Idx: 594\tFile: 1197.Synthetic Knowledge Ingestion Towards Knowledge Refinement and Injection for Enhancing Large Language Models.pdf\n",
      "Idx: 595\tFile: 206.CoTKR Chain-of-Thought Enhanced Knowledge Rewriting for Complex Knowledge Graph Question Answering.pdf\n",
      "Idx: 596\tFile: 1222.Accurate and Data-Efficient Toxicity Prediction when Annotators Disagree.pdf\n",
      "Idx: 597\tFile: 473.RealVul Can We Detect Vulnerabilities in Web Applications with LLM.pdf\n",
      "Idx: 598\tFile: 749.DA-Code Agent Data Science Code Generation Benchmark for Large Language Models.pdf\n",
      "Idx: 599\tFile: 49.Making Large Language Models Better Reasoners with Orchestrated Streaming Experiences.pdf\n",
      "Idx: 600\tFile: 677.TL-CL Task And Language Incremental Continual Learning.pdf\n",
      "Idx: 601\tFile: 632.PCQPR Proactive Conversational Question Planning with Reflection.pdf\n",
      "Idx: 602\tFile: 364.ChatGPT Doesn’t Trust Chargers Fans Guardrail Sensitivity in Context.pdf\n",
      "Idx: 603\tFile: 83.Towards Tool Use Alignment of Large Language Models.pdf\n",
      "Idx: 604\tFile: 267.DVD Dynamic Contrastive Decoding for Knowledge Amplification in Multi-Document Question Answering.pdf\n",
      "Idx: 605\tFile: 269.Retrospex Language Agent Meets Offline Reinforcement Learning Critic.pdf\n",
      "Idx: 606\tFile: 163.Message Passing on Semantic-Anchor-Graphs for Fine-grained Emotion Representation Learning and Classification.pdf\n",
      "Idx: 607\tFile: 1040.Semformer Transformer Language Models with Semantic Planning.pdf\n",
      "Idx: 608\tFile: 1223.Adversarial Text Generation using Large Language Models for Dementia Detection.pdf\n",
      "Idx: 609\tFile: 955.Media Attitude Detection via Framing Analysis with Events and their Relations.pdf\n",
      "Idx: 610\tFile: 330.Benchmarking Vision Language Models for Cultural Understanding.pdf\n",
      "Idx: 611\tFile: 947.Mathador-LM A Dynamic Benchmark for Mathematical Reasoning on Large Language Models.pdf\n",
      "Idx: 612\tFile: 602.DataTales A Benchmark for Real-World Intelligent Data Narration.pdf\n",
      "Idx: 613\tFile: 841.LitSearch A Retrieval Benchmark for Scientific Literature Search.pdf\n",
      "Idx: 614\tFile: 474.Unsupervised End-to-End Task-Oriented Dialogue with LLMs The Power of the Noisy Channel.pdf\n",
      "Idx: 615\tFile: 686.RA2FD Distilling Faithfulness into Efficient Dialogue Systems.pdf\n",
      "Idx: 616\tFile: 1117.Continual Test-time Adaptation for End-to-end Speech Recognition on Noisy Speech.pdf\n",
      "Idx: 617\tFile: 1162.Linear Layer Extrapolation for Fine-Grained Emotion Classification.pdf\n",
      "Idx: 618\tFile: 762.Householder Pseudo-Rotation A Novel Approach to Activation Editing in LLMs with Direction-Magnitude Perspective.pdf\n",
      "Idx: 619\tFile: 1248.CELLO Causal Evaluation of Large Vision-Language Models.pdf\n",
      "Idx: 620\tFile: 1186.CoverICL Selective Annotation for In-Context Learning via Active Graph Coverage.pdf\n",
      "Idx: 621\tFile: 932.Still Not Quite There! Evaluating Large Language Models for Comorbid Mental Health Diagnosis.pdf\n",
      "Idx: 622\tFile: 853.Improving Discriminative Capability of Reward Models in RLHF Using Contrastive Learning.pdf\n",
      "Idx: 623\tFile: 1023.GuardBench A Large-Scale Benchmark for Guardrail Models.pdf\n",
      "Idx: 624\tFile: 437.Middleware for LLMs Tools Are Instrumental for Language Agents in Complex Environments.pdf\n",
      "Idx: 625\tFile: 1268.Instruction Fine-Tuning Does Prompt Loss Matter.pdf\n",
      "Idx: 626\tFile: 980.MP2D An Automated Topic Shift Dialogue Generation Framework Leveraging Knowledge Graphs.pdf\n",
      "Idx: 627\tFile: 12.Eyes Don’t Lie Subjective Hate Annotation and Detection with Gaze.pdf\n",
      "Idx: 628\tFile: 884.ESC-Eval Evaluating Emotion Support Conversations in Large Language Models.pdf\n",
      "Idx: 629\tFile: 84.DecorateLM Data Engineering through Corpus Rating, Tagging, and Editing with Language Models.pdf\n",
      "Idx: 630\tFile: 566.Optimizing Language Models with Fair and Stable Reward Composition in Reinforcement Learning.pdf\n",
      "Idx: 631\tFile: 513.DiVERT Distractor Generation with Variational Errors Represented as Text for Math Multiple-choice Questions.pdf\n",
      "Idx: 632\tFile: 276.Seeing the Forest through the Trees Data Leakage from Partial Transformer Gradients.pdf\n",
      "Idx: 633\tFile: 990.Unleashing the Power of Emojis in Texts via Self-supervised Graph Pre-Training.pdf\n",
      "Idx: 634\tFile: 895.FAME Towards Factual Multi-Task Model Editing.pdf\n",
      "Idx: 635\tFile: 696.The Mystery of the Pathological Path-star Task for Language Models.pdf\n",
      "Idx: 636\tFile: 161.Take Off the Training Wheels! Progressive In-Context Learning for Effective Alignment.pdf\n",
      "Idx: 637\tFile: 692.ABSEval An Agent-based Framework for Script Evaluation.pdf\n",
      "Idx: 638\tFile: 213.Learn to Refuse Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal Mechanism.pdf\n",
      "Idx: 639\tFile: 5.Prompts have evil twins.pdf\n",
      "Idx: 640\tFile: 554.Multi-pass Decoding for Grammatical Error Correction.pdf\n",
      "Idx: 641\tFile: 454.Efficient Performance Tracking Leveraging Large Language Models for Automated Construction of Scientific Leaderboards.pdf\n",
      "Idx: 642\tFile: 1087.Video-Text Prompting for Weakly Supervised Spatio-Temporal Video Grounding.pdf\n",
      "Idx: 643\tFile: 589.Bio-RFX Refining Biomedical Extraction via Advanced Relation Classification and Structural Constraints.pdf\n",
      "Idx: 644\tFile: 248.An Analysis of Multilingual FActScore.pdf\n",
      "Idx: 645\tFile: 1012.NoiseBench Benchmarking the Impact of Real Label Noise on Named Entity Recognition.pdf\n",
      "Idx: 646\tFile: 239.MiTTenS A Dataset for Evaluating Gender Mistranslation.pdf\n",
      "Idx: 647\tFile: 123.Decoding the Echoes of Vision from fMRI Memory Disentangling for Past Semantic Information.pdf\n",
      "Idx: 648\tFile: 691.Self-Powered LLM Modality Expansion for Large Speech-Text Models.pdf\n",
      "Idx: 649\tFile: 1045.Evaluating Large Language Models along Dimensions of Language Variation A Systematik Invesdigatiom uv Cross-lingual Generalization.pdf\n",
      "Idx: 650\tFile: 515.CleanGen Mitigating Backdoor Attacks for Generation Tasks in Large Language Models.pdf\n",
      "Idx: 651\tFile: 1211.Rebuilding ROME  Resolving Model Collapse during Sequential Model Editing.pdf\n",
      "Idx: 652\tFile: 326.To Preserve or To Compress An In-Depth Study of Connector Selection in Multimodal Large Language Models.pdf\n",
      "Idx: 653\tFile: 383.MirrorStories Reflecting Diversity through Personalized Narrative Generation with Large Language Models.pdf\n",
      "Idx: 654\tFile: 479.Stepwise Verification and Remediation of Student Reasoning Errors with Large Language Model Tutors.pdf\n",
      "Idx: 655\tFile: 120.Aligning Language Models to Explicitly Handle Ambiguity.pdf\n",
      "Idx: 656\tFile: 493.Exploring Nested Named Entity Recognition with Large Language Models Methods, Challenges, and Insights.pdf\n",
      "Idx: 657\tFile: 865.FineCops-Ref A new Dataset and Task for Fine-Grained Compositional Referring Expression Comprehension.pdf\n",
      "Idx: 658\tFile: 761.Holistic Automated Red Teaming for Large Language Models through Top-Down Test Case Generation and Multi-turn Interaction.pdf\n",
      "Idx: 659\tFile: 69.Rethinking Pruning Large Language Models Benefits and Pitfalls of Reconstruction Error Minimization.pdf\n",
      "Idx: 660\tFile: 1026.LLMEdgeRefine Enhancing Text Clustering with LLM-Based Boundary Point Refinement.pdf\n",
      "Idx: 661\tFile: 775.TRoTR A Framework for Evaluating the Re-contextualization of Text Reuse.pdf\n",
      "Idx: 662\tFile: 521.Ontologically Faithful Generation of Non-Player Character Dialogues.pdf\n",
      "Idx: 663\tFile: 1148.Do LLMs Know to Respect Copyright Notice.pdf\n",
      "Idx: 664\tFile: 1164.SciDQA A Deep Reading Comprehension Dataset over Scientific Papers.pdf\n",
      "Idx: 665\tFile: 1118.Whiteboard-of-Thought Thinking Step-by-Step Across Modalities.pdf\n",
      "Idx: 666\tFile: 448.ECON On the Detection and Resolution of Evidence Conflicts.pdf\n",
      "Idx: 667\tFile: 954.Measuring Psychological Depth in Language Models.pdf\n",
      "Idx: 668\tFile: 327.What is ”Typological Diversity” in NLP.pdf\n",
      "Idx: 669\tFile: 451.Mixture-of-Subspaces in Low-Rank Adaptation.pdf\n",
      "Idx: 670\tFile: 769.Curriculum Consistency Learning for Conditional Sentence Generation.pdf\n",
      "Idx: 671\tFile: 1145.RAt Injecting Implicit Bias for Text-To-Image Prompt Refinement Models.pdf\n",
      "Idx: 672\tFile: 440.DAMRO Dive into the Attention Mechanism of LVLM to Reduce Object Hallucination.pdf\n",
      "Idx: 673\tFile: 910.Exploring Space Efficiency in a Tree-based Linear Model for Extreme Multi-label Classification.pdf\n",
      "Idx: 674\tFile: 232.Private Language Models via Truncated Laplacian Mechanism.pdf\n",
      "Idx: 675\tFile: 467.EmoKnob Enhance Voice Cloning with Fine-Grained Emotion Control.pdf\n",
      "Idx: 676\tFile: 366.Satyrn A Platform for Analytics Augmented Generation.pdf\n",
      "Idx: 677\tFile: 445.Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations.pdf\n",
      "Idx: 678\tFile: 936.Are Large Language Models In-Context Personalized Summarizers Get an iCOPERNICUS Test Done!.pdf\n",
      "Idx: 679\tFile: 298.Induct-Learn Short Phrase Prompting with Instruction Induction.pdf\n",
      "Idx: 680\tFile: 740.Turn Waste into Worth Rectifying Top-k Router of MoE.pdf\n",
      "Idx: 681\tFile: 434.Divide and Conquer Radiology Report Generation via Observation Level Fine-grained Pretraining and Prompt Tuning.pdf\n",
      "Idx: 682\tFile: 328.The Computational Anatomy of Humility Modeling Intellectual Humility in Online Public Discourse.pdf\n",
      "Idx: 683\tFile: 742.CommVQA Situating Visual Question Answering in Communicative Contexts.pdf\n",
      "Idx: 684\tFile: 20.RoTBench A Multi-Level Benchmark for Evaluating the Robustness of Large Language Models in Tool Learning.pdf\n",
      "Idx: 685\tFile: 838.HalluMeasure Fine-grained Hallucination Measurement Using Chain-of-Thought Reasoning.pdf\n",
      "Idx: 686\tFile: 815.DynaThink Fast or Slow A Dynamic Decision-Making Framework for Large Language Models.pdf\n",
      "Idx: 687\tFile: 149.Instruction Pre-Training Language Models are Supervised Multitask Learners.pdf\n",
      "Idx: 688\tFile: 620.Towards Online Continuous Sign Language Recognition and Translation.pdf\n",
      "Idx: 689\tFile: 1241.On the Fragility of Active Learners for Text Classification.pdf\n",
      "Idx: 690\tFile: 699.SLANG New Concept Comprehension of Large Language Models.pdf\n",
      "Idx: 691\tFile: 1060.TV-TREES Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning.pdf\n",
      "Idx: 692\tFile: 906.Rationale-Aware Answer Verification by Pairwise Self-Evaluation.pdf\n",
      "Idx: 693\tFile: 465.FEDKIM Adaptive Federated Knowledge Injection into Medical Foundation Models.pdf\n",
      "Idx: 694\tFile: 14.“Thinking” Fair and Slow On the Efficacy of Structured Prompts for Debiasing Language Models.pdf\n",
      "Idx: 695\tFile: 831.Unsupervised Named Entity Disambiguation for Low Resource Domains.pdf\n",
      "Idx: 696\tFile: 72.ChatRetriever Adapting Large Language Models for Generalized and Robust Conversational Dense Retrieval.pdf\n",
      "Idx: 697\tFile: 518.Efficient Sequential Decision Making with Large Language Models.pdf\n",
      "Idx: 698\tFile: 1229.Holistic Evaluation for Interleaved Text-and-Image Generation.pdf\n",
      "Idx: 699\tFile: 365.Personas as a Way to Model Truthfulness in Language Models.pdf\n",
      "Idx: 700\tFile: 635.MMTE Corpus and Metrics for Evaluating Machine Translation Quality of Metaphorical Language.pdf\n",
      "Idx: 701\tFile: 1093.IntCoOp Interpretability-Aware Vision-Language Prompt Tuning.pdf\n",
      "Idx: 702\tFile: 94.Watch Every Step! LLM Agent Learning via Iterative Step-level Process Refinement.pdf\n",
      "Idx: 703\tFile: 1020.Please note that I’m just an AI Analysis of Behavior Patterns of LLMs in (Non-)offensive Speech Identification.pdf\n",
      "Idx: 704\tFile: 378.Reconstruct Your Previous Conversations! Comprehensively Investigating Privacy Leakage Risks in Conversations with GPT Models.pdf\n",
      "Idx: 705\tFile: 998.Improving Zero-shot LLM Re-Ranker with Risk Minimization.pdf\n",
      "Idx: 706\tFile: 1137.Will LLMs Replace the Encoder-Only Models in Temporal Relation Classification.pdf\n",
      "Idx: 707\tFile: 343.Video-LLaVA Learning United Visual Representation by Alignment Before Projection.pdf\n",
      "Idx: 708\tFile: 1055.Argument Relation Classification through Discourse Markers and Adversarial Training.pdf\n",
      "Idx: 709\tFile: 395.TimeR4  Time-aware Retrieval-Augmented Large Language Models for Temporal Knowledge Graph Question Answering.pdf\n",
      "Idx: 710\tFile: 198.QUIK Towards End-to-end 4-Bit Inference on Generative Large Language Models.pdf\n",
      "Idx: 711\tFile: 953.How Susceptible are Large Language Models to Ideological Manipulation.pdf\n",
      "Idx: 712\tFile: 251.PromptReps Prompting Large Language Models to Generate Dense and Sparse Representations for Zero-Shot Document Retrieval.pdf\n",
      "Idx: 713\tFile: 904.CMR Scaling Law Predicting Critical Mixture Ratios for Continual Pre-training of Language Models.pdf\n",
      "Idx: 714\tFile: 1106.Perceptions to Beliefs Exploring Precursory Inferences for Theory of Mind in Large Language Models.pdf\n",
      "Idx: 715\tFile: 1046.Fuse to Forget Bias Reduction and Selective Memorization through Model Fusion.pdf\n",
      "Idx: 716\tFile: 827.Commonsense Knowledge Editing Based on Free-Text in LLMs.pdf\n",
      "Idx: 717\tFile: 716.Learning to Write Rationally How Information Is Distributed in Non-native Speakers’ Essays.pdf\n",
      "Idx: 718\tFile: 90.UniFashion A Unified Vision-Language Model for Multimodal Fashion Retrieval and Generation.pdf\n",
      "Idx: 719\tFile: 115.Enhancing Advanced Visual Reasoning Ability of Large Language Models.pdf\n",
      "Idx: 720\tFile: 334.Teaching Small Language Models Reasoning through Counterfactual Distillation.pdf\n",
      "Idx: 721\tFile: 978.Mentor-KD Making Small Language Models Better Multi-step Reasoners.pdf\n",
      "Idx: 722\tFile: 893.Mitigating Training Imbalance in LLM Fine-Tuning via Selective Parameter Merging.pdf\n",
      "Idx: 723\tFile: 1070.Emotion Granularity from Text An Aggregate-Level Indicator of Mental Health.pdf\n",
      "Idx: 724\tFile: 235.When Context Leads but Parametric Memory Follows in Large Language Models.pdf\n",
      "Idx: 725\tFile: 210.Toxicity Detection is NOT all you Need Measuring the Gaps to Supporting Volunteer Content Moderators through a User-Centric Method.pdf\n",
      "Idx: 726\tFile: 1078.CaT-Bench Benchmarking Language Model Understanding of Causal and Temporal Dependencies in Plans.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not convert string to float: b'0.000000000000-2842171' : FloatObject (b'0.000000000000-2842171') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.000000000000-5684342' : FloatObject (b'0.000000000000-5684342') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 727\tFile: 525.Toward Compositional Behavior in Neural Models A Survey of Current Views.pdf\n",
      "Idx: 728\tFile: 150.LEMoE Advanced Mixture of Experts Adaptor for Lifelong Model Editing of Large Language Models.pdf\n",
      "Idx: 729\tFile: 484.Perceptions of Linguistic Uncertainty by Language Models and Humans.pdf\n",
      "Idx: 730\tFile: 299.Multi-Granularity History and Entity Similarity Learning for Temporal Knowledge Graph Reasoning.pdf\n",
      "Idx: 731\tFile: 785.KARL Knowledge-Aware Retrieval and Representations aid Retention and Learning in Students.pdf\n",
      "Idx: 732\tFile: 243.I Could’ve Asked That Reformulating Unanswerable Questions.pdf\n",
      "Idx: 733\tFile: 662.Towards a Greek Proverb Atlas Computational Spatial Exploration and Attribution of Greek Proverbs.pdf\n",
      "Idx: 734\tFile: 690.Unveiling In-Context Learning A Coordinate System to Understand Its Working Mechanism.pdf\n",
      "Idx: 735\tFile: 119.Be Helpful but Don’t Talk too Much - Enhancing Helpfulness in Conversations through Relevance in Multi-Turn Emotional Support.pdf\n",
      "Idx: 736\tFile: 1086.Re-Evaluating Evaluation for Multilingual Summarization.pdf\n",
      "Idx: 737\tFile: 1254.Language Models as Compilers Simulating Pseudocode Execution Improves Algorithmic Reasoning in Language Models.pdf\n",
      "Idx: 738\tFile: 580.MixGR Enhancing Retriever Generalization for Scientific Domain through Complementary Granularity.pdf\n",
      "Idx: 739\tFile: 840.Are Large Language Models Good Classifiers A Study on Edit Intent Classification in Scientific Document Revisions.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not convert string to float: b'0.000000000000-2842171' : FloatObject (b'0.000000000000-2842171') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.000000000000-2842171' : FloatObject (b'0.000000000000-2842171') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.000000000000-2842171' : FloatObject (b'0.000000000000-2842171') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.000000000000-2842171' : FloatObject (b'0.000000000000-2842171') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.000000000000-2842171' : FloatObject (b'0.000000000000-2842171') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.000000000000-2842171' : FloatObject (b'0.000000000000-2842171') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.000000000000-2842171' : FloatObject (b'0.000000000000-2842171') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.000000000000-2842171' : FloatObject (b'0.000000000000-2842171') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.000000000000-2842171' : FloatObject (b'0.000000000000-2842171') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.000000000000-2842171' : FloatObject (b'0.000000000000-2842171') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 740\tFile: 800.Distractor Generation in Multiple-Choice Tasks A Survey of Methods, Datasets, and Evaluation.pdf\n",
      "Idx: 741\tFile: 1204.Unveiling the mystery of visual attributes of concrete and abstract concepts Variability, nearest neighbors, and challenging categories.pdf\n",
      "Idx: 742\tFile: 1185.Computational Meme Understanding A Survey.pdf\n",
      "Idx: 743\tFile: 527.Reverse-Engineering the Reader.pdf\n",
      "Idx: 744\tFile: 745.How to Leverage Demonstration Data in Alignment for Large Language Model A Self-Imitation Learning Perspective.pdf\n",
      "Idx: 745\tFile: 204.LLM4Decompile Decompiling Binary Code with Large Language Models.pdf\n",
      "Idx: 746\tFile: 743.Ouroboros Generating Longer Drafts Phrase by Phrase for Faster Speculative Decoding.pdf\n",
      "Idx: 747\tFile: 257.Deciphering Rumors A Multi-Task Learning Approach with Intent-aware Hierarchical Contrastive Learning.pdf\n",
      "Idx: 748\tFile: 30.On the Influence of Gender and Race in Romantic Relationship Prediction from Large Language Models.pdf\n",
      "Idx: 749\tFile: 452.PARIKSHA A Large-Scale Investigation of Human-LLM Evaluator Agreement on Multilingual and Multi-Cultural Data.pdf\n",
      "Idx: 750\tFile: 1091.Adaptable Moral Stances of Large Language Models on Sexist Content Implications for Society and Gender Discourse.pdf\n",
      "Idx: 751\tFile: 503.From RAG to Riches Retrieval Interlaced with Sequence Generation.pdf\n",
      "Idx: 752\tFile: 53.What’s Mine becomes Yours Defining, Annotating and Detecting Context-Dependent Paraphrases in News Interview Dialogs.pdf\n",
      "Idx: 753\tFile: 401.Encoding and Controlling Global Semantics for Long-form Video Question Answering.pdf\n",
      "Idx: 754\tFile: 92.MAR Matching-Augmented Reasoning for Enhancing Visual-based Entity Question Answering.pdf\n",
      "Idx: 755\tFile: 400.Generative Models for Automatic Medical Decision Rule Extraction from Text.pdf\n",
      "Idx: 756\tFile: 1092.DISCERN Decoding Systematic Errors in Natural Language for Text Classifiers.pdf\n",
      "Idx: 757\tFile: 1064.FoodieQA A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture.pdf\n",
      "Idx: 758\tFile: 357.Finer Investigating and Enhancing Fine-Grained Visual Concept Recognition in Large Vision Language Models.pdf\n",
      "Idx: 759\tFile: 191.SEEKR Selective Attention-Guided Knowledge Retention for Continual Learning of Large Language Models.pdf\n",
      "Idx: 760\tFile: 227.PRompt Optimization in Multi-Step Tasks (PROMST) Integrating Human Feedback and Heuristic-based Sampling.pdf\n",
      "Idx: 761\tFile: 1142.AlphaLoRA Assigning LoRA Experts Based on Layer Training Quality.pdf\n",
      "Idx: 762\tFile: 458.Getting More from Less Large Language Models are Good Spontaneous Multilingual Learners.pdf\n",
      "Idx: 763\tFile: 129.LogicAsker Evaluating and Improving the Logical Reasoning Ability of Large Language Models.pdf\n",
      "Idx: 764\tFile: 1215.RECANTFormer Referring Expression Comprehension with Varying Numbers of Targets.pdf\n",
      "Idx: 765\tFile: 962.StorySparkQA Expert-Annotated QA Pairs with Real-World Knowledge for Children’s Story-Based Learning.pdf\n",
      "Idx: 766\tFile: 284.Modeling Nonnative Sentence Processing with L2 Language Models.pdf\n",
      "Idx: 767\tFile: 391.A Survey of AMR Applications.pdf\n",
      "Idx: 768\tFile: 793.Cross-lingual Back-Parsing Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing.pdf\n",
      "Idx: 769\tFile: 821.BiasAlert A Plug-and-play Tool for Social Bias Detection in LLMs.pdf\n",
      "Idx: 770\tFile: 447.MIND Multimodal Shopping Intention Distillation from Large Vision-language Models for E-commerce Purchase Understanding.pdf\n",
      "Idx: 771\tFile: 76.Pre-trained Language Models Do Not Help Auto-regressive Text-to-Image Generation.pdf\n",
      "Idx: 772\tFile: 165.Alignment-Enhanced Decoding Defending Jailbreaks via Token-Level Adaptive Refining of Probability Distributions.pdf\n",
      "Idx: 773\tFile: 546.Evaluating the Effectiveness of Large Language Models in Establishing Conversational Grounding.pdf\n",
      "Idx: 774\tFile: 1133.DEFT-UCS Data Efficient Fine-Tuning for Pre-Trained Language Models via Unsupervised Core-Set Selection for Text-Editing.pdf\n",
      "Idx: 775\tFile: 504.Task Arithmetic can Mitigate Synthetic-to-Real Gap in Automatic Speech Recognition.pdf\n",
      "Idx: 776\tFile: 640.DECOR Improving Coherence in L2 English Writing with a Novel Benchmark for Incoherence Detection, Reasoning, and Rewriting.pdf\n",
      "Idx: 777\tFile: 768.Related Work and Citation Text Generation A Survey.pdf\n",
      "Idx: 778\tFile: 864.SRF Enhancing Document-Level Relation Extraction with a Novel Secondary Reasoning Framework.pdf\n",
      "Idx: 779\tFile: 127.An Inversion Attack Against Obfuscated Embedding Matrix in Language Model Inference.pdf\n",
      "Idx: 780\tFile: 54.Language Models Learn Rare Phenomena from Less Rare Phenomena The Case of the Missing AANNs.pdf\n",
      "Idx: 781\tFile: 758.Don’t Just Say “I don’t know”! Self-aligning Large Language Models for Responding to Unknown Questions with Explanations.pdf\n",
      "Idx: 782\tFile: 47.Let the Expert Stick to His Last Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models.pdf\n",
      "Idx: 783\tFile: 614.Large Language Models Know What is Key Visual Entity An LLM-assisted Multimodal Retrieval for VQA.pdf\n",
      "Idx: 784\tFile: 64.CryptoTrade A Reflective LLM-based Agent to Guide Zero-shot Cryptocurrency Trading.pdf\n",
      "Idx: 785\tFile: 956.Fill In The Gaps Model Calibration and Generalization with Synthetic Data.pdf\n",
      "Idx: 786\tFile: 426.Strategic Demonstration Selection for Improved Fairness in LLM In-Context Learning.pdf\n",
      "Idx: 787\tFile: 1188.I-AM-G Interest Augmented Multimodal Generator for Item Personalization.pdf\n",
      "Idx: 788\tFile: 1112.Step-by-Step Reasoning to Solve Grid Puzzles Where do LLMs Falter.pdf\n",
      "Idx: 789\tFile: 221.Incubating Text Classifiers Following User Instruction with Nothing but LLM.pdf\n",
      "Idx: 790\tFile: 610.Kiss up, Kick down Exploring Behavioral Changes in Multi-modal Large Language Models with Assigned Visual Personas.pdf\n",
      "Idx: 791\tFile: 1002.Tree of Problems Improving structured problem solving with compositionality.pdf\n",
      "Idx: 792\tFile: 1247.SimLLM Detecting Sentences Generated by Large Language Models Using Similarity between the Generation and its Re-generation.pdf\n",
      "Idx: 793\tFile: 1149.SpecHub Provable Acceleration to Multi-Draft Speculative Decoding.pdf\n",
      "Idx: 794\tFile: 783.Enhancing Training Data Attribution for Large Language Models with Fitting Error Consideration.pdf\n",
      "Idx: 795\tFile: 507.PostMark A Robust Blackbox Watermark for Large Language Models.pdf\n",
      "Idx: 796\tFile: 301.Pretraining Data Detection for Large Language Models A Divergence-based Calibration Method.pdf\n",
      "Idx: 797\tFile: 778.How Do Your Code LLMs perform Empowering Code Instruction Tuning with Really Good Data.pdf\n",
      "Idx: 798\tFile: 1174.ArMeme Propagandistic Content in Arabic Memes.pdf\n",
      "Idx: 799\tFile: 939.Working Memory Identifies Reasoning Limits in Language Models.pdf\n",
      "Idx: 800\tFile: 858.Not Everything is All You Need Toward Low-Redundant Optimization for Large Language Model Alignment.pdf\n",
      "Idx: 801\tFile: 651.Reasoning or a Semblance of it A Diagnostic Study of Transitive Reasoning in LLMs.pdf\n",
      "Idx: 802\tFile: 234.Consistent Autoformalization for Constructing Mathematical Libraries.pdf\n",
      "Idx: 803\tFile: 937.MediTOD An English Dialogue Dataset for Medical History Taking with Comprehensive Annotations.pdf\n",
      "Idx: 804\tFile: 417.MAgIC Investigation of Large Language Model Powered Multi-Agent in Cognition, Adaptability, Rationality and Collaboration.pdf\n",
      "Idx: 805\tFile: 371.Optimized Speculative Sampling for GPU Hardware Accelerators.pdf\n",
      "Idx: 806\tFile: 1196.An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models.pdf\n",
      "Idx: 807\tFile: 1195.CompAct Compressing Retrieved Documents Actively for Question Answering.pdf\n",
      "Idx: 808\tFile: 1161.Multi-LogiEval Towards Evaluating Multi-Step Logical Reasoning Ability of Large Language Models.pdf\n",
      "Idx: 809\tFile: 1022.A linguistically-motivated evaluation methodology for unraveling model’s abilities in reading comprehension tasks.pdf\n",
      "Idx: 810\tFile: 719.Make Some Noise Unlocking Language Model Parallel Inference Capability through Noisy Training.pdf\n",
      "Idx: 811\tFile: 19.Uncertainty in Language Models Assessment through Rank-Calibration.pdf\n",
      "Idx: 812\tFile: 539.ArxivDIGESTables Synthesizing Scientific Literature into Tables using Language Models.pdf\n",
      "Idx: 813\tFile: 1181.Adapters Mixup Mixing Parameter-Efficient Adapters to Enhance the Adversarial Robustness of Fine-tuned Pre-trained Text Classifiers.pdf\n",
      "Idx: 814\tFile: 238.Teaching Embodied Reinforcement Learning Agents Informativeness and Diversity of Language Use.pdf\n",
      "Idx: 815\tFile: 982.Searching for Best Practices in Retrieval-Augmented Generation.pdf\n",
      "Idx: 816\tFile: 730.RLHF Can Speak Many Languages Unlocking Multilingual Preference Optimization for LLMs.pdf\n",
      "Idx: 817\tFile: 704.FIRST Teach A Reliable Large Language Model Through Efficient Trustworthy Distillation.pdf\n",
      "Idx: 818\tFile: 313.Investigating LLMs as Voting Assistants via Contextual Augmentation A Case Study on the European Parliament Elections 2024.pdf\n",
      "Idx: 819\tFile: 519.SignCLIP Connecting Text and Sign Language by Contrastive Learning.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not convert string to float: b'0.00-35493716' : FloatObject (b'0.00-35493716') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 820\tFile: 307.How Far Can We Extract Diverse Perspectives from Large Language Models.pdf\n",
      "Idx: 821\tFile: 164.PhiloGPT A Philology-Oriented Large Language Model for Ancient Chinese Manuscripts with Dunhuang as Case Study.pdf\n",
      "Idx: 822\tFile: 382.Can Large Language Models Learn Independent Causal Mechanisms.pdf\n",
      "Idx: 823\tFile: 173.Verification and Refinement of Natural Language Explanations through LLM-Symbolic Theorem Proving.pdf\n",
      "Idx: 824\tFile: 1062.GRIZAL Generative Prior-guided Zero-Shot Temporal Action Localization.pdf\n",
      "Idx: 825\tFile: 752.Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning.pdf\n",
      "Idx: 826\tFile: 1129.VIEWS Entity-Aware News Video Captioning.pdf\n",
      "Idx: 827\tFile: 885.Cultural Conditioning or Placebo On the Effectiveness of Socio-Demographic Prompting.pdf\n",
      "Idx: 828\tFile: 1191.Contrastive Policy Gradient Aligning LLMs on sequence-level scores in a supervised-friendly fashion.pdf\n",
      "Idx: 829\tFile: 8.LLM-Based Agent Society Investigation Collaboration and Confrontation in Avalon Gameplay.pdf\n",
      "Idx: 830\tFile: 883.Towards Measuring and Modeling “Culture” in LLMs A Survey.pdf\n",
      "Idx: 831\tFile: 563.ESC Efficient Speech Coding with Cross-Scale Residual Vector Quantized Transformers.pdf\n",
      "Idx: 832\tFile: 667.Applying Contrastive Learning to Code Vulnerability Type Classification.pdf\n",
      "Idx: 833\tFile: 180.On the Role of Context in Reading Time Prediction.pdf\n",
      "Idx: 834\tFile: 290.Aligning Translation-Specific Understanding to General Understanding in Large Language Models.pdf\n",
      "Idx: 835\tFile: 810.Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector.pdf\n",
      "Idx: 836\tFile: 388.MMNeuron Discovering Neuron-Level Domain-Specific Interpretation in Multimodal Large Language Model.pdf\n",
      "Idx: 837\tFile: 1218.T-FREE Subword Tokenizer-Free Generative LLMs via Sparse Representations for Memory-Efficient Embeddings.pdf\n",
      "Idx: 838\tFile: 322.REAR A Relevance-Aware Retrieval-Augmented Framework for Open-Domain Question Answering.pdf\n",
      "Idx: 839\tFile: 777.Automatically Generated Definitions and their utility for Modeling Word Meaning.pdf\n",
      "Idx: 840\tFile: 29.Clustering and Ranking Diversity-preserved Instruction Selection through Expert-aligned Quality Estimation.pdf\n",
      "Idx: 841\tFile: 321.Synergizing In-context Learning with Hints for End-to-end Task-oriented Dialog Systems.pdf\n",
      "Idx: 842\tFile: 1205.Evaluating Large Language Models on Time Series Feature Understanding A Comprehensive Taxonomy and Benchmark.pdf\n",
      "Idx: 843\tFile: 286.Quality Matters Evaluating Synthetic Data for Tool-Using LLMs.pdf\n",
      "Idx: 844\tFile: 776.Structured Optimal Brain Pruning for Large Language Models.pdf\n",
      "Idx: 845\tFile: 438.MORPHEUS Modeling Role from Personalized Dialogue History by Exploring and Utilizing Latent Space.pdf\n",
      "Idx: 846\tFile: 928.Explicit Memory Learning with Expectation Maximization.pdf\n",
      "Idx: 847\tFile: 792.Pcc-tuning Breaking the Contrastive Learning Ceiling in Semantic Textual Similarity.pdf\n",
      "Idx: 848\tFile: 57.AdaZeta Adaptive Zeroth-Order Tensor-Train Adaption for Memory-Efficient Large Language Models Fine-Tuning.pdf\n",
      "Idx: 849\tFile: 280.How Does the Disclosure of AI Assistance Affect the Perceptions of Writing.pdf\n",
      "Idx: 850\tFile: 45.GeoGPT4V Towards Geometric Multi-modal Large Language Models with Geometric Image Generation.pdf\n",
      "Idx: 851\tFile: 596.Speechworthy Instruction-tuned Language Models.pdf\n",
      "Idx: 852\tFile: 1128.Back to School Translation Using Grammar Books.pdf\n",
      "Idx: 853\tFile: 621.Mitigate Extrinsic Social Bias in Pre-trained Language Models via Continuous Prompts Adjustment.pdf\n",
      "Idx: 854\tFile: 1138.Eliciting In-Context Learning in Vision-Language Models for Videos Through Curated Data Distributional Properties.pdf\n",
      "Idx: 855\tFile: 849.XDetox Text Detoxification with Token-Level Toxicity Explanations.pdf\n",
      "Idx: 856\tFile: 1246.EHRAgent Code Empowers Large Language Models for Few-shot Complex Tabular Reasoning on Electronic Health Records.pdf\n",
      "Idx: 857\tFile: 1159.Bayesian Example Selection Improves In-Context Learning for Speech, Text and Visual Modalities.pdf\n",
      "Idx: 858\tFile: 557.Efficient Temporal Extrapolation of Multimodal Large Language Models with Temporal Grounding Bridge.pdf\n",
      "Idx: 859\tFile: 1165.Mixture-of-Modules Reinventing Transformers as Dynamic Assemblies of Modules.pdf\n",
      "Idx: 860\tFile: 101.Rethinking Token Reduction for State Space Models.pdf\n",
      "Idx: 861\tFile: 1132.DeMPT Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators.pdf\n",
      "Idx: 862\tFile: 258.Visual Prompting in LLMs for Enhancing Emotion Recognition.pdf\n",
      "Idx: 863\tFile: 196.GoldCoin Grounding Large Language Models in Privacy Laws via Contextual Integrity Theory.pdf\n",
      "Idx: 864\tFile: 156.Whispers that Shake Foundations Analyzing and Mitigating False Premise Hallucinations in Large Language Models.pdf\n",
      "Idx: 865\tFile: 755.An Analysis and Mitigation of the Reversal Curse.pdf\n",
      "Idx: 866\tFile: 1217.Do LLMs Plan Like Human Writers Comparing Journalist Coverage of Press Releases with LLMs.pdf\n",
      "Idx: 867\tFile: 997.In-Context Compositional Generalization for Large Vision-Language Models.pdf\n",
      "Idx: 868\tFile: 160.Does Object Grounding Really Reduce Hallucination of Large Vision-Language Models.pdf\n",
      "Idx: 869\tFile: 1126.Updating CLIP to Prefer Descriptions Over Captions.pdf\n",
      "Idx: 870\tFile: 247.When Reasoning Meets Information Aggregation A Case Study with Sports Narratives.pdf\n",
      "Idx: 871\tFile: 505.Learning to Correct for QA Reasoning with Black-box LLMs.pdf\n",
      "Idx: 872\tFile: 886.Text Fluoroscopy Detecting LLM-Generated Text through Intrinsic Features.pdf\n",
      "Idx: 873\tFile: 668.TheoremLlama Transforming General-Purpose LLMs into Lean4 Experts.pdf\n",
      "Idx: 874\tFile: 779.MAIR A Massive Benchmark for Evaluating Instructed Retrieval.pdf\n",
      "Idx: 875\tFile: 1053.Towards Faithful Knowledge Graph Explanation Through Deep Alignment in Commonsense Question Answering.pdf\n",
      "Idx: 876\tFile: 446.Bridging Modalities Enhancing Cross-Modality Hate Speech Detection with Few-Shot In-Context Learning.pdf\n",
      "Idx: 877\tFile: 88.Advancing Event Causality Identification via Heuristic Semantic Dependency Inquiry Network.pdf\n",
      "Idx: 878\tFile: 176.How Hard is this Test Set NLI Characterization by Exploiting Training Dynamics.pdf\n",
      "Idx: 879\tFile: 225.AlignCap Aligning Speech Emotion Captioning to Human Preferences.pdf\n",
      "Idx: 880\tFile: 1061.Unsupervised Extraction of Dialogue Policies from Conversations.pdf\n",
      "Idx: 881\tFile: 373.Democratizing Large Language Models via Personalized Parameter-Efficient Fine-tuning.pdf\n",
      "Idx: 882\tFile: 1030.D3CODE Disentangling Disagreements in Data across Cultures on Offensiveness Detection and Evaluation.pdf\n",
      "Idx: 883\tFile: 62.Bridging Cultures in the Kitchen A Framework and Benchmark for Cross-Cultural Recipe Retrieval.pdf\n",
      "Idx: 884\tFile: 398.Automatic Instruction Evolving for Large Language Models.pdf\n",
      "Idx: 885\tFile: 1115.Which questions should I answer Salience Prediction of Inquisitive Questions.pdf\n",
      "Idx: 886\tFile: 551.Can Transformers Learn n-gram Language Models.pdf\n",
      "Idx: 887\tFile: 870.Understanding Slang with LLMs Modelling Cross-Cultural Nuances through Paraphrasing.pdf\n",
      "Idx: 888\tFile: 184.On Training Data Influence of GPT Models.pdf\n",
      "Idx: 889\tFile: 1243.Comparing Neighbors Together Makes it Easy Jointly Comparing Multiple Candidates for Efficient and Effective Retrieval.pdf\n",
      "Idx: 890\tFile: 616.Self-AMPLIFY Improving Small Language Models with Self Post Hoc Explanations.pdf\n",
      "Idx: 891\tFile: 1048.Surprise! Uniform Information Density Isn’t the Whole Story Predicting Surprisal Contours in Long-form Discourse.pdf\n",
      "Idx: 892\tFile: 1213.Safety Arithmetic A Framework for Test-time Safety Alignment of Language Models by Steering Parameters and Activations.pdf\n",
      "Idx: 893\tFile: 1157.Automatic sentence segmentation of clinical record narratives in real-world data.pdf\n",
      "Idx: 894\tFile: 179.SEER Self-Aligned Evidence Extraction for Retrieval-Augmented Generation.pdf\n",
      "Idx: 895\tFile: 756.Exploring the Practicality of Generative Retrieval on Dynamic Corpora.pdf\n",
      "Idx: 896\tFile: 817.Weak Reward Model Transforms Generative Models into Robust Causal Event Extraction Systems.pdf\n",
      "Idx: 897\tFile: 1187.Retrieval-enriched zero-shot image classification in low-resource domains.pdf\n",
      "Idx: 898\tFile: 351.SciPrompt Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics.pdf\n",
      "Idx: 899\tFile: 1232.Is Child-Directed Speech Effective Training Data for Language Models.pdf\n",
      "Idx: 900\tFile: 1245.MedAdapter Efficient Test-Time Adaptation of Large Language Models Towards Medical Reasoning.pdf\n",
      "Idx: 901\tFile: 344.SaySelf Teaching LLMs to Express Confidence with Self-Reflective Rationales.pdf\n",
      "Idx: 902\tFile: 468.VPTQ Extreme Low-bit Vector Post-Training Quantization for Large Language Models.pdf\n",
      "Idx: 903\tFile: 287.Cross-Domain Audio Deepfake Detection Dataset and Analysis.pdf\n",
      "Idx: 904\tFile: 347.Boosting Scientific Concepts Understanding Can Analogy from Teacher Models Empower Student Models.pdf\n",
      "Idx: 905\tFile: 223.Conditional and Modal Reasoning in Large Language Models.pdf\n",
      "Idx: 906\tFile: 1238.Evaluating Concurrent Robustness of Language Models Across Diverse Challenge Sets.pdf\n",
      "Idx: 907\tFile: 203.Leading Whitespaces of Language Models’ Subword Vocabulary Pose a Confound for Calculating Word Probabilities.pdf\n",
      "Idx: 908\tFile: 1097.Pron vs Prompt Can Large Language Models already Challenge a World-Class Fiction Author at Creative Text Writing.pdf\n",
      "Idx: 909\tFile: 780.Rethinking the Evaluation of In-Context Learning for LLMs.pdf\n",
      "Idx: 910\tFile: 1209.Is this the real life Is this just fantasy The Misleading Success of Simulating Social Interactions With LLMs.pdf\n",
      "Idx: 911\tFile: 428.Is LLM-as-a-Judge Robust Investigating Universal Adversarial Attacks on Zero-shot LLM Assessment.pdf\n",
      "Idx: 912\tFile: 520.APPLS Evaluating Evaluation Metrics for Plain Language Summarization.pdf\n",
      "Idx: 913\tFile: 514.The Factuality Tax of Diversity-Intervened Text-to-Image Generation Benchmark and Fact-Augmented Intervention.pdf\n",
      "Idx: 914\tFile: 253.ARES Alternating Reinforcement Learning and Supervised Fine-Tuning for Enhanced Multi-Modal Chain-of-Thought Reasoning Through Diverse AI Feedback.pdf\n",
      "Idx: 915\tFile: 579.Contextualized Sequence Likelihood Enhanced Confidence Scores for Natural Language Generation.pdf\n",
      "Idx: 916\tFile: 435.SURf Teaching Large Vision-Language Models to Selectively Utilize Retrieved Information.pdf\n",
      "Idx: 917\tFile: 1038.Recurrent Alignment with Hard Attention for Hierarchical Text Rating.pdf\n",
      "Idx: 918\tFile: 1005.Is C4 Dataset Optimal for Pruning An Investigation of Calibration Data for LLM Pruning.pdf\n",
      "Idx: 919\tFile: 636.Revisiting Supertagging for faster HPSG parsing.pdf\n",
      "Idx: 920\tFile: 362.GAMA A Large Audio-Language Model with Advanced Audio Understanding and Complex Reasoning Abilities.pdf\n",
      "Idx: 921\tFile: 1160.Investigating Multilingual Instruction-Tuning Do Polyglot Models Demand for Multilingual Instructions.pdf\n",
      "Idx: 922\tFile: 581.CARER - ClinicAl Reasoning-Enhanced Representation for Temporal Health Risk Prediction.pdf\n",
      "Idx: 923\tFile: 839.Learning to Rank Salient Content for Query-focused Summarization.pdf\n",
      "Idx: 924\tFile: 109.Evaluating Psychological Safety of Large Language Models.pdf\n",
      "Idx: 925\tFile: 1210.A Simple LLM Framework for Long-Range Video Question-Answering.pdf\n",
      "Idx: 926\tFile: 368.EPO Hierarchical LLM Agents with Environment Preference Optimization.pdf\n",
      "Idx: 927\tFile: 609.Diversity Over Size On the Effect of Sample and Topic Sizes for Topic-Dependent Argument Mining Datasets.pdf\n",
      "Idx: 928\tFile: 765.A Systematic Survey and Critical Review on Evaluating Large Language Models Challenges, Limitations, and Recommendations.pdf\n",
      "Idx: 929\tFile: 147.Large Language Model as an Assignment Evaluator Insights, Feedback, and Challenges in a 1000+ Student Course.pdf\n",
      "Idx: 930\tFile: 1235.M3Hop-CoT Misogynous Meme Identification with Multimodal Multi-hop Chain-of-Thought.pdf\n",
      "Idx: 931\tFile: 289.Hierarchical Deconstruction of LLM Reasoning A Graph-Based Framework for Analyzing Knowledge Utilization.pdf\n",
      "Idx: 932\tFile: 142.AutoScraper A Progressive Understanding Web Agent for Web Scraper Generation.pdf\n",
      "Idx: 933\tFile: 336.Quantifying the Gaps Between Translation and Native Perception in Training for Multimodal, Multilingual Retrieval.pdf\n",
      "Idx: 934\tFile: 534.Read Anywhere Pointed Layout-aware GUI Screen Reading with Tree-of-Lens Grounding.pdf\n",
      "Idx: 935\tFile: 1221.Dynamic Rewarding with Prompt Optimization Enables Tuning-free Self-Alignment of Language Models.pdf\n",
      "Idx: 936\tFile: 73.Fairer Preferences Elicit Improved Human-Aligned Large Language Model Judgments.pdf\n",
      "Idx: 937\tFile: 623.Integrating Argumentation and Hate-Speech-based Techniques for Countering Misinformation.pdf\n",
      "Idx: 938\tFile: 709.Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach.pdf\n",
      "Idx: 939\tFile: 649.Red Teaming Language Models for Processing Contradictory Dialogues.pdf\n",
      "Idx: 940\tFile: 1212.Casablanca Data and Models for Multidialectal Arabic Speech Recognition.pdf\n",
      "Idx: 941\tFile: 798.Vision-Language Model Fine-Tuning via Simple Parameter-Efficient Modification.pdf\n",
      "Idx: 942\tFile: 1032.Annotator-Centric Active Learning for Subjective NLP Tasks.pdf\n",
      "Idx: 943\tFile: 599.Demystifying Verbatim Memorization in Large Language Models.pdf\n",
      "Idx: 944\tFile: 95.Standardize Aligning Language Models with Expert-Defined Standards for Content Generation.pdf\n",
      "Idx: 945\tFile: 1000.Label Confidence Weighted Learning for Target-level Sentence Simplification.pdf\n",
      "Idx: 946\tFile: 22.Scaling Properties of Speech Language Models.pdf\n",
      "Idx: 947\tFile: 1006.Revisiting the Robustness of Watermarking to Paraphrasing Attacks.pdf\n",
      "Idx: 948\tFile: 1230.FOLIO Natural Language Reasoning with First-Order Logic.pdf\n",
      "Idx: 949\tFile: 1190.Enhancing Language Model Alignment A Confidence-Based Approach to Label Smoothing.pdf\n",
      "Idx: 950\tFile: 48.LongEmbed Extending Embedding Models for Long Context Retrieval.pdf\n",
      "Idx: 951\tFile: 1110.Not All Contexts Are Equal Teaching LLMs Credibility-aware Generation.pdf\n",
      "Idx: 952\tFile: 345.Mitigating Frequency Bias and Anisotropy in Language Model Pre-Training with Syntactic Smoothing.pdf\n",
      "Idx: 953\tFile: 805.DC-Instruct An Effective Framework for Generative Multi-intent Spoken Language Understanding.pdf\n",
      "Idx: 954\tFile: 550.MQuinE a Cure for “Z-paradox” in Knowledge Graph Embedding.pdf\n",
      "Idx: 955\tFile: 226.Interpretability-based Tailored Knowledge Editing in Transformers.pdf\n",
      "Idx: 956\tFile: 185.Understanding “Democratization” in NLP and ML Research.pdf\n",
      "Idx: 957\tFile: 346.ToxiCloakCN Evaluating Robustness of Offensive Language Detection in Chinese with Cloaking Perturbations.pdf\n",
      "Idx: 958\tFile: 724.Story Morals Surfacing value-driven narrative schemas using large language models.pdf\n",
      "Idx: 959\tFile: 265.PsyGUARD An Automated System for Suicide Detection and Risk Assessment in Psychological Counseling.pdf\n",
      "Idx: 960\tFile: 786.Large Language Models Can Be Contextual Privacy Protection Learners.pdf\n",
      "Idx: 961\tFile: 547.Unlocking Memorization in Large Language Models with Dynamic Soft Prompting.pdf\n",
      "Idx: 962\tFile: 949.One Thousand and One Pairs A “novel” challenge for long-context language models.pdf\n",
      "Idx: 963\tFile: 766.Consecutive Batch Model Editing with HooK Layers.pdf\n",
      "Idx: 964\tFile: 1031.PALM Few-Shot Prompt Learning for Audio Language Models.pdf\n",
      "Idx: 965\tFile: 1253.ABLE Personalized Disability Support with Politeness and Empathy Integration.pdf\n",
      "Idx: 966\tFile: 1194.QuBE Question-based Belief Enhancement for Agentic LLM Reasoning.pdf\n",
      "Idx: 967\tFile: 855.Exploring the Role of Reasoning Structures for Constructing Proofs in Multi-Step Natural Language Reasoning with Large Language Models.pdf\n",
      "Idx: 968\tFile: 591.LLMs Are Prone to Fallacies in Causal Inference.pdf\n",
      "Idx: 969\tFile: 909.Distract Large Language Models for Automatic Jailbreak Attack.pdf\n",
      "Idx: 970\tFile: 320.Scaling Laws Across Model Architectures A Comparative Analysis of Dense and MoE Models in Large Language Models.pdf\n",
      "Idx: 971\tFile: 860.ECCO Can We Improve Model-Generated Code Efficiency Without Sacrificing Functional Correctness.pdf\n",
      "Idx: 972\tFile: 594.When Generative Adversarial Networks Meet Sequence Labeling Challenges.pdf\n",
      "Idx: 973\tFile: 973.Dual-oriented Disentangled Network with Counterfactual Intervention for Multimodal Intent Detection.pdf\n",
      "Idx: 974\tFile: 700.Towards Interpretable Sequence Continuation Analyzing Shared Circuits in Large Language Models.pdf\n",
      "Idx: 975\tFile: 182.From Insights to Actions The Impact of Interpretability and Analysis Research on NLP.pdf\n",
      "Idx: 976\tFile: 1085.The Illusion of Competence Evaluating the Effect of Explanations on Users’ Mental Models of Visual Question Answering Systems.pdf\n",
      "Idx: 977\tFile: 573.PreAlign Boosting Cross-Lingual Transfer by Early Establishment of Multilingual Alignment.pdf\n",
      "Idx: 978\tFile: 71.AgentReview Exploring Peer Review Dynamics with LLM Agents.pdf\n",
      "Idx: 979\tFile: 116.CMD a framework for Context-aware Model self-Detoxification.pdf\n",
      "Idx: 980\tFile: 627.ORPO Monolithic Preference Optimization without Reference Model.pdf\n",
      "Idx: 981\tFile: 359.VLFeedback A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment.pdf\n",
      "Idx: 982\tFile: 369.Detection and Measurement of Syntactic Templates in Generated Text.pdf\n",
      "Idx: 983\tFile: 999.Game on Tree Visual Hallucination Mitigation via Coarse-to-Fine View Tree and Game Theory.pdf\n",
      "Idx: 984\tFile: 429.Rethinking the Reversal Curse of LLMs a Prescription from Human Knowledge Reversal.pdf\n",
      "Idx: 985\tFile: 836.GRASS Compute Efficient Low-Memory LLM Training with Structured Sparse Gradients.pdf\n",
      "Idx: 986\tFile: 405.Liar, Liar, Logical Mire A Benchmark for Suppositional Reasoning in Large Language Models.pdf\n",
      "Idx: 987\tFile: 386.From Local Concepts to Universals Evaluating the Multicultural Understanding of Vision-Language Models.pdf\n",
      "Idx: 988\tFile: 1139.Waterfall Scalable Framework for Robust Text Watermarking and Provenance for LLMs.pdf\n",
      "Idx: 989\tFile: 463.Language-to-Code Translation with a Single Labeled Example.pdf\n",
      "Idx: 990\tFile: 1103.Error Analysis of Multilingual Language Models in Machine Translation A Case Study of English-Amharic Translation.pdf\n",
      "Idx: 991\tFile: 485.Explaining and Improving Contrastive Decoding by Extrapolating the Probabilities of a Huge and Hypothetical LM.pdf\n",
      "Idx: 992\tFile: 1102.Threshold-driven Pruning with Segmented Maximum Term Weights for Approximate Cluster-based Sparse Retrieval.pdf\n",
      "Idx: 993\tFile: 770.A Systematic Analysis of Large Language Models as Soft Reasoners The Case of Syllogistic Inferences.pdf\n",
      "Idx: 994\tFile: 262.Multiple Sources are Better Than One Incorporating External Knowledge in Low-Resource Glossing.pdf\n",
      "Idx: 995\tFile: 941.LLM-Evolve Evaluation for LLM’s Evolving Capability on Benchmarks.pdf\n",
      "Idx: 996\tFile: 131.FuseGen PLM Fusion for Data-generation based Zero-shot Learning.pdf\n",
      "Idx: 997\tFile: 309.An LLM Feature-based Framework for Dialogue Constructiveness Assessment.pdf\n",
      "Idx: 998\tFile: 687.Subjective Topic meets LLMs Unleashing Comprehensive, Reflective and Creative Thinking through the Negation of Negation.pdf\n",
      "Idx: 999\tFile: 79.Model Balancing Helps Low-data Training and Fine-tuning.pdf\n",
      "Idx: 1000\tFile: 431.Muting Whisper A Universal Acoustic Adversarial Attack on Speech Foundation Models.pdf\n",
      "Idx: 1001\tFile: 923.Multi-Level Information Retrieval Augmented Generation for Knowledge-based Visual Question Answering.pdf\n",
      "Idx: 1002\tFile: 874.ERVQA A Dataset to Benchmark the Readiness of Large Vision Language Models in Hospital Environments.pdf\n",
      "Idx: 1003\tFile: 370.UOUO Uncontextualized Uncommon Objects for Measuring Knowledge Horizons of Vision Language Models.pdf\n",
      "Idx: 1004\tFile: 976.LLoCO Learning Long Contexts Offline.pdf\n",
      "Idx: 1005\tFile: 796.The Mystery of In-Context Learning A Comprehensive Survey on Interpretation and Analysis.pdf\n",
      "Idx: 1006\tFile: 715.Large Language Models Can Self-Correct with Key Condition Verification.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not convert string to float: b'0.000-4304' : FloatObject (b'0.000-4304') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.000-4304' : FloatObject (b'0.000-4304') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.00000-11296588' : FloatObject (b'0.00000-11296588') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 1007\tFile: 381.Understanding and Mitigating Language Confusion in LLMs.pdf\n",
      "Idx: 1008\tFile: 252.Voices Unheard NLP Resources and Models for Yorùbá Regional Dialects.pdf\n",
      "Idx: 1009\tFile: 1083.CoCoST Automatic Complex Code Generation with Online Searching and Correctness Testing.pdf\n",
      "Idx: 1010\tFile: 961.FlipGuard Defending Preference Alignment against Update Regression with Constrained Optimization.pdf\n",
      "Idx: 1011\tFile: 498.Jellyfish Instruction-Tuning Local Large Language Models for Data Preprocessing.pdf\n",
      "Idx: 1012\tFile: 480.On Eliciting Syntax from Language Models via Hashing.pdf\n",
      "Idx: 1013\tFile: 273.A Peek into Token Bias Large Language Models Are Not Yet Genuine Reasoners.pdf\n",
      "Idx: 1014\tFile: 141.In Search of the Long-Tail Systematic Generation of Long-Tail Inferential Knowledge via Logical Rule Guided Search.pdf\n",
      "Idx: 1015\tFile: 244.STOP! Benchmarking Large Language Models with Sensitivity Testing on Offensive Progressions.pdf\n",
      "Idx: 1016\tFile: 1094.Scope-enhanced Compositional Semantic Parsing for DRT.pdf\n",
      "Idx: 1017\tFile: 195.Pixology Probing the Linguistic and Visual Capabilities of Pixel-based Language Models.pdf\n",
      "Idx: 1018\tFile: 450.Improving Retrieval-augmented Text-to-SQL with AST-based Ranking and Schema Pruning.pdf\n",
      "Idx: 1019\tFile: 795.Are LLMs Good Zero-Shot Fallacy Classifiers.pdf\n",
      "Idx: 1020\tFile: 361.Reconsidering Sentence-Level Sign Language Translation.pdf\n",
      "Idx: 1021\tFile: 1177.Scalable Data Ablation Approximations for Language Models through Modular Training and Merging.pdf\n",
      "Idx: 1022\tFile: 249.Prometheus 2 An Open Source Language Model Specialized in Evaluating Other Language Models.pdf\n",
      "Idx: 1023\tFile: 394.Taylor Unswift Secured Weight Release for Large Language Models via Taylor Expansion.pdf\n",
      "Idx: 1024\tFile: 523.RuBLiMP Russian Benchmark of Linguistic Minimal Pairs.pdf\n",
      "Idx: 1025\tFile: 1014.PairDistill Pairwise Relevance Distillation for Dense Retrieval.pdf\n",
      "Idx: 1026\tFile: 868.DocEdit-v2 Document Structure Editing Via Multimodal LLM Grounding.pdf\n",
      "Idx: 1027\tFile: 114.Do We Need Language-Specific Fact-Checking Models The Case of Chinese.pdf\n",
      "Idx: 1028\tFile: 1258.Nearest Neighbor Normalization Improves Multimodal Retrieval.pdf\n",
      "Idx: 1029\tFile: 1113.Reasoning in Token Economies Budget-Aware Evaluation of LLM Reasoning Strategies.pdf\n",
      "Idx: 1030\tFile: 293.LLMs Assist NLP Researchers Critique Paper (Meta-)Reviewing.pdf\n",
      "Idx: 1031\tFile: 1054.Generation with Dynamic Vocabulary.pdf\n",
      "Idx: 1032\tFile: 34.Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection.pdf\n",
      "Idx: 1033\tFile: 788.Mixture-of-Skills Learning to Optimize Data Usage for Fine-Tuning Large Language Models.pdf\n",
      "Idx: 1034\tFile: 275.MuMath-Code Combining Tool-Use Large Language Models with Multi-perspective Data Augmentation for Mathematical Reasoning.pdf\n",
      "Idx: 1035\tFile: 987.From Descriptive Richness to Bias Unveiling the Dark Side of Generative Image Caption Enrichment.pdf\n",
      "Idx: 1036\tFile: 714.ModSCAN Measuring Stereotypical Bias in Large Vision-Language Models from Vision and Language Modalities.pdf\n",
      "Idx: 1037\tFile: 24.An Experimental Analysis on Evaluating Patent Citations.pdf\n",
      "Idx: 1038\tFile: 1018.Simultaneous Masking, Not Prompting Optimization A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation.pdf\n",
      "Idx: 1039\tFile: 1134.Unveiling Multi-level and Multi-modal Semantic Representations in the Human Brain using Large Language Models.pdf\n",
      "Idx: 1040\tFile: 933.The Odyssey of Commonsense Causality From Foundational Benchmarks to Cutting-Edge Reasoning.pdf\n",
      "Idx: 1041\tFile: 186.DocKD Knowledge Distillation from LLMs for Open-World Document Understanding Models.pdf\n",
      "Idx: 1042\tFile: 10.Speaking in Wavelet Domain A Simple and Efficient Approach to Speed up Speech Diffusion Model.pdf\n",
      "Idx: 1043\tFile: 23.“We Demand Justice!” Towards Social Context Grounding of Political Texts.pdf\n",
      "Idx: 1044\tFile: 587.Belief Revision The Adaptability of Large Language Models Reasoning.pdf\n",
      "Idx: 1045\tFile: 981.Can Large Language Models Enhance Predictions of Disease Progression Investigating Through Disease Network Link Prediction.pdf\n",
      "Idx: 1046\tFile: 585.Reasoning Robustness of LLMs to Adversarial Typographical Errors.pdf\n",
      "Idx: 1047\tFile: 349.Do Large Language Models Know How Much They Know.pdf\n",
      "Idx: 1048\tFile: 197.Noise, Novels, Numbers. A Framework for Detecting and Categorizing Noise in Danish and Norwegian Literature.pdf\n",
      "Idx: 1049\tFile: 733.BEEAR Embedding-based Adversarial Removal of Safety Backdoors in Instruction-tuned Language Models.pdf\n",
      "Idx: 1050\tFile: 477.Walking in Others’ Shoes How Perspective-Taking Guides Large Language Models in Reducing Toxicity and Bias.pdf\n",
      "Idx: 1051\tFile: 358.Evaluating LLMs for Targeted Concept Simplification for Domain-Specific Texts.pdf\n",
      "Idx: 1052\tFile: 200.EfficientRAG Efficient Retriever for Multi-Hop Question Answering.pdf\n",
      "Idx: 1053\tFile: 224.Advancing Large Language Model Attribution through Self-Improving.pdf\n",
      "Idx: 1054\tFile: 847.Decoding Susceptibility Modeling Misbelief to Misinformation Through a Computational Approach.pdf\n",
      "Idx: 1055\tFile: 1233.RevMUX Data Multiplexing with Reversible Adapters for Efficient LLM Batch Inference.pdf\n",
      "Idx: 1056\tFile: 423.EAGLE-2 Faster Inference of Language Models with Dynamic Draft Trees.pdf\n",
      "Idx: 1057\tFile: 844.AKEW Assessing Knowledge Editing in the Wild.pdf\n",
      "Idx: 1058\tFile: 60.HEART-felt Narratives Tracing Empathy and Narrative Style in Personal Stories with LLMs.pdf\n",
      "Idx: 1059\tFile: 324.On Mitigating Performance Disparities in Multilingual Speech Recognition.pdf\n",
      "Idx: 1060\tFile: 443.An Empirical Study of Multilingual Reasoning Distillation for Question Answering.pdf\n",
      "Idx: 1061\tFile: 106.HELPD Mitigating Hallucination of LVLMs by Hierarchical Feedback Learning with Vision-enhanced Penalty Decoding.pdf\n",
      "Idx: 1062\tFile: 712.PATIENT-𝜓 Using Large Language Models to Simulate Patients for Training Mental Health Professionals.pdf\n",
      "Idx: 1063\tFile: 1182.Generalizing Clinical De-identification Models by Privacy-safe Data Augmentation using GPT-4.pdf\n",
      "Idx: 1064\tFile: 393.CareCorpus+ Expanding and Augmenting Caregiver Strategy Data to Support Pediatric Rehabilitation.pdf\n",
      "Idx: 1065\tFile: 1251.MIBench Evaluating Multimodal Large Language Models over Multiple Images.pdf\n",
      "Idx: 1066\tFile: 835.Shortcuts Arising from Contrast Towards Effective and Lightweight Clean-Label Attacks in Prompt-Based Learning.pdf\n",
      "Idx: 1067\tFile: 630.Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs.pdf\n",
      "Idx: 1068\tFile: 1116.Revealing Personality Traits A New Benchmark Dataset for Explainable Personality Recognition on Dialogues.pdf\n",
      "Idx: 1069\tFile: 897.Leveraging Large Language Models for NLG Evaluation Advances and Challenges.pdf\n",
      "Idx: 1070\tFile: 192.Neuron-Level Knowledge Attribution in Large Language Models.pdf\n",
      "Idx: 1071\tFile: 292.Concept-skill Transferability-based Data Selection for Large Vision-Language Models.pdf\n",
      "Idx: 1072\tFile: 146.Reusing Transferable Weight Increments for Low-resource Style Generation.pdf\n",
      "Idx: 1073\tFile: 794.Shaking Up VLMs Comparing Transformers and Structured State Space Models for Vision & Language Modeling.pdf\n",
      "Idx: 1074\tFile: 1056.Getting The Most Out of Your Training Data Exploring Unsupervised Tasks for Morphological Inflection.pdf\n",
      "Idx: 1075\tFile: 837.RaTEScore A Metric for Radiology Report Generation.pdf\n",
      "Idx: 1076\tFile: 1080.An Empirical Analysis of the Writing Styles of Persona-Assigned LLMs.pdf\n",
      "Idx: 1077\tFile: 403.Enhancing Legal Case Retrieval via Scaling High-quality Synthetic Query-Candidate Pairs.pdf\n",
      "Idx: 1078\tFile: 726.AnaloBench Benchmarking the Identification of Abstract and Long-context Analogies.pdf\n",
      "Idx: 1079\tFile: 472.Resampled Datasets Are Not Enough Mitigating Societal Bias Beyond Single Attributes.pdf\n",
      "Idx: 1080\tFile: 1109.Adaptation Odyssey in LLMs Why Does Additional Pretraining Sometimes Fail to Improve.pdf\n",
      "Idx: 1081\tFile: 341.C-LLM Learn to Check Chinese Spelling Errors Character by Character.pdf\n",
      "Idx: 1082\tFile: 96.Cross-domain NER with Generated Task-Oriented Knowledge An Empirical Study from Information Density Perspective.pdf\n",
      "Idx: 1083\tFile: 281.An Unsupervised Approach to Achieve Supervised-Level Explainability in Healthcare Records.pdf\n",
      "Idx: 1084\tFile: 433.XplainLLM A Knowledge-Augmented Dataset for Reliable Grounded Explanations in LLMs.pdf\n",
      "Idx: 1085\tFile: 41.Tokenization Is More Than Compression.pdf\n",
      "Idx: 1086\tFile: 1024.Generate-on-Graph Treat LLM as both Agent and KG for Incomplete Knowledge Graph Question Answering.pdf\n",
      "Idx: 1087\tFile: 1099.Evaluating Short-Term Temporal Fluctuations of Social Biases in Social Media Data and Masked Language Models.pdf\n",
      "Idx: 1088\tFile: 39.MSI-Agent Incorporating Multi-Scale Insight into Embodied Agents for Superior Planning and Decision-Making.pdf\n",
      "Idx: 1089\tFile: 706.Enhancing AI Assisted Writing with One-Shot Implicit Negative Feedback.pdf\n",
      "Idx: 1090\tFile: 876.Improve Student’s Reasoning Generalizability through Cascading Decomposed CoTs Distillation.pdf\n",
      "Idx: 1091\tFile: 501.Beyond Label Attention Transparency in Language Models for Automated Medical Coding via Dictionary Learning.pdf\n",
      "Idx: 1092\tFile: 650.Fishing for Magikarp Automatically Detecting Under-trained Tokens in Large Language Models.pdf\n",
      "Idx: 1093\tFile: 682.Where Am I From Identifying Origin of LLM-generated Content.pdf\n",
      "Idx: 1094\tFile: 657.LoRA-Guard Parameter-Efficient Guardrail Adaptation for Content Moderation of Large Language Models.pdf\n",
      "Idx: 1095\tFile: 1189.Twists, Humps, and Pebbles Multilingual Speech Recognition Models Exhibit Gender Performance Gaps.pdf\n",
      "Idx: 1096\tFile: 922.Towards a Similarity-adjusted Surprisal Theory.pdf\n",
      "Idx: 1097\tFile: 408.Taxonomy-guided Semantic Indexing for Academic Paper Search.pdf\n",
      "Idx: 1098\tFile: 264.Bootstrapped Policy Learning for Task-oriented Dialogue through Goal Shaping.pdf\n",
      "Idx: 1099\tFile: 569.On the In-context Generation of Language Models.pdf\n",
      "Idx: 1100\tFile: 392.Beyond Embeddings The Promise of Visual Table in Visual Reasoning.pdf\n",
      "Idx: 1101\tFile: 288.MaPPER Multimodal Prior-guided Parameter Efficient Tuning for Referring Expression Comprehension.pdf\n",
      "Idx: 1102\tFile: 851.Control Large Language Models via Divide and Conquer.pdf\n",
      "Idx: 1103\tFile: 813.Social Bias Probing Fairness Benchmarking for Language Models.pdf\n",
      "Idx: 1104\tFile: 555.Into the Unknown Unknowns Engaged Human Learning through Participation in Language Model Agent Conversations.pdf\n",
      "Idx: 1105\tFile: 642.PrExMe! Large Scale Prompt Exploration of Open Source LLMs for Machine Translation and Summarization Evaluation.pdf\n",
      "Idx: 1106\tFile: 490.A Thorough Examination of Decoding Methods in the Era of LLMs.pdf\n",
      "Idx: 1107\tFile: 945.On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding What Matters in Reading and Reasoning.pdf\n",
      "Idx: 1108\tFile: 255.VIMI Grounding Video Generation through Multi-modal Instruction.pdf\n",
      "Idx: 1109\tFile: 1146.Can LLM Generate Culturally Relevant Commonsense QA Data Case Study in Indonesian and Sundanese.pdf\n",
      "Idx: 1110\tFile: 13.NumeroLogic Number Encoding for Enhanced LLMs’ Numerical Reasoning.pdf\n",
      "Idx: 1111\tFile: 1260.LongRAG A Dual-Perspective Retrieval-Augmented Generation Paradigm for Long-Context Question Answering.pdf\n",
      "Idx: 1112\tFile: 983.Moral Foundations of Large Language Models.pdf\n",
      "Idx: 1113\tFile: 544.Token Erasure as a Footprint of Implicit Vocabulary Items in LLMs.pdf\n",
      "Idx: 1114\tFile: 1144.Advancing Social Intelligence in AI Agents Technical Challenges and Open Questions.pdf\n",
      "Idx: 1115\tFile: 676.Unknown Claims Generation of Fact-Checking Training Examples from Unstructured and Structured Data.pdf\n",
      "Idx: 1116\tFile: 1101.Grounding Language in Multi-Perspective Referential Communication.pdf\n",
      "Idx: 1117\tFile: 811.Interpretable Composition Attribution Enhancement for Visio-linguistic Compositional Understanding.pdf\n",
      "Idx: 1118\tFile: 263.Adaptive Immune-based Sound-Shape Code Substitution for Adversarial Chinese Text Attacks.pdf\n",
      "Idx: 1119\tFile: 261.Outcome-Constrained Large Language Models for Countering Hate Speech.pdf\n",
      "Idx: 1120\tFile: 530.Less is More Parameter-Efficient Selection of Intermediate Tasks for Transfer Learning.pdf\n",
      "Idx: 1121\tFile: 873.Adaptive Axes A Pipeline for In-domain Social Stereotype Analysis.pdf\n",
      "Idx: 1122\tFile: 1007.A Survey of Ontology Expansion for Conversational Understanding.pdf\n",
      "Idx: 1123\tFile: 797.More DWUGs Extending and Evaluating Word Usage Graph Datasets in Multiple Languages.pdf\n",
      "Idx: 1124\tFile: 404.Does Large Language Model Contain Task-Specific Neurons.pdf\n",
      "Idx: 1125\tFile: 333.Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models.pdf\n",
      "Idx: 1126\tFile: 848.Layer by Layer Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models.pdf\n",
      "Idx: 1127\tFile: 671.PANDA Persona Attributes Navigation for Detecting and Alleviating Overuse Problem in Large Language Models.pdf\n",
      "Idx: 1128\tFile: 823.A Probability–Quality Trade-off in Aligned Language Models and its Relation to Sampling Adaptors.pdf\n",
      "Idx: 1129\tFile: 974.From LLMs to MLLMs Exploring the Landscape of Multimodal Jailbreaking.pdf\n",
      "Idx: 1130\tFile: 17.Systematic Biases in LLM Simulations of Debates.pdf\n",
      "Idx: 1131\tFile: 801.Evaluating n-Gram Novelty of Language Models Using Rusty-DAWG.pdf\n",
      "Idx: 1132\tFile: 782.Hopping Too Late Exploring the Limitations of Large Language Models on Multi-Hop Queries.pdf\n",
      "Idx: 1133\tFile: 748.Grasping the Essentials Tailoring Large Language Models for Zero-Shot Relation Extraction.pdf\n",
      "Idx: 1134\tFile: 37.Evaluating Readability and Faithfulness of Concept-based Explanations.pdf\n",
      "Idx: 1135\tFile: 1206.Can LLMs Learn Uncertainty on Their Own Expressing Uncertainty Effectively in A Self-Training Manner.pdf\n",
      "Idx: 1136\tFile: 302.Scaling Synthetic Logical Reasoning Datasets with Context-Sensitive Declarative Grammars.pdf\n",
      "Idx: 1137\tFile: 416.Leveraging pre-trained language models for linguistic analysis A case of argument structure constructions.pdf\n",
      "Idx: 1138\tFile: 526.Optimizing Instructions and Demonstrations for Multi-Stage Language Model Programs.pdf\n",
      "Idx: 1139\tFile: 512.Annotation alignment Comparing LLM and human annotations of conversational safety.pdf\n",
      "Idx: 1140\tFile: 590.Decoding Matters Addressing Amplification Bias and Homogeneity Issue in Recommendations for Large Language Models.pdf\n",
      "Idx: 1141\tFile: 940.RAFT Realistic Attacks to Fool Text Detectors.pdf\n",
      "Idx: 1142\tFile: 107.TopViewRS Vision-Language Models as Top-View Spatial Reasoners.pdf\n",
      "Idx: 1143\tFile: 1169.ApiQ Finetuning of 2-Bit Quantized Large Language Model.pdf\n",
      "Idx: 1144\tFile: 1049.Model-based Preference Optimization in Abstractive Summarization without Human Feedback.pdf\n",
      "Idx: 1145\tFile: 522.LLM See, LLM Do Leveraging Active Inheritance to Target Non-Differentiable Objectives.pdf\n",
      "Idx: 1146\tFile: 58.RoseLoRA Row and Column-wise Sparse Low-rank Adaptation of Pre-trained Language Model for Knowledge Editing and Fine-tuning.pdf\n",
      "Idx: 1147\tFile: 1033.On the Proper Treatment of Tokenization in Psycholinguistics.pdf\n",
      "Idx: 1148\tFile: 15.A Usage-centric Take on Intent Understanding in E-Commerce.pdf\n",
      "Idx: 1149\tFile: 764.Preserving Generalization of Language models in Few-shot Continual Relation Extraction.pdf\n",
      "Idx: 1150\tFile: 787.A SMART Mnemonic Sounds like “Glue Tonic” Mixing LLMs with Student Feedback to Make Mnemonic Learning Stick.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not convert string to float: b'0.000000000000-5684342' : FloatObject (b'0.000000000000-5684342') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.00000000000-11368684' : FloatObject (b'0.00000000000-11368684') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.000000000000-5684342' : FloatObject (b'0.000000000000-5684342') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.00000000000-11368684' : FloatObject (b'0.00000000000-11368684') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.00000000000-11368684' : FloatObject (b'0.00000000000-11368684') invalid; use 0.0 instead\n",
      "could not convert string to float: b'0.00000000000-11368684' : FloatObject (b'0.00000000000-11368684') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 1151\tFile: 1236.GPT-4 Jailbreaks Itself with Near-Perfect Success Using Self-Explanation.pdf\n",
      "Idx: 1152\tFile: 934.Investigating Large Language Models for Complex Word Identification in Multilingual and Multidomain Setups.pdf\n",
      "Idx: 1153\tFile: 975.Symbolic Working Memory Enhances Language Models for Complex Rule Application.pdf\n",
      "Idx: 1154\tFile: 843.LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law.pdf\n",
      "Idx: 1155\tFile: 663.Unraveling Babel Exploring Multilingual Activation Patterns of LLMs and Their Applications.pdf\n",
      "Idx: 1156\tFile: 1108.Jump Starting Bandits with LLM-Generated Prior Knowledge.pdf\n",
      "Idx: 1157\tFile: 533.Susu Box or Piggy Bank Assessing Cultural Commonsense Knowledge between Ghana and the US.pdf\n",
      "Idx: 1158\tFile: 857.AppBench Planning of Multiple APIs from Various APPs for Complex User Instruction.pdf\n",
      "Idx: 1159\tFile: 834.ActPlan-1K Benchmarking the Procedural Planning Ability of Visual Language Models in Household Activities.pdf\n",
      "Idx: 1160\tFile: 385.Locating Information Gaps and Narrative Inconsistencies Across Languages A Case Study of LGBT People Portrayals on Wikipedia.pdf\n",
      "Idx: 1161\tFile: 387.Dynamic Multi-Reward Weighting for Multi-Style Controllable Generation.pdf\n",
      "Idx: 1162\tFile: 597.Data, Data Everywhere A Guide for Pretraining Dataset Construction.pdf\n",
      "Idx: 1163\tFile: 986.Enhancing Post-Hoc Attributions in Long Document Comprehension via Coarse Grained Answer Decomposition.pdf\n",
      "Idx: 1164\tFile: 626.Unlocking Markets A Multilingual Benchmark to Cross-Market Question Answering.pdf\n",
      "Idx: 1165\tFile: 442.Breaking Language Barriers Cross-Lingual Continual Pre-Training at Scale.pdf\n",
      "Idx: 1166\tFile: 751.Linguistic Bias in ChatGPT Language Models Reinforce Dialect Discrimination.pdf\n",
      "Idx: 1167\tFile: 612.Dynamic Multi-granularity Attribution Network for Aspect-based Sentiment Analysis.pdf\n",
      "Idx: 1168\tFile: 1200.RAR Retrieval-augmented retrieval for code generation in low resource languages.pdf\n",
      "Idx: 1169\tFile: 316.Concept Space Alignment in Multilingual LLMs.pdf\n",
      "Idx: 1170\tFile: 959.MedReadMe A Systematic Study for Fine-grained Sentence Readability in Medical Domain.pdf\n",
      "Idx: 1171\tFile: 375.Neuron Specialization Leveraging Intrinsic Task Modularity for Multilingual Machine Translation.pdf\n",
      "Idx: 1172\tFile: 809.Scalable Efficient Training of Large Language Models with Low-dimensional Projected Attention.pdf\n",
      "Idx: 1173\tFile: 294.Academics Can Contribute to Domain-Specialized Language Models.pdf\n",
      "Idx: 1174\tFile: 315.LogicST A Logical Self-Training Framework for Document-Level Relation Extraction with Incomplete Annotations.pdf\n",
      "Idx: 1175\tFile: 1173.BiasWipe Mitigating Unintended Bias in Text Classifiers through Model Interpretability.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not convert string to float: b'0.000000000000-5684342' : FloatObject (b'0.000000000000-5684342') invalid; use 0.0 instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 1176\tFile: 571.Towards Robust Speech Representation Learning for Thousands of Languages.pdf\n",
      "Idx: 1177\tFile: 1095.The Generation Gap Exploring Age Bias in the Value Systems of Large Language Models.pdf\n",
      "Idx: 1178\tFile: 1171.A Morphology-Based Investigation of Positional Encodings.pdf\n",
      "Idx: 1179\tFile: 121.Tag-grounded Visual Instruction Tuning with Retrieval Augmentation.pdf\n",
      "Idx: 1180\tFile: 476.WPO Enhancing RLHF with Weighted Preference Optimization.pdf\n",
      "Idx: 1181\tFile: 422.Lexically Grounded Subword Segmentation.pdf\n",
      "Idx: 1182\tFile: 340.Story Embeddings — Narrative-Focused Representations of Fictional Stories.pdf\n",
      "Idx: 1183\tFile: 1225.The Greatest Good Benchmark Measuring LLMs’ Alignment with Utilitarian Moral Dilemmas.pdf\n",
      "Idx: 1184\tFile: 128.VideoScore Building Automatic Metrics to Simulate Fine-grained Human Feedback for Video Generation.pdf\n",
      "Idx: 1185\tFile: 643.Universal Vulnerabilities in Large Language Models Backdoor Attacks for In-context Learning.pdf\n",
      "Idx: 1186\tFile: 1224.xCOMET-lite Bridging the Gap Between Efficiency and Quality in Learned MT Evaluation Metrics.pdf\n",
      "Idx: 1187\tFile: 993.Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate.pdf\n",
      "Idx: 1188\tFile: 31.EmphAssess  a Prosodic Benchmark on Assessing Emphasis Transfer in Speech-to-Speech Models.pdf\n",
      "Idx: 1189\tFile: 56.Chain-of-Dictionary Prompting Elicits Translation in Large Language Models.pdf\n",
      "Idx: 1190\tFile: 638.ToolBeHonest A Multi-level Hallucination Diagnostic Benchmark for Tool-Augmented Large Language Models.pdf\n",
      "Idx: 1191\tFile: 496.Revisiting Who’s Harry Potter Towards Targeted Unlearning from a Causal Intervention Perspective.pdf\n",
      "Idx: 1192\tFile: 1234.Inference Helps PLMs’ Conceptual Understanding Improving the Abstract Inference Ability with Hierarchical Conceptual Entailment Graphs.pdf\n",
      "Idx: 1193\tFile: 217.External Knowledge-Driven Argument Mining Leveraging Attention-Enhanced Multi-Network Models.pdf\n",
      "Idx: 1194\tFile: 1111.Virtual Personas for Language Models via an Anthology of Backstories.pdf\n",
      "Idx: 1195\tFile: 323.Leave No Document Behind Benchmarking Long-Context LLMs with Extended Multi-Doc QA.pdf\n",
      "Idx: 1196\tFile: 86.Controllable Preference Optimization Toward Controllable Multi-Objective Alignment.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Impossible to decode XFormObject /Im10: list index out of range\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idx: 1197\tFile: 124.Optimizing Code Retrieval High-Quality and Scalable Dataset Annotation through Large Language Models.pdf\n",
      "Idx: 1198\tFile: 753.A Learning Rate Path Switching Training Paradigm for Version Updates of Large Language Models.pdf\n",
      "Idx: 1199\tFile: 194.Interpreting Arithmetic Mechanism in Large Language Models through Comparative Neuron Analysis.pdf\n",
      "Idx: 1200\tFile: 680.Do LLMs Overcome Shortcut Learning An Evaluation of Shortcut Challenges in Large Language Models.pdf\n",
      "Idx: 1201\tFile: 168.With Ears to See and Eyes to Hear Sound Symbolism Experiments with Multimodal Large Language Models.pdf\n",
      "Idx: 1202\tFile: 471.Pelican Correcting Hallucination in Vision-LLMs via Claim Decomposition and Program of Thought Verification.pdf\n",
      "Idx: 1203\tFile: 1178.Exploring Intrinsic Language-specific Subspaces in Fine-tuning Multilingual Neural Machine Translation.pdf\n",
      "Idx: 1204\tFile: 1096.TempoFormer A Transformer for Temporally-aware Representations in Change Detection.pdf\n",
      "Idx: 1205\tFile: 390.Efficient LLM Comparative Assessment A Product of Experts Framework for Pairwise Comparisons.pdf\n",
      "Idx: 1206\tFile: 412.FRoG Evaluating Fuzzy Reasoning of Generalized Quantifiers in LLMs.pdf\n",
      "Idx: 1207\tFile: 985.Knowledge Planning in Large Language Models for Domain-Aligned Counseling Summarization.pdf\n",
      "Idx: 1208\tFile: 189.Word Alignment as Preference for Machine Translation.pdf\n",
      "Idx: 1209\tFile: 759.Fewer is More Boosting Math Reasoning with Reinforced Context Pruning.pdf\n",
      "Idx: 1210\tFile: 285.From the Least to the Most Building a Plug-and-Play Visual Reasoner via Data Synthesis.pdf\n",
      "Idx: 1211\tFile: 637.Improve Dense Passage Retrieval with Entailment Tuning.pdf\n",
      "Idx: 1212\tFile: 158.ASETF A Novel Method for Jailbreak Attack on LLMs through Translate Suffix Embeddings.pdf\n",
      "Idx: 1213\tFile: 675.Let Me Teach You Pedagogical Foundations of Feedback for Language Models.pdf\n",
      "Idx: 1214\tFile: 542.Birdie Advancing State Space Language Modeling with Dynamic Mixtures of Training Objectives.pdf\n",
      "Idx: 1215\tFile: 693.Latent Concept-based Explanation of NLP Models.pdf\n",
      "Idx: 1216\tFile: 77.QUDSELECT Selective Decoding for Questions Under Discussion Parsing.pdf\n",
      "Idx: 1217\tFile: 130.Integrating Structural Semantic Knowledge for Enhanced Information Extraction Pre-training.pdf\n",
      "Idx: 1218\tFile: 218.C3PA An Open Dataset of Expert-Annotated and Regulation-Aware Privacy Policies to Enable Scalable Regulatory Compliance Audits.pdf\n",
      "Idx: 1219\tFile: 1090.Discovering Biases in Information Retrieval Models Using Relevance Thesaurus as Global Explanation.pdf\n",
      "Idx: 1220\tFile: 697.Voices in a Crowd Searching for clusters of unique perspectives.pdf\n",
      "Idx: 1221\tFile: 655.What Are the Odds Language Models Are Capable of Probabilistic Reasoning.pdf\n",
      "Idx: 1222\tFile: 560.OmAgent A Multi-modal Agent Framework for Complex Video Understanding with Task Divide-and-Conquer.pdf\n",
      "Idx: 1223\tFile: 1214.Communicating with Speakers and Listeners of Different Pragmatic Levels.pdf\n",
      "Idx: 1224\tFile: 379.Right for Right Reasons Large Language Models for Verifiable Commonsense Knowledge Graph Question Answering.pdf\n",
      "Idx: 1225\tFile: 469.An L Algorithm for Deterministic Weighted Regular Languages.pdf\n",
      "Idx: 1226\tFile: 506.AssistantBench Can Web Agents Solve Realistic and Time-Consuming Tasks.pdf\n",
      "Idx: 1227\tFile: 455.Efficient Vision-Language pre-training via domain-specific learning for human activities.pdf\n",
      "Idx: 1228\tFile: 634.TroL Traversal of Layers for Large Language and Vision Models.pdf\n",
      "Idx: 1229\tFile: 950.Foundational Autoraters Taming Large Language Models for Better Automatic Evaluation.pdf\n",
      "Idx: 1230\tFile: 250.RAG-QA Arena Evaluating Domain Robustness for Long-form Retrieval Augmented Question Answering.pdf\n",
      "Idx: 1231\tFile: 553.Summary of a Haystack A Challenge to Long-Context LLMs and RAG Systems.pdf\n",
      "Idx: 1232\tFile: 943.LLM-based Code-Switched Text Generation for Grammatical Error Correction.pdf\n",
      "Idx: 1233\tFile: 1044.Transformers are Multi-State RNNs.pdf\n",
      "Idx: 1234\tFile: 535.Ranking Manipulation for Conversational Search Engines.pdf\n",
      "Idx: 1235\tFile: 28.Impeding LLM-assisted Cheating in Introductory Programming Assignments via Adversarial Perturbation.pdf\n",
      "Idx: 1236\tFile: 70.LLMs Are Zero-Shot Context-Aware Simultaneous Translators.pdf\n",
      "Idx: 1237\tFile: 25.Fine-Tuning Large Language Models to Translate Will a Touch of Noisy Data in Misaligned Languages Suffice.pdf\n",
      "Idx: 1238\tFile: 112.PsFuture A Pseudo-Future-based Zero-Shot Adaptive Policy for Simultaneous Machine Translation.pdf\n",
      "Idx: 1239\tFile: 919.Intrinsic Self-correction for Enhanced Morality An Analysis of Internal Mechanisms and the Superficial Hypothesis.pdf\n",
      "Idx: 1240\tFile: 68.EFUF Efficient Fine-Grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models.pdf\n",
      "Idx: 1241\tFile: 899.VideoCLIP-XL Advancing Long Description Understanding for Video CLIP Models.pdf\n",
      "Idx: 1242\tFile: 172.Evaluating D-MERIT of Partial-annotation on Information Retrieval.pdf\n",
      "Idx: 1243\tFile: 495.“Flex Tape Can’t Fix That” Bias and Misinformation in Edited Language Models.pdf\n",
      "Idx: 1244\tFile: 1259.Rethinking Pragmatics in Large Language Models Towards Open-Ended Evaluation and Preference Tuning.pdf\n",
      "Idx: 1245\tFile: 80.Reuse Your Rewards Reward Model Transfer for Zero-Shot Cross-Lingual Alignment.pdf\n",
      "Idx: 1246\tFile: 260.Leveraging Conflicts in Social Media Posts Unintended Offense Dataset.pdf\n",
      "Idx: 1247\tFile: 972.Zero-Shot Detection of LLM-Generated Text using Token Cohesiveness.pdf\n",
      "Idx: 1248\tFile: 216.Performance-Guided LLM Knowledge Distillation for Efficient Text Classification at Scale.pdf\n",
      "Idx: 1249\tFile: 1140.MASIVE Open-Ended Affective State Identification in English and Spanish.pdf\n",
      "Idx: 1250\tFile: 207.MTLS Making Texts into Linguistic Symbols.pdf\n",
      "Idx: 1251\tFile: 707.Atomic Self-Consistency for Better Long Form Generations.pdf\n",
      "Idx: 1252\tFile: 46.DyVo Dynamic Vocabularies for Learned Sparse Retrieval with Entities.pdf\n",
      "Idx: 1253\tFile: 332.Analyzing Key Factors Influencing Emotion Prediction Performance of VLLMs in Conversational Contexts.pdf\n",
      "Idx: 1254\tFile: 74.Learning Interpretable Legal Case Retrieval via Knowledge-Guided Case Reformulation.pdf\n",
      "Idx: 1255\tFile: 453.LawBench Benchmarking Legal Knowledge of Large Language Models.pdf\n",
      "Idx: 1256\tFile: 771.Pre-training Cross-lingual Open Domain Question Answering with Large-scale Synthetic Supervision.pdf\n",
      "Idx: 1257\tFile: 1152.Visual Text Matters Improving Text-KVQA with Visual Text Entity Knowledge-aware Large Multimodal Assistant.pdf\n",
      "Idx: 1258\tFile: 967.A Simple yet Effective Training-free Prompt-free Approach to Chinese Spelling Correction Based on Large Language Models.pdf\n",
      "Idx: 1259\tFile: 784.Where am I Large Language Models Wandering between Semantics and Structures in Long Contexts.pdf\n",
      "Idx: 1260\tFile: 695.Enhancing Data Quality through Simple De-duplication Navigating Responsible Computational Social Science Research.pdf\n",
      "Idx: 1261\tFile: 491.AGRaME Any-Granularity Ranking with Multi-Vector Embeddings.pdf\n",
      "Idx: 1262\tFile: 1105.Unsupervised Discrete Representations of American Sign Language.pdf\n",
      "Idx: 1263\tFile: 215.What do Large Language Models Need for Machine Translation Evaluation.pdf\n",
      "Idx: 1264\tFile: 935.Model Editing Harms General Abilities of Large Language Models Regularization to the Rescue.pdf\n",
      "Idx: 1265\tFile: 494.ReCaLL Membership Inference via Relative Conditional Log-Likelihoods.pdf\n",
      "Idx: 1266\tFile: 1114.The Empirical Variability of Narrative Perceptions of Social Media Texts.pdf\n",
      "Idx: 1267\tFile: 157.To Word Senses and Beyond Inducing Concepts with Contextualized Language Models.pdf\n",
      "Idx: 1268\tFile: 312.Words Worth a Thousand Pictures Measuring and Understanding Perceptual Variability in Text-to-Image Generation.pdf\n"
     ]
    }
   ],
   "source": [
    "year = 2024\n",
    "data = pd.DataFrame(columns=['title', 'text', 'year', 'pages'])\n",
    "\n",
    "lst_data = []\n",
    "lst_pdfs = [f for f in os.listdir(f'./data/EMNLP/emnlp_{year}_main') if f.endswith('.pdf')]\n",
    "\n",
    "for idx, path in enumerate(lst_pdfs):\n",
    "    print(f\"Idx: {idx}\\tFile: {path}\")\n",
    "    # Read pdf\n",
    "    pdf = PyPDFLoader(f'./data/EMNLP/emnlp_{year}_main/{path}')\n",
    "    text = pdf.load()\n",
    "\n",
    "    # Extract title\n",
    "    titel = text[0].metadata['source']\n",
    "    total_pages = text[0].metadata['total_pages']\n",
    "\n",
    "    # Extract title between number*. and .pdf\n",
    "    match = re.search(r'\\d+\\.(.*)\\.pdf', titel)\n",
    "    if match:\n",
    "        titel = match.group(1)\n",
    "    \n",
    "    page_content = \"\"\n",
    "    for t in text:\n",
    "        page_content += \" \" + t.page_content\n",
    "\n",
    "    row = {'titel': titel, 'text': page_content, 'year': year, 'pages': total_pages}\n",
    "    lst_data.append(row)\n",
    "\n",
    "data = pd.DataFrame(lst_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(f'./data/EMNLP/emnlp_{year}_main.csv', index=False, encoding='utf-8', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Proceedings of the 2024 Conference on Empirical Methods in Natural Language '\n",
      " 'Processing, pages 5421–5440\\n'\n",
      " 'November 12-16, 2024 ©2024 Association for Computational Linguistics\\n'\n",
      " 'Dialog2Flow: Pre-training Soft-Contrastive Action-Driven Sentence\\n'\n",
      " 'Embeddings for Automatic Dialog Flow Extraction\\n'\n",
      " 'Sergio Burdisso1, Srikanth Madikeri1,2 and Petr Motlicek1,3\\n'\n",
      " '1Idiap Research Institute, Martigny, Switzerland\\n'\n",
      " '2Department of Computational Linguistics, University of Zurich, Zurich, '\n",
      " 'Switzerland\\n'\n",
      " '3Brno University of Technology, Brno, Czech Republic\\n'\n",
      " 'sergio.burdisso@idiap.ch\\n'\n",
      " 'Abstract\\n'\n",
      " 'Efficiently deriving structured workflows from\\n'\n",
      " 'unannotated dialogs remains an underexplored\\n'\n",
      " 'and formidable challenge in computational lin-\\n'\n",
      " 'guistics. Automating this process could signif-\\n'\n",
      " 'icantly accelerate the manual design of work-\\n'\n",
      " 'flows in new domains and enable the grounding\\n'\n",
      " 'of large language models in domain-specific\\n'\n",
      " 'flowcharts, enhancing transparency and con-\\n'\n",
      " 'trollability. In this paper, we introduce Di-\\n'\n",
      " 'alog2Flow (D2F) embeddings, which differ\\n'\n",
      " 'from conventional sentence embeddings by\\n'\n",
      " 'mapping utterances to a latent space where\\n'\n",
      " 'they are grouped according to their commu-\\n'\n",
      " 'nicative and informative functions (i.e., the ac-\\n'\n",
      " 'tions they represent). D2F allows for modeling\\n'\n",
      " 'dialogs as continuous trajectories in a latent\\n'\n",
      " 'space with distinct action-related regions. By\\n'\n",
      " 'clustering D2F embeddings, the latent space is\\n'\n",
      " 'quantized, and dialogs can be converted into\\n'\n",
      " 'sequences of region/action IDs, facilitating the\\n'\n",
      " 'extraction of the underlying workflow. To pre-\\n'\n",
      " 'train D2F, we build a comprehensive dataset by\\n'\n",
      " 'unifying twenty task-oriented dialog datasets\\n'\n",
      " 'with normalized per-turn action annotations.\\n'\n",
      " 'We also introduce a novel soft contrastive loss\\n'\n",
      " 'that leverages the semantic information of these\\n'\n",
      " 'actions to guide the representation learning pro-\\n'\n",
      " 'cess, showing superior performance compared\\n'\n",
      " 'to standard supervised contrastive loss. Evalua-\\n'\n",
      " 'tion against various sentence embeddings, in-\\n'\n",
      " 'cluding dialog-specific ones, demonstrates that\\n'\n",
      " 'D2F yields superior qualitative and quantitative\\n'\n",
      " 'results across diverse domains.1\\n'\n",
      " '1 Introduction\\n'\n",
      " 'Conversational AI has seen significant advance-\\n'\n",
      " 'ments, especially with the rise of Large Language\\n'\n",
      " 'Models (LLMs) (Bubeck et al., 2023; Lu et al.,\\n'\n",
      " '2022; Hendrycks et al., 2021a,b; Cobbe et al.,\\n'\n",
      " '2021). Dialog modeling can be divided into open-\\n'\n",
      " 'domain dialogs and task-oriented dialogs (TOD),\\n'\n",
      " '1https://github.com/idiap/dialog2flow\\n'\n",
      " 'User: i’m looking for the transplant unit department please\\n'\n",
      " 'Action: INFORM DEPARTMENT\\n'\n",
      " 'System: okay the transfer unit department give me a sec-\\n'\n",
      " 'ond let me look okay yes i found the transplant unit depart-\\n'\n",
      " 'ment can i help\\n'\n",
      " 'Action: REQMORE\\n'\n",
      " 'User: may you please provide me with the phone number\\n'\n",
      " 'please\\n'\n",
      " 'Action: REQUEST PHONE\\n'\n",
      " 'System: get no problem okay so the number is 1223217711\\n'\n",
      " 'Action: INFORM PHONE\\n'\n",
      " 'User: okay um just repeat it it’s 1 2 2 3 2 1 7 1 1\\n'\n",
      " 'Action: CONFIRM PHONE\\n'\n",
      " 'System: okay thank you very much\\n'\n",
      " 'Action: THANK_YOU\\n'\n",
      " 'Figure 1: Example segment of the dialog SNG1533 from\\n'\n",
      " 'the hospital domain of the SpokenWOZ dataset. Ac-\\n'\n",
      " 'tions are defined by concatenating the dialog act label\\n'\n",
      " '(in bold) with the slot label(s) associated to each utter-\\n'\n",
      " 'ance.\\n'\n",
      " 'with the latter focusing on helping users achieve\\n'\n",
      " 'specific tasks (Jurafsky, 2006). In TOD, struc-\\n'\n",
      " 'tured workflows guide agents in assisting users\\n'\n",
      " 'effectively. This paper explores the underexplored\\n'\n",
      " 'terrain of automatically extracting such workflow\\n'\n",
      " 'from a collection of conversations.\\n'\n",
      " 'Extracting workflows automatically is crucial for\\n'\n",
      " 'enhancing dialog system design, discourse analysis,\\n'\n",
      " 'data augmentation (Qiu et al., 2022), and training\\n'\n",
      " 'human agents (Sohn et al., 2023). Additionally, it\\n'\n",
      " 'can ground LLMs in domain-specific workflows,\\n'\n",
      " 'improving transparency and control (Raghu et al.,\\n'\n",
      " '2021; Chen et al., 2024). Recent works have at-\\n'\n",
      " 'tempted to induce structural representations from\\n'\n",
      " 'dialogs using either ground truth annotation or ad\\n'\n",
      " 'hoc methods (Hattami et al., 2023; Qiu et al., 2022;\\n'\n",
      " 'Sun et al., 2021; Qiu et al., 2020). We believe\\n'\n",
      " 'that models specifically pre-trained for this purpose\\n'\n",
      " 'could significantly advance the field.\\n'\n",
      " 'In Task-Oriented Dialog (TOD), dialog acts and\\n'\n",
      " 'slots are key concepts (Jurafsky, 2006). Dialog\\n'\n",
      " '5421 Figure 2: Directed graph representing the hospital do-\\n'\n",
      " 'main workflow obtained from all the hospital dialogs\\n'\n",
      " 'in the SpokenWOZ dataset. Nodes correspond to indi-\\n'\n",
      " 'vidual actions. The width of edges and the underline\\n'\n",
      " 'thickness of nodes indicate their frequency. User actions\\n'\n",
      " 'are colored to distinguish them from system actions.\\n'\n",
      " 'acts represent the speaker’s communicative intent,\\n'\n",
      " 'while slots capture task-specific information. A di-\\n'\n",
      " 'alog action encapsulates both the dialog act and its\\n'\n",
      " 'corresponding slots, enabling us to view dialogs as\\n'\n",
      " 'sequences of canonical steps that convey both com-\\n'\n",
      " 'municative and informative functions (Figure 1).\\n'\n",
      " 'Motivated by this perspective, we propose embed-\\n'\n",
      " 'ding sentences into a latent space grouped by rep-\\n'\n",
      " 'resentative actions rather than solely by sentence\\n'\n",
      " 'semantics. Similar to how aggregating action se-\\n'\n",
      " 'quences from multiple dialogs reveals a common\\n'\n",
      " 'underlying workflow (Figure 2), clustering sen-\\n'\n",
      " 'tence embeddings in this latent space could uncover\\n'\n",
      " 'common conversational steps, potentially reveal-\\n'\n",
      " 'ing the underlying workflow. The main contribu-\\n'\n",
      " 'tions of this work are threefold: (a) we consoli-\\n'\n",
      " 'date twenty task-oriented dialog datasets to create\\n'\n",
      " 'the largest publicly available dataset with standard-\\n'\n",
      " 'ized action annotations; (b) we introduce a novel\\n'\n",
      " 'soft contrastive loss that leverages the semantic\\n'\n",
      " 'information of dialog actions to guide the repre-\\n'\n",
      " 'sentation learning process, outperforming standard\\n'\n",
      " 'supervised contrastive loss; and (c) we introduce\\n'\n",
      " 'and release Dialog2Flow (D2F), to the best of our\\n'\n",
      " 'knowledge, the first sentence embedding model\\n'\n",
      " 'pre-trained specifically for dialog flow extraction.\\n'\n",
      " '2 Related Work\\n'\n",
      " 'Sentence Embeddings Transformer-based en-\\n'\n",
      " 'coders like Universal Sentence Encoder (Cer et al.,\\n'\n",
      " '2018) and Sentence-BERT (Reimers and Gurevych,\\n'\n",
      " '2019) outperformed RNN-based ones such as Skip-\\n'\n",
      " 'Thought (Kiros et al., 2015) and InferSent (Con-\\n'\n",
      " 'neau et al., 2017). These models use a pooling\\n'\n",
      " 'strategy (e.g., mean pooling, [CLS] token) to ob-\\n'\n",
      " 'tain a single sentence embedding optimized for\\n'\n",
      " 'semantic similarity. However, specific domains re-\\n'\n",
      " 'quire different similarity notions. In the context of\\n'\n",
      " 'dialogs, models like TOD-BERT (Wu et al., 2020),\\n'\n",
      " 'DialogueCSE (Liu et al., 2021) and Dialog Sen-\\n'\n",
      " 'tence Embedding (DSE) (Zhou et al., 2022) have\\n'\n",
      " 'shown that conversation-based similarity outper-\\n'\n",
      " 'forms semantic similarity across different TOD\\n'\n",
      " 'tasks. Likewise, we hypothesize that action-based\\n'\n",
      " 'similarity can yield meaningful workflow-related\\n'\n",
      " 'sentence embeddings.\\n'\n",
      " 'Contrastive Learning Contrastive learning has\\n'\n",
      " 'achieved success in representation learning for both\\n'\n",
      " 'images (Chen et al., 2020; He et al., 2020; Henaff,\\n'\n",
      " '2020; Tian et al., 2020; Chen et al., 2020; Hjelm\\n'\n",
      " 'et al., 2019) and text (Zhou et al., 2022; Zhang\\n'\n",
      " 'et al., 2022, 2021; Gao et al., 2021; Wu et al.,\\n'\n",
      " '2020). It learns a representation space where sim-\\n'\n",
      " 'ilar instances cluster together and dissimilar in-\\n'\n",
      " 'stances are separated. More precisely, given an\\n'\n",
      " 'anchor with positive and negative counterparts, the\\n'\n",
      " 'goal is to minimize the distance between anchor-\\n'\n",
      " 'positive pairs while maximizing the distance be-\\n'\n",
      " 'tween anchor-negative pairs. Negatives are typi-\\n'\n",
      " 'cally obtained through in-batch negative sampling,\\n'\n",
      " 'where positives from different anchors in the mini-\\n'\n",
      " 'batch are used as negatives.\\n'\n",
      " '3 Method\\n'\n",
      " '3.1 Representation Learning Framework\\n'\n",
      " 'Following common practices (Zhou et al., 2022;\\n'\n",
      " 'Chen et al., 2020; Tian et al., 2020; Khosla et al.,\\n'\n",
      " '2020), the main components of our framework are:\\n'\n",
      " '• Encoder, f(·) ∈ Rn, which maps x to a\\n'\n",
      " 'representation vector, x = f(x). Following\\n'\n",
      " '5422 Sentence-BERT (Reimers and Gurevych, 2019)\\n'\n",
      " 'and DSE (Reimers and Gurevych, 2019), f(·) con-\\n'\n",
      " 'sists of a BERT-based encoder with mean pooling\\n'\n",
      " 'strategy trained as a bi-encoder with shared weights\\n'\n",
      " '(siamese network).\\n'\n",
      " '•Contrastive head , g(·) ∈ Rd, used during\\n'\n",
      " 'training to map representations x to the space\\n'\n",
      " 'where contrastive loss is applied. Following Chen\\n'\n",
      " 'et al. (2020) and DSE, we instantiate g(·) as the\\n'\n",
      " 'multi-layer perceptron with a single hidden layer\\n'\n",
      " 'z = g(x) = ReLU(x·W1)W2 where W1 ∈Rn×n\\n'\n",
      " 'and W2 ∈Rn×d.\\n'\n",
      " '•Similarity measure, sim(u,v), used to learn the\\n'\n",
      " 'representation is cosine similarity. Thus, similarity\\n'\n",
      " 'is then measured only by the angle between u and\\n'\n",
      " 'v, making our latent space geometrically a unit hy-\\n'\n",
      " 'persphere. Hence, in this study, we treat similarity\\n'\n",
      " 'and alignment interchangeably. Additionally, we\\n'\n",
      " 'assume f(·) and g(·) vectors are L2-normalized,\\n'\n",
      " 'leading to sim(u,v) = cos(u,v) = u ·v.\\n'\n",
      " '3.1.1 Supervised Contrastive Loss\\n'\n",
      " 'For a batch of N randomly sampled anchor, posi-\\n'\n",
      " 'tive, and label triples, B = {(xi,x+\\n'\n",
      " 'i ,yi)}N\\n'\n",
      " 'i=1, the\\n'\n",
      " 'supervised contrastive loss (Khosla et al., 2020),\\n'\n",
      " 'for each i-th triplet (xi,x+\\n'\n",
      " 'i ,yi) is defined as:\\n'\n",
      " 'ℓsup\\n'\n",
      " 'i = −\\n'\n",
      " '∑\\n'\n",
      " 'j∈Pi\\n'\n",
      " '1\\n'\n",
      " '|Pi|log ezi·z+\\n'\n",
      " 'j /τ\\n'\n",
      " '∑N\\n'\n",
      " 'k=1 ezi·z+\\n'\n",
      " 'k/τ (1)\\n'\n",
      " 'where Pi = {j |yi = yj}is the set of indexes of\\n'\n",
      " 'all the samples with the same label as the i-th sam-\\n'\n",
      " 'ple in the batch, and τ is the softmax temperature\\n'\n",
      " 'parameter that controls how soft/strongly positive\\n'\n",
      " 'pairs are pulled together and negative pairs pushed\\n'\n",
      " 'apart in the embedding space. 2 The final loss is\\n'\n",
      " 'computed across all the N pairs in the mini-batch\\n'\n",
      " 'as Lsup = 1\\n'\n",
      " 'N\\n'\n",
      " '∑N\\n'\n",
      " 'i=1 ℓsup\\n'\n",
      " 'i .\\n'\n",
      " '3.1.2 Supervised Soft Contrastive Loss\\n'\n",
      " 'Let δ(yi,yj) be a semantic similarity measure be-\\n'\n",
      " 'tween labels yi and yj. We define our soft con-\\n'\n",
      " 'trastive loss as follows:\\n'\n",
      " 'ℓsoft\\n'\n",
      " 'i =−\\n'\n",
      " 'N∑\\n'\n",
      " 'j=1\\n'\n",
      " 'eδ(yi,yj)/τ′\\n'\n",
      " '∑N\\n'\n",
      " 'k=1 e\\n'\n",
      " 'δ(yi,yk)\\n'\n",
      " 'τ′\\n'\n",
      " 'log ezi·z+\\n'\n",
      " 'j /τ\\n'\n",
      " '∑N\\n'\n",
      " 'k=1 e\\n'\n",
      " 'zi·z+\\n'\n",
      " 'k\\n'\n",
      " 'τ\\n'\n",
      " 'where τ′ is a temperature parameter controlling\\n'\n",
      " 'the \"softness\" of the negative labels (impact anal-\\n'\n",
      " 'ysis available in Appendix E). For further details,\\n'\n",
      " '2The lower τ, the sharper the softmax output distribution\\n'\n",
      " 'and the stronger the push/pull factor.\\n'\n",
      " 'Dataset #U #D #DA #S\\n'\n",
      " 'ABCD (Chen et al., 2021) 20.4K 10 0 10\\n'\n",
      " 'BiTOD (Lin et al., 2021) 72.5K 6 13 33\\n'\n",
      " 'Disambiguation (Qian et al., 2022) 114.3K 8 9 28\\n'\n",
      " 'DSTC2-Clean (Mrkši´c et al., 2017) 25K 1 2 8\\n'\n",
      " 'FRAMES (El Asri et al., 2017) 20K 1 21 46\\n'\n",
      " 'GECOR (Quan et al., 2019) 2.5K 1 2 10\\n'\n",
      " 'HDSA-Dialog (Chen et al., 2019) 91.9K 8 6 24\\n'\n",
      " 'KETOD (Chen et al., 2022) 107.7K 20 15 182\\n'\n",
      " 'MS-DC (Li et al., 2018) 71.9K 3 11 56\\n'\n",
      " 'MulDoGO (Peskov et al., 2019) 74.8K 6 0 63\\n'\n",
      " 'MultiWOZ2.1 (Eric et al., 2020) 108.3K 8 9 27\\n'\n",
      " 'MultiWOZ2.2 (Zang et al., 2020) 55.9K 8 2 26\\n'\n",
      " 'SGD (Rastogi et al., 2020) 479.5K 20 15 184\\n'\n",
      " 'Taskmaster1 (Byrne et al., 2019) 30.7K 6 1 59\\n'\n",
      " 'Taskmaster2 (Byrne et al., 2019) 147K 11 1 117\\n'\n",
      " 'Taskmaster3 (Byrne et al., 2019) 589.7K 1 1 21\\n'\n",
      " 'WOZ2.0 (Mrkši´c et al., 2017) 4.4K 1 2 10\\n'\n",
      " 'SimJointMovie (Shah et al., 2018) 7.2K 1 14 5\\n'\n",
      " 'SimJointRestaurant (Shah et al., 2018) 20K 1 15 9\\n'\n",
      " 'SimJointGEN (Zhang et al., 2024) 1.3M 1 16 5\\n'\n",
      " 'Total 3.4M 52 44 524\\n'\n",
      " 'Table 1: Details of used TOD datasets, including the\\n'\n",
      " 'number of utterances (#U), unique domains (#D), dialog\\n'\n",
      " 'act labels (#DA), and slot labels (#S).\\n'\n",
      " 'including the underlying intuition behind the equa-\\n'\n",
      " 'tion, please refer to Appendix D. Unlike Equation 1,\\n'\n",
      " 'this loss encourages the encoder to separate anchors\\n'\n",
      " 'and negatives proportionally to the semantic sim-\\n'\n",
      " 'ilarity of their labels. Finally, the mini-batch loss\\n'\n",
      " 'Lsoft is computed as in Lsup.\\n'\n",
      " '3.2 Training Targets\\n'\n",
      " 'We experiment with four types of training targets,\\n'\n",
      " 'distinguished by whether the dialogue action label\\n'\n",
      " 'is used directly or decomposed into dialogue act\\n'\n",
      " 'and slot labels, and by the type of contrastive loss\\n'\n",
      " 'employed. Specifically, we consider the following\\n'\n",
      " 'two targets using the proposed soft contrastive loss:\\n'\n",
      " '•D2Fsingle: L= Lsoft\\n'\n",
      " 'act+slots\\n'\n",
      " '•D2Fjoint: L= Lsoft\\n'\n",
      " 'act + Lsoft\\n'\n",
      " 'slots\\n'\n",
      " 'and the two corresponding targets using the default\\n'\n",
      " 'supervised contrastive loss:\\n'\n",
      " '•D2F-Hardsingle: L= Lsup\\n'\n",
      " 'act+slots\\n'\n",
      " '•D2F-Hardjoint: L= Lsup\\n'\n",
      " 'act + Lsup\\n'\n",
      " 'slots\\n'\n",
      " 'The subscript in bold indicates the type of label\\n'\n",
      " 'used to compute the loss, either the dialog action as\\n'\n",
      " 'a single label (act+slots), or the dialog act and slots\\n'\n",
      " 'separately. In the case of the joint loss, separate\\n'\n",
      " 'contrastive heads g(·) are employed.\\n'\n",
      " '4 Training Corpus\\n'\n",
      " 'We identified and collected 20 TOD datasets from\\n'\n",
      " 'which we could extract dialog act and/or slot anno-\\n'\n",
      " '5423 tations, as summarized in Table 1. We then man-\\n'\n",
      " 'ually inspected each dataset to locate and extract\\n'\n",
      " 'the necessary annotations, manually standardizing\\n'\n",
      " 'domain names and dialog act labels across datasets.\\n'\n",
      " 'Finally, we unified all datasets under a consistent\\n'\n",
      " 'format, incorporating per-turn dialog act and slot\\n'\n",
      " 'annotations. The resulting unified TOD dataset\\n'\n",
      " 'comprises 3.4 million utterances annotated with\\n'\n",
      " '18 standardized dialog acts, 524 unique slot labels,\\n'\n",
      " 'and 3,982 unique action labels (dialog act + slots)\\n'\n",
      " 'spanning across 52 different domains (details in\\n'\n",
      " 'Appendix A).\\n'\n",
      " '5 Experimental Setup\\n'\n",
      " 'For training D2F we mostly follow the experimen-\\n'\n",
      " 'tal setup of DSE (Zhou et al., 2022) and TOD-\\n'\n",
      " 'BERT (Wu et al., 2020), using BERT base as the\\n'\n",
      " 'backbone model for the encoder to report results\\n'\n",
      " 'in the main text. Additional configurations are re-\\n'\n",
      " 'ported in the ablation study (Appendix C) while\\n'\n",
      " 'implementation details are given in Appendix B.\\n'\n",
      " '5.1 Baselines\\n'\n",
      " 'General sentence embeddings. •GloVe: the av-\\n'\n",
      " 'erage of GloVe embeddings (Pennington et al.,\\n'\n",
      " '2014). • BERT: the vanilla BERT base model\\n'\n",
      " 'with mean pooling strategy, corresponding to\\n'\n",
      " 'our untrained encoder. •Sentence-BERT: the\\n'\n",
      " 'model with the best average performance re-\\n'\n",
      " 'ported among all Sentence-BERT pre-trained mod-\\n'\n",
      " 'els, namely the all-mpnet-base-v2 model pre-\\n'\n",
      " 'trained using MPNet (Song et al., 2020) and\\n'\n",
      " 'further fine-tuned on a 1 billion sentence pairs\\n'\n",
      " 'dataset. •GTR-T5: the Generalizable T5-based\\n'\n",
      " 'dense Retriever (Ni et al., 2022) pre-trained on\\n'\n",
      " 'a 2 billion web question-answer pairs dataset,\\n'\n",
      " 'outperforming previous sparse and dense retriev-\\n'\n",
      " 'ers on the BEIR benchmark (Thakur et al.,\\n'\n",
      " '2021). •OpenAI: the recently released OpenAI’s\\n'\n",
      " 'text-embedding-3-large model (OpenAI, 2024;\\n'\n",
      " 'Neelakantan et al., 2022)\\n'\n",
      " 'Dialog sentence embeddings. •TOD-BERT: the\\n'\n",
      " 'TOD-BERT-jnt model reported in Wu et al. (2020)\\n'\n",
      " 'pre-trained to optimize a contrastive response se-\\n'\n",
      " 'lection objective by treating utterances and their\\n'\n",
      " 'dialog context as positive pairs. The pre-training\\n'\n",
      " 'data is the combination of 9 publicly available\\n'\n",
      " 'task-oriented datasets around 1.4 million total ut-\\n'\n",
      " 'terances across 60 domains. •DSE: pre-trained on\\n'\n",
      " 'the same dataset as TOD-BERT, DSE learns sen-\\n'\n",
      " 'tence embeddings by simply taking consecutive\\n'\n",
      " 'utterances of the same dialog as positive pairs for\\n'\n",
      " 'contrastive learning. DSE has shown to achieve\\n'\n",
      " 'better representation capability than the other di-\\n'\n",
      " 'alog and general sentence embeddings on TOD\\n'\n",
      " 'downstream tasks (Gung et al., 2023; Zhou et al.,\\n'\n",
      " '2022). •SBD-BERT: the TOD-BERT-SBDMWOZ\\n'\n",
      " 'model reported in Qiu et al. (2022) in which sen-\\n'\n",
      " 'tences are represented as the mean pooling of\\n'\n",
      " 'the tokens that are part of the slots of the utter-\\n'\n",
      " 'ance, as identified by a Slot Boundary Detection\\n'\n",
      " '(SBD) model trained on the original MultiWOZ\\n'\n",
      " 'dataset (Budzianowski et al., 2018). •DialogGPT:\\n'\n",
      " 'following TOD-BERT and DSE, we also report re-\\n'\n",
      " 'sults with DialogGPT (Zhang et al., 2020) using the\\n'\n",
      " 'mean pooling of its hidden states as the sentence\\n'\n",
      " 'representation. •SPACE-2: a dialog representation\\n'\n",
      " 'model pre-trained on a corpus of 22.8 million utter-\\n'\n",
      " 'ances, 3.3 million of which are annotated with TOD\\n'\n",
      " 'labels (He et al., 2022). The annotation is used for\\n'\n",
      " 'supervised contrastive learning and follows a four-\\n'\n",
      " 'layer domain→intent→slot→value semantic tree\\n'\n",
      " 'structure.3\\n'\n",
      " '5.2 Evaluation Data\\n'\n",
      " 'Most of the TOD datasets are constructed solely\\n'\n",
      " 'based on written texts, which may not accurately\\n'\n",
      " 'reflect the nuances of real-world spoken conver-\\n'\n",
      " 'sations, potentially leading to a gap between aca-\\n'\n",
      " 'demic research and real-world spoken TOD sce-\\n'\n",
      " 'narios. Therefore, we evaluate our performance\\n'\n",
      " 'not only on a subset of our unified TOD dataset\\n'\n",
      " 'but also on SpokenWOZ (Si et al., 2023), the first\\n'\n",
      " 'large-scale human-to-human speech-text dataset\\n'\n",
      " 'for TOD designed to address this limitation. More\\n'\n",
      " 'precisely, we use the following two evaluation sets:\\n'\n",
      " '•Unified TOD evaluation set: 26,910 utterances\\n'\n",
      " 'with 1,794 unique action labels (dialog act + slots)\\n'\n",
      " 'extracted from the training data. These utterances\\n'\n",
      " 'were extracted by sampling and removing 15 ut-\\n'\n",
      " 'terances for each action label with more than 100\\n'\n",
      " 'utterances in the training data.\\n'\n",
      " '•SpokenWOZ: 31,303 utterances with 427 unique\\n'\n",
      " 'action labels corresponding to all the 1,710 single\\n'\n",
      " 'domain conversations in SpokenWOZ. We are only\\n'\n",
      " 'using complete single-domain conversations so that\\n'\n",
      " 'we can also use them later to extract the domain-\\n'\n",
      " 'specific workflow for each of the 7 domains in\\n'\n",
      " 'SpokenWOZ.4\\n'\n",
      " '3Except DSE and SBD-BERT, models are optimized for dia-\\n'\n",
      " 'log context and may underperform on isolated sentences due\\n'\n",
      " 'to reliance on dialog-specific features like turns and roles.\\n'\n",
      " '4There are no single-domain calls for the profile domain\\n'\n",
      " '5424 6 Similarity-based Evaluation\\n'\n",
      " 'Before the dialog flow-based evaluation, we assess\\n'\n",
      " 'the quality of the representation space geometry\\n'\n",
      " 'through the similarity of the embeddings represent-\\n'\n",
      " 'ing differentactions. We use the following methods\\n'\n",
      " 'as quality proxies:\\n'\n",
      " '•Anisotropy. Following Jiang et al. (2022); Etha-\\n'\n",
      " 'yarajh (2019), we measure the anisotropy of a set\\n'\n",
      " 'of embeddings as the average cosine (absolute) sim-\\n'\n",
      " 'ilarity among all embeddings in the set. 5 Ideally,\\n'\n",
      " 'embeddings of the same action should be simi-\\n'\n",
      " 'lar (high intra-action anisotropy) while being dis-\\n'\n",
      " 'similar to those of other actions (low inter-action\\n'\n",
      " 'anisotropy). We report the average intra- and inter-\\n'\n",
      " 'action anisotropy across all actions.\\n'\n",
      " '•Similarity-based few-shot classification. We\\n'\n",
      " 'use Prototypical Networks (Snell et al., 2017) to\\n'\n",
      " 'perform a similarity-based classification. A pro-\\n'\n",
      " 'totype embedding for each action is calculated by\\n'\n",
      " 'averaging kof its embeddings (k-shot). All other\\n'\n",
      " 'embeddings are then classified based on the closest\\n'\n",
      " 'prototype embedding. We report the macro aver-\\n'\n",
      " 'aged F1 score and Accuracy for k= 1 and k= 5\\n'\n",
      " '(i.e., 1-shot and 5-shot classification).\\n'\n",
      " '•Ranking. For each action, we randomly select\\n'\n",
      " 'one utterance as the query and retrieve the top- k\\n'\n",
      " 'closest embeddings, creating a ranking with their\\n'\n",
      " 'actions. Ideally, the top- k retrieved embeddings\\n'\n",
      " 'should predominantly correspond to the same ac-\\n'\n",
      " 'tion as the query, thus ranked first. We report Nor-\\n'\n",
      " 'malized Discounted Cumulative Gain(nDCG@10),\\n'\n",
      " 'averaged over all actions.\\n'\n",
      " '6.1 Similarity-based Results\\n'\n",
      " 'Tables 2 and 3 present the similarity-based classifi-\\n'\n",
      " 'cation and anisotropy results on the unified TOD\\n'\n",
      " 'evaluation set and SpokenWOZ, respectively. Re-\\n'\n",
      " 'sults are averaged over 1,794 and 427 different\\n'\n",
      " 'action labels for both datasets, respectively. For\\n'\n",
      " 'classification results, we report the mean and stan-\\n'\n",
      " 'dard deviation from 10 repetitions, each sampling\\n'\n",
      " 'different embeddings for the 1-shot and 5-shot pro-\\n'\n",
      " 'totypes. All D2F variants consistently outperform\\n'\n",
      " 'the baselines across all metrics. This is expected,\\n'\n",
      " 'as D2F models, unlike the baselines, are explic-\\n'\n",
      " 'itly trained to learn a representation space where\\n'\n",
      " 'embeddings are clustered by their corresponding\\n'\n",
      " 'actions. However, the baseline results serve as\\n'\n",
      " 'a proxy for assessing the inherent suitability of\\n'\n",
      " 'so it is not included.\\n'\n",
      " '5 1\\n'\n",
      " 'n2−n\\n'\n",
      " '⏐⏐⏐∑\\n'\n",
      " 'i\\n'\n",
      " '∑\\n'\n",
      " 'j̸=icos(xi,xj)\\n'\n",
      " '⏐⏐⏐for given {x1,··· ,xn}\\n'\n",
      " 'existing sentence embedding models for our ob-\\n'\n",
      " 'jective.6 For instance, as shown in Table 2, DSE,\\n'\n",
      " 'which clusters sentences based on conversational\\n'\n",
      " 'context similarity (i.e., how often they appear con-\\n'\n",
      " 'secutively in task-oriented dialogs), outperforms\\n'\n",
      " 'general-purpose embeddings that rely on semantic\\n'\n",
      " 'similarity. Notably, D2F embeddings trained with\\n'\n",
      " 'the proposed soft contrastive loss exhibit superior\\n'\n",
      " 'performance compared to D2F-Hard embeddings\\n'\n",
      " 'trained with the standard supervised contrastive\\n'\n",
      " 'loss. In Table 3, the difference among the vari-\\n'\n",
      " 'ous embeddings narrows, and standard deviations\\n'\n",
      " 'increase significantly compared to Table 2. This\\n'\n",
      " 'indicates that results vary considerably depend-\\n'\n",
      " 'ing on the sampled prototypes, suggesting that the\\n'\n",
      " 'SpokenWOZ data is considerably noisier than the\\n'\n",
      " 'unified TOD evaluation set. This is expected as\\n'\n",
      " 'SpokenWOZ utterances were obtained by an ASR\\n'\n",
      " 'model from real-world human-to-human spoken\\n'\n",
      " 'TOD conversations, thus affected by ASR noise\\n'\n",
      " 'and various linguistic phenomena such as back-\\n'\n",
      " 'channels, disfluencies, and incomplete utterances.7\\n'\n",
      " 'Classification results provide a local view of the\\n'\n",
      " 'representation space quality around the different\\n'\n",
      " 'sampled prototypes. Actions spread into multiple\\n'\n",
      " 'sub-clusters could still yield good classification re-\\n'\n",
      " 'sults. Thus, we also consider anisotropy results\\n'\n",
      " 'for a more global view of the representation space\\n'\n",
      " 'quality. Among the baselines, TOD-BERT has the\\n'\n",
      " 'highest intra-action anisotropy but also the high-\\n'\n",
      " 'est inter-action value, which means that, on av-\\n'\n",
      " 'erage, embeddings of different actions are closer\\n'\n",
      " 'than embeddings of the same action! (negative\\n'\n",
      " '∆ values). Sentence-BERT has the lowest inter-\\n'\n",
      " 'action anisotropy, indicating different actions are\\n'\n",
      " 'the most dissimilar, although embeddings of the\\n'\n",
      " 'same action are less similar (∆ = 0.094) compared\\n'\n",
      " 'to DSE (∆ = 0.108) in Table 2. D2F embeddings\\n'\n",
      " 'exhibit the best anisotropy values, with a ∆ differ-\\n'\n",
      " 'ence between intra- and inter-action embeddings of\\n'\n",
      " '0.597 and 0.451 in Table 2, and 0.193 and 0.103\\n'\n",
      " 'in Table 3, for single and joint targets, respectively,\\n'\n",
      " 'roughly doubling their D2F-Hard counterparts.\\n'\n",
      " '6Throughout this paper, baseline results are intended to\\n'\n",
      " 'provide the reader with insights into the potential usability of\\n'\n",
      " 'available sentence embedding models if they were to be used\\n'\n",
      " 'for automatic dialog flow extraction, compared to our task-\\n'\n",
      " 'adaptive pre-trained embeddings (Gururangan et al., 2020).\\n'\n",
      " '7Indeed, SpokenWOZ authors conducted experiments us-\\n'\n",
      " 'ing newly proposed LLMs and dual-modal models, showing\\n'\n",
      " 'that current models face challenges on this more-realistic spo-\\n'\n",
      " 'ken dataset (Si et al., 2023).\\n'\n",
      " '5425 F1 score Accuracy Anisotropy\\n'\n",
      " 'Embeddings 1-shot 5-shot 1-shot 5-shot intra(↑) inter(↓) ∆ (↑)\\n'\n",
      " 'GloVe 23.24 ±0.87 24.45 ±0.94 26.04 ±0.81 30.01 ±0.86 0.674 0.633 0.041\\n'\n",
      " 'BERT 23.85 ±0.47 28.22 ±0.60 26.32 ±0.62 32.92 ±0.38 0.737 0.781 -0.044\\n'\n",
      " 'Sentence-BERT 27.86 ±0.93 33.30 ±0.68 30.55 ±0.82 38.22 ±0.46 0.527 0.433 '\n",
      " '0.094\\n'\n",
      " 'GTR-T5 30.86 ±0.39 38.38 ±0.64 33.34 ±0.29 42.96 ±0.60 0.694 0.706 -0.012\\n'\n",
      " 'OpenAI 32.12 ±0.87 41.06 ±0.68 34.95 ±0.84 45.51 ±0.60 0.541 0.424 0.117\\n'\n",
      " 'DSE 35.43 ±0.96 42.21 ±0.90 38.12 ±0.77 46.85 ±0.79 0.649 0.541 0.108\\n'\n",
      " 'SPACE-2 26.93 ±0.64 37.04 ±0.66 28.95 ±0.62 41.32 ±0.57 0.664 0.646 0.018\\n'\n",
      " 'TOD-BERT 27.58 ±0.92 33.35 ±0.58 29.63 ±1.06 36.88 ±0.87 0.840 0.864 -0.024\\n'\n",
      " 'DialoGPT 25.86 ±0.34 31.34 ±0.73 28.24 ±0.53 36.15 ±0.83 0.734 0.758 -0.024\\n'\n",
      " 'SBD-BERT 24.31 ±0.95 27.71 ±0.38 26.40 ±0.96 31.53 ±0.44 0.687 0.604 0.083\\n'\n",
      " 'D2F-Hardsingle 58.84 ±0.62 67.82 ±0.52 61.52 ±0.54 70.69 ±0.43 0.646 0.313 '\n",
      " '0.332\\n'\n",
      " 'D2F-Hardjoint 56.25 ±1.16 66.22 ±0.62 58.98 ±1.08 69.23 ±0.48 0.629 0.399 '\n",
      " '0.230\\n'\n",
      " 'D2Fsingle 65.36 ±0.91 70.89 ±0.30 68.06 ±0.87 74.15 ±0.40 0.782 0.186 0.597\\n'\n",
      " 'D2Fjoint 63.70 ±1.35 70.94 ±0.41 66.53 ±1.15 74.03 ±0.31 0.741 0.289 0.451\\n'\n",
      " 'Table 2: Similarity-based few-shot classification results on our unified TOD '\n",
      " 'evaluation set. The intra- and inter-\\n'\n",
      " 'action anisotropy are also provided along their difference (∆). Bold '\n",
      " 'indicates the best values in each group while\\n'\n",
      " 'underlined the global best.\\n'\n",
      " '(a) Sentence-BERT\\n'\n",
      " ' (b) D2F-Hardjoint\\n'\n",
      " ' (c) D2Fjoint\\n'\n",
      " 'Figure 3: Spherical V oronoi diagram of embeddings projected onto the unit '\n",
      " 'sphere using UMAP with cosine\\n'\n",
      " 'distance as the metric. The embeddings represent system utterances from the '\n",
      " 'police domain of the MultiWOZ2.1\\n'\n",
      " 'dataset. Legends indicate the ground-truth action associated to each '\n",
      " 'embedding and the centroids used to generate\\n'\n",
      " 'the partitions for all the actions in this domain.\\n'\n",
      " 'F1 score Accuracy Anisotropy\\n'\n",
      " 'Embeddings 1-shot 5-shot 1-shot 5-shot intra(↑) inter(↓) ∆ (↑)\\n'\n",
      " 'GloVe 19.47 ±2.47 24.54 ±2.45 26.07 ±4.52 33.30 ±4.19 0.653 0.642 0.010\\n'\n",
      " 'BERT 21.93 ±2.40 31.11 ±2.56 28.33 ±3.76 39.98 ±3.56 0.711 0.761 -0.049\\n'\n",
      " 'Sentence-BERT 23.48 ±2.62 35.71 ±2.94 33.03 ±4.70 47.47 ±3.60 0.440 0.404 '\n",
      " '0.036\\n'\n",
      " 'GTR-T5 26.53 ±2.29 41.10 ±2.37 35.76 ±4.00 52.73 ±3.16 0.681 0.714 -0.033\\n'\n",
      " 'OpenAI 28.67 ±2.33 42.49 ±2.54 39.98 ±3.77 55.37 ±3.24 0.496 0.468 0.029\\n'\n",
      " 'DSE 27.53 ±2.70 39.90 ±3.08 35.93 ±4.54 51.73 ±3.41 0.633 0.608 0.026\\n'\n",
      " 'SPACE-2 25.07 ±2.06 38.31 ±2.38 34.00 ±3.91 48.45 ±3.21 0.653 0.650 0.003\\n'\n",
      " 'TOD-BERT 21.23 ±2.03 32.28 ±2.33 29.26 ±3.99 41.71 ±3.68 0.848 0.885 -0.038\\n'\n",
      " 'DialoGPT 21.74 ±2.10 32.01 ±2.38 27.65 ±3.47 41.05 ±3.64 0.700 0.726 -0.026\\n'\n",
      " 'SBD-BERT 19.09 ±2.10 23.83 ±2.22 25.80 ±3.56 32.14 ±3.62 0.651 0.596 0.055\\n'\n",
      " 'D2F-Hardsingle 34.64 ±2.90 49.63 ±2.87 42.77 ±4.61 58.63 ±3.27 0.526 0.424 '\n",
      " '0.103\\n'\n",
      " 'D2F-Hardjoint 31.46 ±2.61 46.89 ±2.50 39.45 ±4.22 56.43 ±2.98 0.514 0.481 '\n",
      " '0.033\\n'\n",
      " 'D2Fsingle 35.55 ±3.51 49.75 ±2.48 43.15 ±5.24 59.93 ±3.06 0.516 0.321 0.195\\n'\n",
      " 'D2Fjoint 33.19 ±2.95 46.90 ±2.66 41.22 ±4.40 57.07 ±2.92 0.545 0.429 0.116\\n'\n",
      " 'Table 3: Similarity-based few-shot classification results on SpokenWOZ. The '\n",
      " 'intra- and inter-action anisotropy are\\n'\n",
      " 'also provided along their difference (∆).\\n'\n",
      " 'We hypothesize that the performance improve-\\n'\n",
      " 'ment observed when using the proposed soft-\\n'\n",
      " 'contrastive loss ( i.e., D2F vs. D2F-Hard) stems\\n'\n",
      " 'from a more semantically informed arrangement\\n'\n",
      " 'of embeddings within the representation space. By\\n'\n",
      " 'leveraging action semantics during training, the\\n'\n",
      " 'soft-contrastive loss guides the learning process\\n'\n",
      " 'towards a more meaningful organization of embed-\\n'\n",
      " '5426 Embeddings NDCG@10 ♣ NDCG@10⋆\\n'\n",
      " 'GloVe 26.55 ±0.57 25.09 ±2.28\\n'\n",
      " 'BERT 26.98 ±0.80 27.74 ±2.00\\n'\n",
      " 'Sentence-BERT 30.88 ±0.70 30.07 ±2.23\\n'\n",
      " 'GTR-T5 33.21 ±0.60 32.74 ±2.44\\n'\n",
      " 'OpenAI 35.82 ±0.62 34.52 ±2.01\\n'\n",
      " 'DSE 38.09 ±0.71 33.94 ±2.47\\n'\n",
      " 'SPACE-2 30.01 ±0.48 30.58 ±2.01\\n'\n",
      " 'TOD-BERT 30.55 ±0.74 25.63 ±1.88\\n'\n",
      " 'DialoGPT 28.86 ±0.71 27.92 ±2.01\\n'\n",
      " 'SBD-BERT 27.20 ±0.83 22.24 ±1.93\\n'\n",
      " 'D2F-Hardsingle 60.87 ±0.47 42.48 ±2.77\\n'\n",
      " 'D2F-Hardjoint 58.38 ±0.72 40.03 ±2.52\\n'\n",
      " 'D2Fsingle 67.31 ±0.42 43.12 ±2.92\\n'\n",
      " 'D2Fjoint 66.50 ±0.49 40.97 ±2.61\\n'\n",
      " 'Table 4: Ranking-based results on the unified TOD\\n'\n",
      " 'evaluation set (♣) and SpokenWOZ (⋆).\\n'\n",
      " 'dings. For instance, Figure 3 shows the projection\\n'\n",
      " 'of the embeddings onto the unit sphere for a sub-\\n'\n",
      " 'set of six related actions. 8 Sentence-BERT clus-\\n'\n",
      " 'ters embeddings into roughly two main semantic\\n'\n",
      " 'groups, with price-related actions on top and others\\n'\n",
      " 'at the bottom. D2F-Hard correctly clusters embed-\\n'\n",
      " 'dings of the same action together while maintain-\\n'\n",
      " 'ing separation among centroids of different actions.\\n'\n",
      " 'However, the arrangement among different clus-\\n'\n",
      " 'ters is better in D2F, guided by action semantics\\n'\n",
      " '–namely, all clusters are adjacent, with •[request\\n'\n",
      " 'price] next to •[inform price]; •[inform name\\n'\n",
      " 'price] between •[inform name] and •[inform\\n'\n",
      " 'price]; and •[inform name price area ] be-\\n'\n",
      " 'tween •[inform name price] and •[inform name\\n'\n",
      " 'area].\\n'\n",
      " 'Finally, Table 4 presents the ranking-based re-\\n'\n",
      " 'sults on both evaluation sets. We report the mean\\n'\n",
      " 'and standard deviation from 10 repetitions, each\\n'\n",
      " 'sampling different query utterances for all actions.\\n'\n",
      " 'We observe a similar pattern across both datasets:\\n'\n",
      " 'an increase in variability and a drop in perfor-\\n'\n",
      " 'mance for all embedding types in SpokenWOZ.\\n'\n",
      " 'However, D2F embeddings still outperform all\\n'\n",
      " 'baselines and their D2F-Hard counterparts. For\\n'\n",
      " 'a more qualitative analysis, Table 5 provides an ex-\\n'\n",
      " 'ample of the rankings obtained for the query \"your\\n'\n",
      " 'phone please\" with the target action [ request\\n'\n",
      " 'phone_number] on SpokenWOZ. As seen, DSE\\n'\n",
      " 'errors arise due to embeddings being closer if they\\n'\n",
      " 'correspond to consecutive utterances (inform and\\n'\n",
      " '8The original manifold in which utterances are embedded\\n'\n",
      " 'correspond to the unit hyper-sphere, thus, we believe the unit\\n'\n",
      " 'sphere provides a more truthful visualization than a 2D plane.\\n'\n",
      " 'request utterances). Sentence-BERT errors oc-\\n'\n",
      " 'cur due to the retrieval of utterances semantically\\n'\n",
      " 'related to \"number\" and \"phone.\" In contrast, all\\n'\n",
      " 'D2F-retrieved utterances correctly represent differ-\\n'\n",
      " 'ent ways to request a phone number, even though\\n'\n",
      " 'half were considered incorrect due to the lack\\n'\n",
      " 'of slot name standardization across different do-\\n'\n",
      " 'mains (e.g., phone_number and phone).9 Nonethe-\\n'\n",
      " 'less, for clustering utterances by similarity to ex-\\n'\n",
      " 'tract a dialog flow without annotation, D2F would\\n'\n",
      " 'successfully cluster these 10 utterances together\\n'\n",
      " 'as they correspond to semantically equivalent ac-\\n'\n",
      " 'tions ([ request phone_number ] and [ request\\n'\n",
      " 'phone]).\\n'\n",
      " '7 Dialog Flow Extraction Evaluation\\n'\n",
      " 'Dialog flow extraction is an underexplored hard-\\n'\n",
      " 'to-quantify and challenging task with nuances in\\n'\n",
      " 'definition. However, to evaluate embedding qual-\\n'\n",
      " 'ity, we formally define the problem as follows:\\n'\n",
      " 'Let Uand Adenote sets of TOD utterances and\\n'\n",
      " 'actions, respectively. Let U and Abe sets of\\n'\n",
      " 'TOD utterances and actions, respectively. Let\\n'\n",
      " 'α : U ↦→ Abe a (usually unknown) function\\n'\n",
      " 'mapping an utterance to its corresponding action.\\n'\n",
      " 'Let di = (u1,··· ,uk) be a dialog with uj ∈U ,\\n'\n",
      " 'and ti = ( α(u1),··· ,α(uk)) = ( a1,··· ,ak)\\n'\n",
      " 'its conversion to a sequence of actions, referred\\n'\n",
      " 'to as a trajectory. Given a set of m dialogs,\\n'\n",
      " 'D = {d1,··· ,dm}, and after conversion to a set\\n'\n",
      " 'of action trajectories, Dt = {t1,··· ,tm}, the goal\\n'\n",
      " 'is to extract the common dialog flow by combin-\\n'\n",
      " 'ing all the trajectories in Dt. We represent the\\n'\n",
      " 'common dialog flow as a weighted actions tran-\\n'\n",
      " 'sition graph (Ferreira, 2023). 10 More precisely,\\n'\n",
      " 'the common flow is represented as a weighted\\n'\n",
      " 'graph GD = ⟨A,E,w A,wE⟩where Ais the set\\n'\n",
      " 'of actions, E represents edges between actions,\\n'\n",
      " 'the edge weight wE(ai,aj) ∈[0,1] indicates how\\n'\n",
      " 'often ai is followed by aj, and the action weight\\n'\n",
      " 'wA(ai) ∈[0,1] is its normalized frequency.\\n'\n",
      " '7.1 Evaluation Details\\n'\n",
      " 'For each domain in SpokenWOZ, we build and\\n'\n",
      " 'compare its reference graph GD against the in-\\n'\n",
      " 'duced graph ˆGD using different embeddings. The\\n'\n",
      " '9Slot names mismatch across domains is also partially\\n'\n",
      " 'affecting all results reported in SpokenWOZ (Tables 3 and 4).\\n'\n",
      " '10Even though having states as individual actions makes\\n'\n",
      " 'them non-Markovian, this graph is easy to interpret and di-\\n'\n",
      " 'rectly links the quality of individual actions to the overall\\n'\n",
      " 'flow’s quality.\\n'\n",
      " '5427 Rank DSE Sentence-BERT D2Fsingle\\n'\n",
      " '1. -uh my phone number is 7 4■ -okay may i have your phone number please□ '\n",
      " '-please get their phone number□\\n'\n",
      " '2. -okay okay now please get your number -may i get your phone number -okay '\n",
      " 'may i have your phone number please□\\n'\n",
      " '3. -okay may i have your phone number please□ -okay may i know your '\n",
      " 'telephone number please -okay may i know your telephone number please\\n'\n",
      " '4. -thank you on the phone number□ -okay can i please get your id number♣ '\n",
      " '-may i get your phone number\\n'\n",
      " '5. -okay may i know your telephone number please -okay may i have your phone '\n",
      " 'name in case for cooking\\n'\n",
      " 'the table⋆\\n'\n",
      " '-um can i please have their phone number□\\n'\n",
      " '6. -okay great emma please have your contact number -okay and may i have '\n",
      " 'your number please -okay so may i have the phone number with me\\n'\n",
      " '7. -my number is 2 10■ -okay and may i have your number please -okay i’m i '\n",
      " 'also need phone number□\\n'\n",
      " '8. -the number is you see♠ -okay and may i have your number please -no '\n",
      " 'problem um but for the information can i have\\n'\n",
      " 'your phone number\\n'\n",
      " '9. -okay and may i have your number please -okay and your car number♡ -thank '\n",
      " 'you on the phone number□\\n'\n",
      " '10. -okay and may i have your number please -this product uh may i have your '\n",
      " 'phone number please -okay can i get your phone number please to make that\\n'\n",
      " 'booking\\n'\n",
      " 'Table 5: Top-10 retrieved utterances on SpokenWOZ for the query\"your phone '\n",
      " 'please\" with action label [request\\n'\n",
      " 'phone_number]. Errors are highlighted in red with wrong action marked as: '\n",
      " '■[inform phone_number]; ♠[inform\\n'\n",
      " 'plate_number]; ♣[request id_number]; ⋆[request name]; ♡[request '\n",
      " 'plate_number]; □[request phone].\\n'\n",
      " 'Embeddings Taxi (31) Police (23) Hospital (18) Train (49) Restaurant (59) '\n",
      " 'Attraction (45) A VG.\\n'\n",
      " 'D2Fsingle 9.68% (+3) 4.35% (-1) 11.11% (-2) 2.04% (+1) 5.08% (-3) 8.89% (+4) '\n",
      " '6.86%\\n'\n",
      " 'D2Fjoint 3.23% (+1) 8.70% (-2) 5.56% (-1) 10.20% (-5) 23.73% (-14) 0.00% (0) '\n",
      " '8.57%\\n'\n",
      " 'D2F-Hardsingle 12.90% (-4) 26.09% (-6) 16.67% (-3) 10.20% (-5) 10.17% (-6) '\n",
      " '15.56% (+7) 15.26%\\n'\n",
      " 'D2F-Hardjoint 0.00% (0) 8.70% (-2) 33.33% (-6) 20.41% (-10) 25.42% (-15) '\n",
      " '13.33% (-6) 16.87%\\n'\n",
      " 'DSE 32.26% (-10) 17.39% (-4) 33.33% (-6) 30.61% (-15) 27.12% (-16) 26.67% '\n",
      " '(-12) 27.90%\\n'\n",
      " 'SPACE-2 32.26% (-10) 30.43% (-7) 38.89% (-7) 18.37% (-9) 32.20% (-19) 33.33% '\n",
      " '(-15) 30.91%\\n'\n",
      " 'DialoGPT 32.26% (-10) 34.78% (-8) 22.22% (-4) 44.90% (-22) 64.41% (-38) '\n",
      " '51.11% (-23) 41.61%\\n'\n",
      " 'BERT 54.84% (-17) 30.43% (-7) 22.22% (-4) 46.94% (-23) 59.32% (-35) 42.22% '\n",
      " '(-19) 42.66%\\n'\n",
      " 'OpenAI 54.84% (-17) 52.17% (-12) 55.56% (-10) 42.86% (-21) 49.15% (-29) '\n",
      " '44.44% (-20) 49.84%\\n'\n",
      " 'Sentence-BERT 48.39% (-15) 43.48% (-10) 55.56% (-10) 57.14% (-28) 50.85% '\n",
      " '(-30) 55.56% (-25) 51.83%\\n'\n",
      " 'GTR-T5 41.94% (-13) 43.48% (-10) 66.67% (-12) 51.02% (-25) 61.02% (-36) '\n",
      " '53.33% (-24) 52.91%\\n'\n",
      " 'SBD-BERT 77.42% (-24) 43.48% (-10) 38.89% (-7) 71.43% (-35) 86.44% (-51) '\n",
      " '86.67% (-39) 67.39%\\n'\n",
      " 'TOD-BERT 74.19% (-23) 78.26% (-18) 55.56% (-10) 85.71% (-42) 83.05% (-49) '\n",
      " '82.22% (-37) 76.50%\\n'\n",
      " 'Table 6: Comparison of induced graph size vs. reference graph size for each '\n",
      " 'single-domain in SpokenWOZ,\\n'\n",
      " 'measured by the number of nodes (actions). The table shows the normalized '\n",
      " 'absolute difference (%) and raw\\n'\n",
      " 'difference in parentheses. Column headers indicate the size of each '\n",
      " 'reference graph ( GD). Lower differences\\n'\n",
      " 'suggest a better match in graph complexity.\\n'\n",
      " 'reference graph GD is built from the trajectories\\n'\n",
      " 'Dt generated using the ground truth action labels\\n'\n",
      " '—e.g. Figure 2 is indeed Ghospital. In contrast, the\\n'\n",
      " 'induced graph ˆGD is built without any annotation\\n'\n",
      " 'by clustering all the utterance embeddings in D\\n'\n",
      " 'and using the cluster ids as action labels to gener-\\n'\n",
      " 'ate the trajectories ˆDt. That is, for GD, we have\\n'\n",
      " 'α(ui) = ai, while for ˆGD, we have α(ui) = ci\\n'\n",
      " 'where ci is the cluster id assigned to ui. To com-\\n'\n",
      " 'pare the induced and reference graphs, we report\\n'\n",
      " 'the difference in the number of nodes between them\\n'\n",
      " 'as the evaluation metric.11 Despite its simplicity,\\n'\n",
      " 'this metric allows us to compare the complexity of\\n'\n",
      " 'the induced vs. reference graph in terms of their\\n'\n",
      " 'sizes (i.e. the number of discovered/extracted ac-\\n'\n",
      " 'tions by each embedding model). Furthermore, to\\n'\n",
      " 'avoid the influence of infrequently occurring ut-\\n'\n",
      " 'terances/actions on graph size, we prune them by\\n'\n",
      " '11One cluster id ci can correspond to multiple ais and vice\\n'\n",
      " 'versa, preventing a direct comparison between ˆGD and GD.\\n'\n",
      " 'removing all nodes a with wA(a) < ϵ = 0 .02\\n'\n",
      " '(noise threshold).\\n'\n",
      " 'In practice, the total number of actions to cluster\\n'\n",
      " 'is unknown in advance. For instance, a hierarchical\\n'\n",
      " 'clustering algorithm can be used to approximate\\n'\n",
      " 'this number (see Appendix F). However, for eval-\\n'\n",
      " 'uation purposes, we set the number of clusters in\\n'\n",
      " 'each domain to be equal to the ground truth num-\\n'\n",
      " 'ber so that all the embeddings are evaluated under\\n'\n",
      " 'the same best-case scenario in which this number\\n'\n",
      " 'is known in advance. Therefore, all the induced\\n'\n",
      " 'graphs are built and processed equally, making the\\n'\n",
      " 'input embeddings the only factor influencing the\\n'\n",
      " 'final graph.\\n'\n",
      " '7.2 Dialog Flow Extraction Results\\n'\n",
      " 'Table 6 shows the results obtained when compar-\\n'\n",
      " 'ing the different extracted graphs. We can see that\\n'\n",
      " 'graphs obtained with available sentence embed-\\n'\n",
      " 'ding models tend to underestimate the complexity\\n'\n",
      " 'of each domain, producing less meaningful graphs\\n'\n",
      " '5428 with fewer states/actions than their references. We\\n'\n",
      " 'hypothesize this is due to available models group-\\n'\n",
      " 'ing the utterances either by conversational context\\n'\n",
      " 'or semantic similarity, thus, only allowing us to\\n'\n",
      " 'discover either semantic or conversational-context\\n'\n",
      " '\"steps\" (clusters/actions) in the dialogs from each\\n'\n",
      " 'domain. For instance, Figure A1 and A2 in Ap-\\n'\n",
      " 'pendix show the extracted graphs ˆGhospital with\\n'\n",
      " 'Sentence-BERT and DSE containing 10 and 6 less\\n'\n",
      " 'nodes (\"steps\") than the reference graph (Figure 2),\\n'\n",
      " 'respectively.\\n'\n",
      " 'Among the baseline embeddings, DSE stands\\n'\n",
      " 'out (27.90% average difference across domains),\\n'\n",
      " 'suggesting that conversational-context embeddings\\n'\n",
      " 'are better at capturing the communicative and infor-\\n'\n",
      " 'mative functions of dialog utterances than semanti-\\n'\n",
      " 'cally meaningful embeddings. Notably, D2F em-\\n'\n",
      " 'beddings trained with the proposed soft contrastive\\n'\n",
      " 'loss extract graphs closest in complexity to the\\n'\n",
      " 'references across domains (6.86% and 8.57% aver-\\n'\n",
      " 'age difference for D2Fsingle and D2Fjoint, respec-\\n'\n",
      " 'tively) compared to both D2F-Hard embeddings\\n'\n",
      " 'trained with the vanilla supervised contrastive loss\\n'\n",
      " 'and the other embeddings. For instance, Figure 4\\n'\n",
      " 'shows the corresponding ˆGhospital obtained with\\n'\n",
      " 'D2Fjoint.12 Finally, it is also worth noting that the\\n'\n",
      " 'D2F graphs are relatively consistent across differ-\\n'\n",
      " 'ent domains, even though some domains had only\\n'\n",
      " 'a small amount of in-domain data during training.\\n'\n",
      " 'For instance, the hospital and police domains\\n'\n",
      " 'make up only 0.11% and 0.07% of the training set\\n'\n",
      " '(details in Table A1).\\n'\n",
      " '8 Conclusions\\n'\n",
      " 'This paper introduced Dialog2Flow (D2F), embed-\\n'\n",
      " 'dings pre-trained for dialog flow extraction group-\\n'\n",
      " 'ing utterances by their communicative and informa-\\n'\n",
      " 'tive functions in a latent space. D2F embeddings\\n'\n",
      " 'were trained on a comprehensive dataset of twenty\\n'\n",
      " 'task-oriented dialog datasets with standardized ac-\\n'\n",
      " 'tion annotations, released along with this work.\\n'\n",
      " 'Future work will enhance D2F embeddings by\\n'\n",
      " 'exploring larger backbone models and advanced\\n'\n",
      " 'methods for sentence embeddings (Jiang et al.,\\n'\n",
      " '2023, 2022). We will also investigate more sophis-\\n'\n",
      " 'ticated techniques for extracting and representing\\n'\n",
      " 'dialog flows, such as using subtask graphs (Sohn\\n'\n",
      " 'et al., 2023) or adapting dependency parsing for\\n'\n",
      " '12Source code is provided to generate graphs for any given\\n'\n",
      " 'dialogue collection and any embedding model, allowing man-\\n'\n",
      " 'ual assessment of superior D2F graph quality.\\n'\n",
      " 'Figure 4: ˆGhospital graph obtained with D2Fjoint con-\\n'\n",
      " 'taining only one node less than the reference graph\\n'\n",
      " 'in Figure 2. Node labels correspond to the cluster ID\\n'\n",
      " 'along a representative utterance (the closest to the clus-\\n'\n",
      " 'ter centroid). Although not the exact same graph as the\\n'\n",
      " 'reference, this graph still allows us to understand the\\n'\n",
      " 'common flow of the conversations with a similar degree\\n'\n",
      " 'of detail: first, the user and system greet each other\\n'\n",
      " '(U0 and S6), then the user inform the reason of the call\\n'\n",
      " 'requesting the phone number of a department (U4), the\\n'\n",
      " 'agent may confirm the department (S7) or request more\\n'\n",
      " 'information (S4) before providing the phone number\\n'\n",
      " '(S2). The user may then either confirm the number (U3)\\n'\n",
      " 'or thank the system ( U5). Finally, the system asks if\\n'\n",
      " 'anything else is required ( S5), to which the user may\\n'\n",
      " 'either finish the conversation (U6) or, more likely, thank\\n'\n",
      " 'the system (U2) before the system says goodbye (S0).\\n'\n",
      " 'complex dialog structures (Qiu et al., 2020). Addi-\\n'\n",
      " 'tionally, potential applications include using D2F\\n'\n",
      " 'embeddings to ground LLMs in domain-specific\\n'\n",
      " 'flows for improved transparency and controllabil-\\n'\n",
      " 'ity (Raghu et al., 2021), and integrating D2F em-\\n'\n",
      " 'beddings into various TOD downstream tasks like\\n'\n",
      " 'dialog state tracking and policy learning.\\n'\n",
      " '5429 9 Limitations\\n'\n",
      " 'Our work represents a preliminary exploration with\\n'\n",
      " 'a focus on task-oriented dialogues (TODs) using a\\n'\n",
      " 'relatively simple encoder model. While this work\\n'\n",
      " 'aims to draw attention to this underexplored area,\\n'\n",
      " 'there are a number of limitations that must be ac-\\n'\n",
      " 'knowledged:\\n'\n",
      " '1. Scope of Dialogues: Our study is restricted\\n'\n",
      " 'to task-oriented dialogues. Consequently, the find-\\n'\n",
      " 'ings and methods may not generalize well to more\\n'\n",
      " 'complex and diverse types of dialogues, particu-\\n'\n",
      " 'larly those of a non-task-oriented nature. Future\\n'\n",
      " 'research should explore these methods in a broader\\n'\n",
      " 'range of dialogue types to assess their generaliz-\\n'\n",
      " 'ability.\\n'\n",
      " '2. Domain Specificity: The model has been\\n'\n",
      " 'trained on a specific collection of domains, dia-\\n'\n",
      " 'logue acts, and slots. This limits its ability to gen-\\n'\n",
      " 'eralize to unseen domains or dialogues that involve\\n'\n",
      " 'more complex and varied interactions. Expanding\\n'\n",
      " 'the range of training data to include a wider vari-\\n'\n",
      " 'ety of domains and dialogue types is necessary to\\n'\n",
      " 'improve the model’s robustness and applicability.\\n'\n",
      " '3. Model Complexity: The encoder model used\\n'\n",
      " 'in this work is relatively standard. There is poten-\\n'\n",
      " 'tial for improvement by employing larger and more\\n'\n",
      " 'advanced models to obtained the final sentence em-\\n'\n",
      " 'beddings.\\n'\n",
      " '4. Data Size: Despite being the largest dataset\\n'\n",
      " 'with standardized utterance annotations and the\\n'\n",
      " 'largest spoken TOD dataset, the datasets used in\\n'\n",
      " 'this study are limited in size. Larger datasets are\\n'\n",
      " 'necessary to fully explore and validate the proposed\\n'\n",
      " 'methods. We encourage the research community\\n'\n",
      " 'to build upon this work by utilizing more extensive\\n'\n",
      " 'datasets to enhance the reliability and validity of\\n'\n",
      " 'the results. For instance, perhaps named entity tags\\n'\n",
      " 'may be used as slots to expand annotation beyond\\n'\n",
      " 'pure task-oriented dialogues.\\n'\n",
      " '5. Evaluation Metrics: The evaluation met-\\n'\n",
      " 'rics employed in this study, while standard, may\\n'\n",
      " 'not capture all aspects of performance relevant to\\n'\n",
      " 'real-world applications. Developing and utilizing a\\n'\n",
      " 'broader set of evaluation metrics would provide a\\n'\n",
      " 'more comprehensive assessment of model perfor-\\n'\n",
      " 'mance. Specifically for dialogue flow evaluation,\\n'\n",
      " 'since there is not a standard metric yet, we encour-\\n'\n",
      " 'age the research community to explore better ways\\n'\n",
      " 'to represent and quantify the quality of dialogue\\n'\n",
      " 'flows.\\n'\n",
      " 'By highlighting these limitations, we hope to\\n'\n",
      " 'inspire further research that addresses these chal-\\n'\n",
      " 'lenges, leading to more robust and generalizable\\n'\n",
      " 'solutions building on top of this work.\\n'\n",
      " '10 Ethical Considerations\\n'\n",
      " 'We are committed to ensuring the ethical use of our\\n'\n",
      " 'research outcomes. To promote transparency and\\n'\n",
      " 'reproducibility, we will release the source code and\\n'\n",
      " 'pre-trained model weights under the MIT license.\\n'\n",
      " 'This allows for wide usage and adaptation while\\n'\n",
      " 'maintaining open-source principles.\\n'\n",
      " 'However, to prevent potential license incompat-\\n'\n",
      " 'ibilities among the various task-oriented dialogue\\n'\n",
      " '(TOD) datasets we have utilized, we will not re-\\n'\n",
      " 'lease our unified TOD dataset directly. Instead, we\\n'\n",
      " 'will provide a script that can generate the unified\\n'\n",
      " 'dataset introduced in this paper. This approach\\n'\n",
      " 'allows users to select the specific TOD datasets\\n'\n",
      " 'they wish to include, ensuring compliance with\\n'\n",
      " 'individual dataset licenses.\\n'\n",
      " 'We acknowledge that gender bias present in the\\n'\n",
      " 'original data could be partially encoded in the em-\\n'\n",
      " 'beddings. This may manifest as assumptions about\\n'\n",
      " 'the agent’s gender, such as the agent being male\\n'\n",
      " 'or female. We advise users to be aware of this\\n'\n",
      " 'potential bias and encourage further research to\\n'\n",
      " 'mitigate such issues. Continuous efforts to audit\\n'\n",
      " 'and address biases in data and models are essential\\n'\n",
      " 'to ensure fair and equitable AI systems.\\n'\n",
      " 'Acknowledgments\\n'\n",
      " 'This work was supported by EU Horizon\\n'\n",
      " '2020 project ELOQUENCE 13 (grant number\\n'\n",
      " '101070558). In addition, this work was inspired\\n'\n",
      " 'by insights gained from the 2023 Jelinek Memorial\\n'\n",
      " 'Summer Workshop on Speech and Language Tech-\\n'\n",
      " 'nologies (JSALT)14 and was partially supported\\n'\n",
      " 'with funds from Johns Hopkins University and EU\\n'\n",
      " 'project ESPERANTO (grant number 101007666).\\n'\n",
      " 'References\\n'\n",
      " 'Sébastien Bubeck, Varun Chandrasekaran, Ronen El-\\n'\n",
      " 'dan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\\n'\n",
      " 'Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\\n'\n",
      " 'berg, et al. 2023. Sparks of artificial general intelli-\\n'\n",
      " 'gence: Early experiments with GPT-4. arXiv preprint\\n'\n",
      " 'arXiv:2303.12712.\\n'\n",
      " 'Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\\n'\n",
      " 'Tseng, Iñigo Casanueva, Stefan Ultes, Osman Ra-\\n'\n",
      " '13https://eloquenceai.eu/\\n'\n",
      " '14https://jsalt2023.univ-lemans.fr\\n'\n",
      " '5430 madan, and Milica Gaši´c. 2018. MultiWOZ - a large-\\n'\n",
      " 'scale multi-domain Wizard-of-Oz dataset for task-\\n'\n",
      " 'oriented dialogue modelling. In Proceedings of the\\n'\n",
      " '2018 Conference on Empirical Methods in Natural\\n'\n",
      " 'Language Processing, pages 5016–5026, Brussels,\\n'\n",
      " 'Belgium. Association for Computational Linguistics.\\n'\n",
      " 'Bill Byrne, Karthik Krishnamoorthi, Chinnadhurai\\n'\n",
      " 'Sankar, Arvind Neelakantan, Ben Goodrich, Daniel\\n'\n",
      " 'Duckworth, Semih Yavuz, Amit Dubey, Kyu-Young\\n'\n",
      " 'Kim, and Andy Cedilnik. 2019. Taskmaster-1: To-\\n'\n",
      " 'ward a realistic and diverse dialog dataset. In Pro-\\n'\n",
      " 'ceedings of the 2019 Conference on Empirical Meth-\\n'\n",
      " 'ods in Natural Language Processing and the 9th In-\\n'\n",
      " 'ternational Joint Conference on Natural Language\\n'\n",
      " 'Processing (EMNLP-IJCNLP) , pages 4516–4525,\\n'\n",
      " 'Hong Kong, China. Association for Computational\\n'\n",
      " 'Linguistics.\\n'\n",
      " 'Daniel Cer, Yinfei Yang, Sheng yi Kong, Nan Hua,\\n'\n",
      " 'Nicole Limtiaco, Rhomni St. John, Noah Constant,\\n'\n",
      " 'Mario Guajardo-Cespedes, Steve Yuan, Chris Tar,\\n'\n",
      " 'Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil.\\n'\n",
      " '2018. Universal sentence encoder. Preprint,\\n'\n",
      " 'arXiv:1803.11175.\\n'\n",
      " 'Derek Chen, Howard Chen, Yi Yang, Alexander Lin,\\n'\n",
      " 'and Zhou Yu. 2021. Action-based conversations\\n'\n",
      " 'dataset: A corpus for building more in-depth task-\\n'\n",
      " 'oriented dialogue systems. In Proceedings of the\\n'\n",
      " '2021 Conference of the North American Chapter of\\n'\n",
      " 'the Association for Computational Linguistics: Hu-\\n'\n",
      " 'man Language Technologies, pages 3002–3017, On-\\n'\n",
      " 'line. Association for Computational Linguistics.\\n'\n",
      " 'Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\\n'\n",
      " '2024. Benchmarking large language models in\\n'\n",
      " 'retrieval-augmented generation. In Proceedings of\\n'\n",
      " 'the AAAI Conference on Artificial Intelligence, vol-\\n'\n",
      " 'ume 38, pages 17754–17762.\\n'\n",
      " 'Ting Chen, Simon Kornblith, Mohammad Norouzi, and\\n'\n",
      " 'Geoffrey Hinton. 2020. A simple framework for\\n'\n",
      " 'contrastive learning of visual representations. In In-\\n'\n",
      " 'ternational conference on machine learning, pages\\n'\n",
      " '1597–1607. PMLR.\\n'\n",
      " 'Wenhu Chen, Jianshu Chen, Pengda Qin, Xifeng Yan,\\n'\n",
      " 'and William Yang Wang. 2019. Semantically con-\\n'\n",
      " 'ditioned dialog response generation via hierarchical\\n'\n",
      " 'disentangled self-attention. In Proceedings of the\\n'\n",
      " '57th Annual Meeting of the Association for Computa-\\n'\n",
      " 'tional Linguistics, pages 3696–3709, Florence, Italy.\\n'\n",
      " 'Association for Computational Linguistics.\\n'\n",
      " 'Zhiyu Chen, Bing Liu, Seungwhan Moon, Chinnad-\\n'\n",
      " 'hurai Sankar, Paul Crook, and William Yang Wang.\\n'\n",
      " '2022. KETOD: Knowledge-enriched task-oriented\\n'\n",
      " 'dialogue. In Findings of the Association for Compu-\\n'\n",
      " 'tational Linguistics: NAACL 2022, pages 2581–2593,\\n'\n",
      " 'Seattle, United States. Association for Computational\\n'\n",
      " 'Linguistics.\\n'\n",
      " 'Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\\n'\n",
      " 'Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\\n'\n",
      " 'Plappert, Jerry Tworek, Jacob Hilton, Reiichiro\\n'\n",
      " 'Nakano, et al. 2021. Training verifiers to solve math\\n'\n",
      " 'word problems. arXiv preprint arXiv:2110.14168.\\n'\n",
      " 'Alexis Conneau, Douwe Kiela, Holger Schwenk, Loïc\\n'\n",
      " 'Barrault, and Antoine Bordes. 2017. Supervised\\n'\n",
      " 'learning of universal sentence representations from\\n'\n",
      " 'natural language inference data. In Proceedings of\\n'\n",
      " 'the 2017 Conference on Empirical Methods in Nat-\\n'\n",
      " 'ural Language Processing, pages 670–680, Copen-\\n'\n",
      " 'hagen, Denmark. Association for Computational Lin-\\n'\n",
      " 'guistics.\\n'\n",
      " 'Layla El Asri, Hannes Schulz, Shikhar Sharma, Jeremie\\n'\n",
      " 'Zumer, Justin Harris, Emery Fine, Rahul Mehrotra,\\n'\n",
      " 'and Kaheer Suleman. 2017. Frames: a corpus for\\n'\n",
      " 'adding memory to goal-oriented dialogue systems.\\n'\n",
      " 'In Proceedings of the 18th Annual SIGdial Meeting\\n'\n",
      " 'on Discourse and Dialogue , pages 207–219, Saar-\\n'\n",
      " 'brücken, Germany. Association for Computational\\n'\n",
      " 'Linguistics.\\n'\n",
      " 'Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi,\\n'\n",
      " 'Sanchit Agarwal, Shuyang Gao, Adarsh Kumar, Anuj\\n'\n",
      " 'Goyal, Peter Ku, and Dilek Hakkani-Tur. 2020. Mul-\\n'\n",
      " 'tiWOZ 2.1: A consolidated multi-domain dialogue\\n'\n",
      " 'dataset with state corrections and state tracking base-\\n'\n",
      " 'lines. In Proceedings of the Twelfth Language Re-\\n'\n",
      " 'sources and Evaluation Conference, pages 422–428,\\n'\n",
      " 'Marseille, France. European Language Resources\\n'\n",
      " 'Association.\\n'\n",
      " 'Kawin Ethayarajh. 2019. How contextual are contextu-\\n'\n",
      " 'alized word representations? Comparing the geom-\\n'\n",
      " 'etry of BERT, ELMo, and GPT-2 embeddings. In\\n'\n",
      " 'Proceedings of the 2019 Conference on Empirical\\n'\n",
      " 'Methods in Natural Language Processing and the\\n'\n",
      " '9th International Joint Conference on Natural Lan-\\n'\n",
      " 'guage Processing (EMNLP-IJCNLP), pages 55–65,\\n'\n",
      " 'Hong Kong, China. Association for Computational\\n'\n",
      " 'Linguistics.\\n'\n",
      " 'Patrícia Ferreira. 2023. Automatic dialog flow extrac-\\n'\n",
      " 'tion and guidance. In Proceedings of the 17th Confer-\\n'\n",
      " 'ence of the European Chapter of the Association for\\n'\n",
      " 'Computational Linguistics: Student Research Work-\\n'\n",
      " 'shop, pages 112–122, Dubrovnik, Croatia. Associa-\\n'\n",
      " 'tion for Computational Linguistics.\\n'\n",
      " 'Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\\n'\n",
      " 'SimCSE: Simple contrastive learning of sentence em-\\n'\n",
      " 'beddings. In Proceedings of the 2021 Conference\\n'\n",
      " 'on Empirical Methods in Natural Language Process-\\n'\n",
      " 'ing, pages 6894–6910, Online and Punta Cana, Do-\\n'\n",
      " 'minican Republic. Association for Computational\\n'\n",
      " 'Linguistics.\\n'\n",
      " 'James Gung, Raphael Shu, Emily Moeng, Wesley Rose,\\n'\n",
      " 'Salvatore Romeo, Arshit Gupta, Yassine Benajiba,\\n'\n",
      " 'Saab Mansour, and Yi Zhang. 2023. Intent induction\\n'\n",
      " 'from conversations for task-oriented dialogue track\\n'\n",
      " 'at DSTC 11. In Proceedings of The Eleventh Dia-\\n'\n",
      " 'log System Technology Challenge, pages 242–259,\\n'\n",
      " 'Prague, Czech Republic. Association for Computa-\\n'\n",
      " 'tional Linguistics.\\n'\n",
      " '5431 Suchin Gururangan, Ana Marasovi ´c, Swabha\\n'\n",
      " 'Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\\n'\n",
      " 'and Noah A. Smith. 2020. Don’t stop pretraining:\\n'\n",
      " 'Adapt language models to domains and tasks. In\\n'\n",
      " 'Proceedings of the 58th Annual Meeting of the\\n'\n",
      " 'Association for Computational Linguistics , pages\\n'\n",
      " '8342–8360, Online. Association for Computational\\n'\n",
      " 'Linguistics.\\n'\n",
      " 'Amine El Hattami, Issam H. Laradji, Stefania Rai-\\n'\n",
      " 'mondo, David Vázquez, Pau Rodríguez, and Christo-\\n'\n",
      " 'pher Pal. 2023. Workflow discovery from dialogues\\n'\n",
      " 'in the low data regime. Transactions on Machine\\n'\n",
      " 'Learning Research, 2023.\\n'\n",
      " 'Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\\n'\n",
      " 'Ross Girshick. 2020. Momentum contrast for un-\\n'\n",
      " 'supervised visual representation learning. In 2020\\n'\n",
      " 'IEEE/CVF Conference on Computer Vision and Pat-\\n'\n",
      " 'tern Recognition (CVPR), pages 9726–9735.\\n'\n",
      " 'Wanwei He, Yinpei Dai, Binyuan Hui, Min Yang, Zheng\\n'\n",
      " 'Cao, Jianbo Dong, Fei Huang, Luo Si, and Yongbin\\n'\n",
      " 'Li. 2022. SPACE-2: Tree-structured semi-supervised\\n'\n",
      " 'contrastive pre-training for task-oriented dialog un-\\n'\n",
      " 'derstanding. In Proceedings of the 29th International\\n'\n",
      " 'Conference on Computational Linguistics, pages 553–\\n'\n",
      " '569, Gyeongju, Republic of Korea. International\\n'\n",
      " 'Committee on Computational Linguistics.\\n'\n",
      " 'Olivier Henaff. 2020. Data-efficient image recognition\\n'\n",
      " 'with contrastive predictive coding. In International\\n'\n",
      " 'conference on machine learning, pages 4182–4192.\\n'\n",
      " 'PMLR.\\n'\n",
      " 'Dan Hendrycks, Collin Burns, Steven Basart, Andy\\n'\n",
      " 'Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-\\n'\n",
      " 'hardt. 2021a. Measuring massive multitask language\\n'\n",
      " 'understanding. Proceedings of the International Con-\\n'\n",
      " 'ference on Learning Representations (ICLR).\\n'\n",
      " 'Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul\\n'\n",
      " 'Arora, Steven Basart, Eric Tang, Dawn Song, and\\n'\n",
      " 'Jacob Steinhardt. 2021b. Measuring mathematical\\n'\n",
      " 'problem solving with the math dataset. NeurIPS.\\n'\n",
      " 'R Devon Hjelm, Alex Fedorov, Samuel Lavoie-\\n'\n",
      " 'Marchildon, Karan Grewal, Phil Bachman, Adam\\n'\n",
      " 'Trischler, and Yoshua Bengio. 2019. Learning deep\\n'\n",
      " 'representations by mutual information estimation and\\n'\n",
      " 'maximization. In International Conference on Learn-\\n'\n",
      " 'ing Representations.\\n'\n",
      " 'Ting Jiang, Shaohan Huang, Zhongzhi Luan, Deqing\\n'\n",
      " 'Wang, and Fuzhen Zhuang. 2023. Scaling sentence\\n'\n",
      " 'embeddings with large language models. Preprint,\\n'\n",
      " 'arXiv:2307.16645.\\n'\n",
      " 'Ting Jiang, Jian Jiao, Shaohan Huang, Zihan Zhang,\\n'\n",
      " 'Deqing Wang, Fuzhen Zhuang, Furu Wei, Haizhen\\n'\n",
      " 'Huang, Denvy Deng, and Qi Zhang. 2022. Prompt-\\n'\n",
      " 'BERT: Improving BERT sentence embeddings with\\n'\n",
      " 'prompts. In Proceedings of the 2022 Conference on\\n'\n",
      " 'Empirical Methods in Natural Language Processing,\\n'\n",
      " 'pages 8826–8837, Abu Dhabi, United Arab Emirates.\\n'\n",
      " 'Association for Computational Linguistics.\\n'\n",
      " 'Daniel Jurafsky. 2006. Pragmatics and computational\\n'\n",
      " 'linguistics. The handbook of pragmatics, pages 578–\\n'\n",
      " '604.\\n'\n",
      " 'Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron\\n'\n",
      " 'Sarna, Yonglong Tian, Phillip Isola, Aaron\\n'\n",
      " 'Maschinot, Ce Liu, and Dilip Krishnan. 2020. Su-\\n'\n",
      " 'pervised contrastive learning. In Advances in Neural\\n'\n",
      " 'Information Processing Systems, volume 33, pages\\n'\n",
      " '18661–18673. Curran Associates, Inc.\\n'\n",
      " 'Ryan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard\\n'\n",
      " 'Zemel, Raquel Urtasun, Antonio Torralba, and Sanja\\n'\n",
      " 'Fidler. 2015. Skip-thought vectors. In Advances in\\n'\n",
      " 'Neural Information Processing Systems, volume 28.\\n'\n",
      " 'Curran Associates, Inc.\\n'\n",
      " 'Xiujun Li, Sarah Panda, JJ (Jingjing) Liu, and Jianfeng\\n'\n",
      " 'Gao. 2018. Microsoft dialogue challenge: Building\\n'\n",
      " 'end-to-end task-completion dialogue systems. In SLT\\n'\n",
      " '2018.\\n'\n",
      " 'Zhaojiang Lin, Andrea Madotto, Genta Winata, Peng\\n'\n",
      " 'Xu, Feijun Jiang, Yuxiang Hu, Chen Shi, and Pas-\\n'\n",
      " 'cale N Fung. 2021. BiToD: A bilingual multi-domain\\n'\n",
      " 'dataset for task-oriented dialogue modeling. In Pro-\\n'\n",
      " 'ceedings of the Neural Information Processing Sys-\\n'\n",
      " 'tems Track on Datasets and Benchmarks, volume 1.\\n'\n",
      " 'Che Liu, Rui Wang, Jinghua Liu, Jian Sun, Fei Huang,\\n'\n",
      " 'and Luo Si. 2021. DialogueCSE: Dialogue-based\\n'\n",
      " 'contrastive learning of sentence embeddings. In Pro-\\n'\n",
      " 'ceedings of the 2021 Conference on Empirical Meth-\\n'\n",
      " 'ods in Natural Language Processing , pages 2396–\\n'\n",
      " '2406, Online and Punta Cana, Dominican Republic.\\n'\n",
      " 'Association for Computational Linguistics.\\n'\n",
      " 'Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-\\n'\n",
      " 'Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\\n'\n",
      " 'Clark, and Ashwin Kalyan. 2022. Learn to explain:\\n'\n",
      " 'Multimodal reasoning via thought chains for science\\n'\n",
      " 'question answering. In The 36th Conference on Neu-\\n'\n",
      " 'ral Information Processing Systems (NeurIPS).\\n'\n",
      " 'Nikola Mrkši ´c, Diarmuid Ó Séaghdha, Tsung-Hsien\\n'\n",
      " 'Wen, Blaise Thomson, and Steve Young. 2017. Neu-\\n'\n",
      " 'ral belief tracker: Data-driven dialogue state tracking.\\n'\n",
      " 'In Proceedings of the 55th Annual Meeting of the\\n'\n",
      " 'Association for Computational Linguistics (Volume 1:\\n'\n",
      " 'Long Papers), pages 1777–1788, Vancouver, Canada.\\n'\n",
      " 'Association for Computational Linguistics.\\n'\n",
      " 'Arvind Neelakantan, Tao Xu, Raul Puri, Alec Rad-\\n'\n",
      " 'ford, Jesse Michael Han, Jerry Tworek, Qiming Yuan,\\n'\n",
      " 'Nikolas Tezak, Jong Wook Kim, Chris Hallacy, et al.\\n'\n",
      " '2022. Text and code embeddings by contrastive pre-\\n'\n",
      " 'training. arXiv preprint arXiv:2201.10005.\\n'\n",
      " 'Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo\\n'\n",
      " 'Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan,\\n'\n",
      " 'Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022.\\n'\n",
      " 'Large dual encoders are generalizable retrievers. In\\n'\n",
      " 'Proceedings of the 2022 Conference on Empirical\\n'\n",
      " 'Methods in Natural Language Processing , pages\\n'\n",
      " '9844–9855, Abu Dhabi, United Arab Emirates. As-\\n'\n",
      " 'sociation for Computational Linguistics.\\n'\n",
      " '5432 OpenAI. 2024. New embedding models and api up-\\n'\n",
      " 'dates.\\n'\n",
      " 'Jeffrey Pennington, Richard Socher, and Christopher\\n'\n",
      " 'Manning. 2014. GloVe: Global vectors for word\\n'\n",
      " 'representation. In Proceedings of the 2014 Confer-\\n'\n",
      " 'ence on Empirical Methods in Natural Language Pro-\\n'\n",
      " 'cessing (EMNLP), pages 1532–1543, Doha, Qatar.\\n'\n",
      " 'Association for Computational Linguistics.\\n'\n",
      " 'Denis Peskov, Nancy Clarke, Jason Krone, Brigi Fodor,\\n'\n",
      " 'Yi Zhang, Adel Youssef, and Mona Diab. 2019.\\n'\n",
      " 'Multi-domain goal-oriented dialogues (MultiDoGO):\\n'\n",
      " 'Strategies toward curating and annotating large scale\\n'\n",
      " 'dialogue data. In Proceedings of the 2019 Confer-\\n'\n",
      " 'ence on Empirical Methods in Natural Language Pro-\\n'\n",
      " 'cessing and the 9th International Joint Conference\\n'\n",
      " 'on Natural Language Processing (EMNLP-IJCNLP),\\n'\n",
      " 'pages 4526–4536, Hong Kong, China. Association\\n'\n",
      " 'for Computational Linguistics.\\n'\n",
      " 'Kun Qian, Satwik Kottur, Ahmad Beirami, Shahin\\n'\n",
      " 'Shayandeh, Paul Crook, Alborz Geramifard, Zhou\\n'\n",
      " 'Yu, and Chinnadhurai Sankar. 2022. Database search\\n'\n",
      " 'results disambiguation for task-oriented dialog sys-\\n'\n",
      " 'tems. In Proceedings of the 2022 Conference of\\n'\n",
      " 'the North American Chapter of the Association for\\n'\n",
      " 'Computational Linguistics: Human Language Tech-\\n'\n",
      " 'nologies, pages 1158–1173, Seattle, United States.\\n'\n",
      " 'Association for Computational Linguistics.\\n'\n",
      " 'Liang Qiu, Chien-Sheng Wu, Wenhao Liu, and Caiming\\n'\n",
      " 'Xiong. 2022. Structure extraction in task-oriented\\n'\n",
      " 'dialogues with slot clustering. arXiv preprint\\n'\n",
      " 'arXiv:2203.00073.\\n'\n",
      " 'Liang Qiu, Yizhou Zhao, Weiyan Shi, Yuan Liang, Feng\\n'\n",
      " 'Shi, Tao Yuan, Zhou Yu, and Song-Chun Zhu. 2020.\\n'\n",
      " 'Structured attention for unsupervised dialogue struc-\\n'\n",
      " 'ture induction. In Proceedings of the 2020 Confer-\\n'\n",
      " 'ence on Empirical Methods in Natural Language\\n'\n",
      " 'Processing (EMNLP), pages 1889–1899, Online. As-\\n'\n",
      " 'sociation for Computational Linguistics.\\n'\n",
      " 'Jun Quan, Deyi Xiong, Bonnie Webber, and Changjian\\n'\n",
      " 'Hu. 2019. GECOR: An end-to-end generative el-\\n'\n",
      " 'lipsis and co-reference resolution model for task-\\n'\n",
      " 'oriented dialogue. In Proceedings of the 2019 Confer-\\n'\n",
      " 'ence on Empirical Methods in Natural Language Pro-\\n'\n",
      " 'cessing and the 9th International Joint Conference\\n'\n",
      " 'on Natural Language Processing (EMNLP-IJCNLP),\\n'\n",
      " 'pages 4547–4557, Hong Kong, China. Association\\n'\n",
      " 'for Computational Linguistics.\\n'\n",
      " 'Dinesh Raghu, Shantanu Agarwal, Sachindra Joshi, and\\n'\n",
      " 'Mausam. 2021. End-to-end learning of flowchart\\n'\n",
      " 'grounded task-oriented dialogs. In Proceedings of\\n'\n",
      " 'the 2021 Conference on Empirical Methods in Natu-\\n'\n",
      " 'ral Language Processing, pages 4348–4366, Online\\n'\n",
      " 'and Punta Cana, Dominican Republic. Association\\n'\n",
      " 'for Computational Linguistics.\\n'\n",
      " 'Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara,\\n'\n",
      " 'Raghav Gupta, and Pranav Khaitan. 2020. Towards\\n'\n",
      " 'scalable multi-domain conversational agents: The\\n'\n",
      " 'schema-guided dialogue dataset. In Proceedings of\\n'\n",
      " 'the AAAI conference on artificial intelligence , vol-\\n'\n",
      " 'ume 34, pages 8689–8696.\\n'\n",
      " 'Nils Reimers and Iryna Gurevych. 2019. Sentence-\\n'\n",
      " 'BERT: Sentence embeddings using Siamese BERT-\\n'\n",
      " 'networks. In Proceedings of the 2019 Conference on\\n'\n",
      " 'Empirical Methods in Natural Language Processing\\n'\n",
      " 'and the 9th International Joint Conference on Natu-\\n'\n",
      " 'ral Language Processing (EMNLP-IJCNLP), pages\\n'\n",
      " '3982–3992, Hong Kong, China. Association for Com-\\n'\n",
      " 'putational Linguistics.\\n'\n",
      " 'Pararth Shah, Dilek Hakkani-Tür, Bing Liu, and Gokhan\\n'\n",
      " 'Tür. 2018. Bootstrapping a neural conversational\\n'\n",
      " 'agent with dialogue self-play, crowdsourcing and\\n'\n",
      " 'on-line reinforcement learning. In Proceedings of\\n'\n",
      " 'the 2018 Conference of the North American Chap-\\n'\n",
      " 'ter of the Association for Computational Linguistics:\\n'\n",
      " 'Human Language Technologies, Volume 3 (Indus-\\n'\n",
      " 'try Papers), pages 41–51, New Orleans - Louisiana.\\n'\n",
      " 'Association for Computational Linguistics.\\n'\n",
      " 'Shuzheng Si, Wentao Ma, Haoyu Gao, Yuchuan Wu,\\n'\n",
      " 'Ting-En Lin, Yinpei Dai, Hangyu Li, Rui Yan, Fei\\n'\n",
      " 'Huang, and Yongbin Li. 2023. SpokenWOZ: A large-\\n'\n",
      " 'scale speech-text benchmark for spoken task-oriented\\n'\n",
      " 'dialogue agents. In Thirty-seventh Conference on\\n'\n",
      " 'Neural Information Processing Systems Datasets and\\n'\n",
      " 'Benchmarks Track.\\n'\n",
      " 'Jake Snell, Kevin Swersky, and Richard Zemel. 2017.\\n'\n",
      " 'Prototypical networks for few-shot learning. Ad-\\n'\n",
      " 'vances in neural information processing systems, 30.\\n'\n",
      " 'Sungryull Sohn, Yiwei Lyu, Anthony Liu, Lajanugen\\n'\n",
      " 'Logeswaran, Dong-Ki Kim, Dongsub Shim, and\\n'\n",
      " 'Honglak Lee. 2023. TOD-Flow: Modeling the struc-\\n'\n",
      " 'ture of task-oriented dialogues. In Proceedings of the\\n'\n",
      " '2023 Conference on Empirical Methods in Natural\\n'\n",
      " 'Language Processing, pages 3355–3371, Singapore.\\n'\n",
      " 'Association for Computational Linguistics.\\n'\n",
      " 'Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan\\n'\n",
      " 'Liu. 2020. Mpnet: masked and permuted pre-training\\n'\n",
      " 'for language understanding. In Proceedings of the\\n'\n",
      " '34th International Conference on Neural Information\\n'\n",
      " 'Processing Systems, NIPS ’20, Red Hook, NY , USA.\\n'\n",
      " 'Curran Associates Inc.\\n'\n",
      " 'Makesh Narsimhan Sreedhar, Traian Rebedea, and\\n'\n",
      " 'Christopher Parisien. 2024. Unsupervised extrac-\\n'\n",
      " 'tion of dialogue policies from conversations. arXiv\\n'\n",
      " 'preprint arXiv:2406.15214.\\n'\n",
      " 'Yajing Sun, Yong Shan, Chengguang Tang, Yue Hu,\\n'\n",
      " 'Yinpei Dai, Jing Yu, Jian Sun, Fei Huang, and Luo\\n'\n",
      " 'Si. 2021. Unsupervised learning of deterministic\\n'\n",
      " 'dialogue structure with edge-enhanced graph auto-\\n'\n",
      " 'encoder. In Proceedings of the AAAI conference\\n'\n",
      " 'on artificial intelligence, volume 35, pages 13869–\\n'\n",
      " '13877.\\n'\n",
      " 'Nandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\\n'\n",
      " 'hishek Srivastava, and Iryna Gurevych. 2021. BEIR:\\n'\n",
      " 'A heterogeneous benchmark for zero-shot evaluation\\n'\n",
      " '5433 of information retrieval models. In Thirty-fifth Con-\\n'\n",
      " 'ference on Neural Information Processing Systems\\n'\n",
      " 'Datasets and Benchmarks Track (Round 2).\\n'\n",
      " 'Yonglong Tian, Dilip Krishnan, and Phillip Isola. 2020.\\n'\n",
      " 'Contrastive multiview coding. In Computer Vision–\\n'\n",
      " 'ECCV 2020: 16th European Conference, Glasgow,\\n'\n",
      " 'UK, August 23–28, 2020, Proceedings, Part XI 16 ,\\n'\n",
      " 'pages 776–794. Springer.\\n'\n",
      " 'Chien-Sheng Wu, Steven C.H. Hoi, Richard Socher,\\n'\n",
      " 'and Caiming Xiong. 2020. TOD-BERT: Pre-trained\\n'\n",
      " 'natural language understanding for task-oriented di-\\n'\n",
      " 'alogue. In Proceedings of the 2020 Conference on\\n'\n",
      " 'Empirical Methods in Natural Language Processing\\n'\n",
      " '(EMNLP), pages 917–929, Online. Association for\\n'\n",
      " 'Computational Linguistics.\\n'\n",
      " 'Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara,\\n'\n",
      " 'Raghav Gupta, Jianguo Zhang, and Jindong Chen.\\n'\n",
      " '2020. MultiWOZ 2.2 : A dialogue dataset with\\n'\n",
      " 'additional annotation corrections and state tracking\\n'\n",
      " 'baselines. In Proceedings of the 2nd Workshop on\\n'\n",
      " 'Natural Language Processing for Conversational AI,\\n'\n",
      " 'pages 109–117, Online. Association for Computa-\\n'\n",
      " 'tional Linguistics.\\n'\n",
      " 'Dejiao Zhang, Shang-Wen Li, Wei Xiao, Henghui Zhu,\\n'\n",
      " 'Ramesh Nallapati, Andrew O. Arnold, and Bing Xi-\\n'\n",
      " 'ang. 2021. Pairwise supervised contrastive learning\\n'\n",
      " 'of sentence representations. In Proceedings of the\\n'\n",
      " '2021 Conference on Empirical Methods in Natural\\n'\n",
      " 'Language Processing, pages 5786–5798, Online and\\n'\n",
      " 'Punta Cana, Dominican Republic. Association for\\n'\n",
      " 'Computational Linguistics.\\n'\n",
      " 'Dejiao Zhang, Wei Xiao, Henghui Zhu, Xiaofei Ma,\\n'\n",
      " 'and Andrew Arnold. 2022. Virtual augmentation\\n'\n",
      " 'supported contrastive learning of sentence represen-\\n'\n",
      " 'tations. In Findings of the Association for Com-\\n'\n",
      " 'putational Linguistics: ACL 2022 , pages 864–876,\\n'\n",
      " 'Dublin, Ireland. Association for Computational Lin-\\n'\n",
      " 'guistics.\\n'\n",
      " 'Jianguo Zhang, Kun Qian, Zhiwei Liu, Shelby Heinecke,\\n'\n",
      " 'Rui Meng, Ye Liu, Zhou Yu, Huan Wang, Silvio\\n'\n",
      " 'Savarese, and Caiming Xiong. 2024. DialogStudio:\\n'\n",
      " 'Towards richest and most diverse unified dataset col-\\n'\n",
      " 'lection for conversational AI. In Findings of the Asso-\\n'\n",
      " 'ciation for Computational Linguistics: EACL 2024,\\n'\n",
      " 'pages 2299–2315, St. Julian’s, Malta. Association\\n'\n",
      " 'for Computational Linguistics.\\n'\n",
      " 'Yizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\\n'\n",
      " 'Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\\n'\n",
      " 'Liu, and Bill Dolan. 2020. DialoGPT : Large-scale\\n'\n",
      " 'generative pre-training for conversational response\\n'\n",
      " 'generation. In Proceedings of the 58th Annual Meet-\\n'\n",
      " 'ing of the Association for Computational Linguistics:\\n'\n",
      " 'System Demonstrations, pages 270–278, Online. As-\\n'\n",
      " 'sociation for Computational Linguistics.\\n'\n",
      " 'Zhihan Zhou, Dejiao Zhang, Wei Xiao, Nicholas Ding-\\n'\n",
      " 'wall, Xiaofei Ma, Andrew Arnold, and Bing Xiang.\\n'\n",
      " '2022. Learning dialogue representations from con-\\n'\n",
      " 'secutive utterances. In Proceedings of the 2022 Con-\\n'\n",
      " 'ference of the North American Chapter of the Asso-\\n'\n",
      " 'ciation for Computational Linguistics: Human Lan-\\n'\n",
      " 'guage Technologies, pages 754–768, Seattle, United\\n'\n",
      " 'States. Association for Computational Linguistics.\\n'\n",
      " '5434 A Unified TOD Dataset\\n'\n",
      " 'Dialog acts: inform(64.66%) request(12.62%) offer(6.62%)\\n'\n",
      " 'inform_success(3.07%) good_bye(2.67%) agreement(2.45%)\\n'\n",
      " 'thank_you(2.25%) confirm(2.10%) disagreement(1.60%)\\n'\n",
      " 'request_more(1.06%) request_alternative(0.90%) recommendation(0.70%)\\n'\n",
      " 'inform_failure(0.64%) greeting(0.31%) confirm_answer(0.18%)\\n'\n",
      " 'confirm_question(0.17%) request_update(0.02%) request_compare(0.01%)\\n'\n",
      " 'Domains: movie(32.98%) restaurant(13.48%) hotel(10.15%)\\n'\n",
      " 'train(4.52%) flight(4.30%) event(3.56%) attraction(3.50%)\\n'\n",
      " 'service(2.44%) bus(2.28%) taxi(2.21%) rentalcars(2.20%)\\n'\n",
      " 'travel(2.16%) music(1.81%) medium(1.66%) ridesharing(1.30%)\\n'\n",
      " 'booking(1.21%) home(1.01%) finance(0.79%) airline(0.69%)\\n'\n",
      " 'calendar(0.69%) fastfood(0.68%) insurance(0.61%) weather(0.58%)\\n'\n",
      " 'bank(0.47%) hkmtr(0.36%) mlb(0.35%) ml(0.31%) food(0.30%)\\n'\n",
      " 'epl(0.30%) pizza(0.25%) coffee(0.24%) uber(0.24%)\\n'\n",
      " 'software(0.23%) auto(0.21%) nba(0.20%) product_defect(0.17%)\\n'\n",
      " 'shipping_issue(0.16%) alarm(0.13%) order_issue(0.13%) messaging(0.13%)\\n'\n",
      " 'hospital(0.11%) subscription_inquiry(0.11%) account_access(0.11%)\\n'\n",
      " 'payment(0.10%) purchase_dispute(0.10%) nfl(0.09%) chat(0.08%)\\n'\n",
      " 'police(0.07%) single_item_query(0.06%) storewide_query(0.06%)\\n'\n",
      " 'troubleshoot_site(0.06%) manage_account(0.06%)\\n'\n",
      " 'Table A1: Standardized dialog act and domain labels in\\n'\n",
      " 'our unified TOD datasets, ordered by their proportion\\n'\n",
      " 'of utterances.\\n'\n",
      " 'Our training data is sourced from a diverse range\\n'\n",
      " 'of TOD datasets meticulously curated in DialogStu-\\n'\n",
      " 'dio (Zhang et al., 2024). DialogStudio comprises\\n'\n",
      " 'over 80 dialog datasets, with 30 focusing on task-\\n'\n",
      " 'oriented conversations. We conducted a compre-\\n'\n",
      " 'hensive manual analysis of these 30 TOD datasets\\n'\n",
      " 'to identify those from which we could extract dia-\\n'\n",
      " 'log act and/or slot annotations. From this analysis,\\n'\n",
      " 'we identified 20 datasets that met our criteria, as\\n'\n",
      " 'summarized in Table 1. The datasets in DialogStu-\\n'\n",
      " 'dio are unified under a consistent format while\\n'\n",
      " 'retaining their original information. However, this\\n'\n",
      " 'format only unifies the access to the conversations\\n'\n",
      " 'per se, omitting annotations and components of\\n'\n",
      " 'task-oriented dialogs. We then manually inspected\\n'\n",
      " 'each dataset to locate and extract the necessary an-\\n'\n",
      " 'notations. This process involved identifying where\\n'\n",
      " 'and how annotations were stored originally in each\\n'\n",
      " 'dataset, extracting dialog act and/or slot annota-\\n'\n",
      " 'tions for each turn, either explicitly or implicitly\\n'\n",
      " 'by keeping track of the changes in the dialog state\\n'\n",
      " 'annotation from one turn to the next, and standard-\\n'\n",
      " 'izing domain names and dialog act labels across\\n'\n",
      " 'datasets.\\n'\n",
      " 'To standardize dialog act labels, we mapped the\\n'\n",
      " '44 unique labels found across datasets to 18 nor-\\n'\n",
      " 'malized dialog act labels, informed by the semantic\\n'\n",
      " 'meaning described in the original dataset papers\\n'\n",
      " '(mapping detailed in Table A3). After this process,\\n'\n",
      " 'we unified all datasets under a consistent format,\\n'\n",
      " 'detailed in the next subsection, incorporating per-\\n'\n",
      " 'turn dialog act and slot annotations. The resulting\\n'\n",
      " 'unified TOD dataset comprises 3.4 million utter-\\n'\n",
      " 'ances annotated with 18 standardized dialog acts,\\n'\n",
      " '524 unique slot labels, and 3,982 unique action\\n'\n",
      " 'labels (dialog act + slots). These annotations span\\n'\n",
      " 'across 52 different domains, as detailed in Table 1.\\n'\n",
      " 'Our unified TOD dataset is a valuable resource\\n'\n",
      " 'providing a comprehensive and standardized collec-\\n'\n",
      " 'tion of annotated utterances across diverse domains\\n'\n",
      " 'under a common format.\\n'\n",
      " 'A.1 Dataset Format\\n'\n",
      " 'Our unified dataset standardizes the TOD datasets\\n'\n",
      " 'into the following common JSON format with per-\\n'\n",
      " 'utterance annotations:\\n'\n",
      " '{\\n'\n",
      " '\" s t a t s \" : { \" domains \" : { . . . } ,\\n'\n",
      " '\" l a b e l s \" : { . . } } ,\\n'\n",
      " '\" d i a l o g s \" : {\\n'\n",
      " '\"<DIALOGUE_ID0 >\" : [\\n'\n",
      " '{\\n'\n",
      " '\" s p e a k e r \" : <SPEAKER> ,\\n'\n",
      " '\" t e x t \" : <RAW_UTTERANCE> ,\\n'\n",
      " '\" domains \" : [ . . . ] ,\\n'\n",
      " '\" l a b e l s \" : {\\n'\n",
      " '\" d i a l o g _ a c t s \" : {\\n'\n",
      " '\" a c t s \" : [ . . . ] ,\\n'\n",
      " '\" m a i n _ a c t s \" : [ . . . ] ,\\n'\n",
      " '\" o r i g i n a l _ a c t s \" : [ . . . ] ,\\n'\n",
      " '} ,\\n'\n",
      " '\" s l o t s \" : [ . . . ] ,\\n'\n",
      " '\" i n t e n t s \" : [ . . . ]\\n'\n",
      " '}\\n'\n",
      " '} ,\\n'\n",
      " '. . .\\n'\n",
      " '] ,\\n'\n",
      " '\"<DIALOGUE_ID1 >\" : [ . . . ] ,\\n'\n",
      " '. . .\\n'\n",
      " '}\\n'\n",
      " '}\\n'\n",
      " 'The JSON structure has two main parts: a\\n'\n",
      " '\"stats\" header and a \"dialogs\" body. The\\n'\n",
      " '\"stats\" field provides statistics about the labels\\n'\n",
      " 'and domains in the dataset. The \"dialogs\"\\n'\n",
      " 'field contains dialog IDs, each linked to a list\\n'\n",
      " 'of annotated utterance objects. Each utterance\\n'\n",
      " 'object includes its speaker, text, domains, and\\n'\n",
      " 'associated labels for dialog acts, slots, and in-\\n'\n",
      " 'tents. Dialog act labels contain the original labels\\n'\n",
      " '(\"original_acts\") as well as their standardized\\n'\n",
      " 'values (\"acts\") and parent values (\"main_acts\")\\n'\n",
      " 'as mapped in Table A3.\\n'\n",
      " 'B Training Details\\n'\n",
      " 'Following the experimental setup of DSE (Zhou\\n'\n",
      " 'et al., 2022) and TOD-BERT (Wu et al., 2020),\\n'\n",
      " 'we set the contrastive head dimension to d= 128\\n'\n",
      " 'and use BERTbase as the backbone model for the\\n'\n",
      " 'encoder15. Additional configurations reported in\\n'\n",
      " '15Thus, the embedding size is n= 768.\\n'\n",
      " '5435 Figure A1: ˆGhospital graph obtained with Sentence-\\n'\n",
      " 'BERT (8 nodes/actions in total). Node labels correspond\\n'\n",
      " 'to the cluster ID along a representative utterance (the\\n'\n",
      " 'closest to the cluster centroid).\\n'\n",
      " 'Figure A2: ˆGhospital graph obtained with DSE (12\\n'\n",
      " 'nodes/actions in total). Node labels correspond to the\\n'\n",
      " 'cluster ID along a representative utterance (the closest\\n'\n",
      " 'to the cluster centroid).\\n'\n",
      " 'Appendix C.\\n'\n",
      " 'For the soft contrastive loss, the semantic\\n'\n",
      " 'similarity measure δ(yi,yj) = yi ·yj was\\n'\n",
      " 'computed using label embeddings y obtained\\n'\n",
      " 'with the best-performing pre-trained Sentence-\\n'\n",
      " 'BERT model on semantic search, namely the\\n'\n",
      " 'multi-qa-mpnet-base-dot-v1 model. As shown\\n'\n",
      " 'in Appendix C, we also experimented with the\\n'\n",
      " 'all-mpnet-base-v2 model, which has the best av-\\n'\n",
      " 'erage performance among all pre-trained Sentence-\\n'\n",
      " 'BERT models. The soft label temperature parame-\\n'\n",
      " 'ter was set to τ′= 0.35 after a preliminary study\\n'\n",
      " 'determined it to be a reasonable threshold for both\\n'\n",
      " 'joint and single training targets (Appendix E).\\n'\n",
      " 'In line with the settings of DSE and TOD-BERT,\\n'\n",
      " 'the learning rates for the contrastive head and the\\n'\n",
      " 'encoder model were set to 3e-4 and 3e-6, respec-\\n'\n",
      " 'tively. The contrastive temperature parameter τ\\n'\n",
      " 'DF2 Variation F 1 score ∆ Anisotropy (↑)\\n'\n",
      " 'D2F-Hardsingle 67.82 0.332\\n'\n",
      " '* DSE Backbone +2.66 +0.011\\n'\n",
      " '+ Self-Supervision -7.41 -0.002\\n'\n",
      " 'D2F-Hardjoint 66.22 0.230\\n'\n",
      " '* DSE Backbone +1.97 +0.010\\n'\n",
      " '+ Self-Supervision -6.01 -0.064\\n'\n",
      " 'D2Fsingle 70.89 0.597\\n'\n",
      " '* DSE Backbone +0.97 +0.012\\n'\n",
      " '* all-mpnet-base-v2 Label -0.60 -0.038\\n'\n",
      " '+ Self-Supervision -6.65 -0.189\\n'\n",
      " '– Contrastive Head -1.13 -0.047\\n'\n",
      " 'D2Fjoint 70.94 0.451\\n'\n",
      " '* DSE Backbone +0.65 +0.011\\n'\n",
      " '* all-mpnet-base-v2 Label -0.34 -0.038\\n'\n",
      " '+ Self-Supervision -8.06 -0.126\\n'\n",
      " '– Contrastive Head -3.78 -0.073\\n'\n",
      " 'Table A2: Ablation study results for various D2F con-\\n'\n",
      " 'figurations. Additions, subtractions, and replacements\\n'\n",
      " 'of components are marked with +, –, and * symbols,\\n'\n",
      " 'respectively. Values show the impact on 5-shot classifi-\\n'\n",
      " 'cation F1 score and anisotropy as reported in Table 2.\\n'\n",
      " 'was set to 0.05. Models were trained for 15 epochs\\n'\n",
      " 'and then saved for evaluation. The maximum se-\\n'\n",
      " 'quence length for the Transformer encoder was\\n'\n",
      " 'empirically set to 64 to accommodate at least 99%\\n'\n",
      " 'of the samples, as most TOD utterances are short.\\n'\n",
      " 'Finally, the batch size was set to 64 since we found\\n'\n",
      " 'that, contrary to typical self-supervised contrastive\\n'\n",
      " 'learning, larger batch sizes resulted in lower perfor-\\n'\n",
      " 'mance.16\\n'\n",
      " 'C Ablation study\\n'\n",
      " 'We conducted an ablation study to evaluate the ef-\\n'\n",
      " 'fects of different configurations on the performance\\n'\n",
      " 'of our D2F models. The following variations were\\n'\n",
      " 'tested:\\n'\n",
      " '• DSE Backbone: Replacing the original BERT\\n'\n",
      " 'encoder with the pre-trained DSE model.\\n'\n",
      " '• Label Encoder: Using the Sentence-BERT\\n'\n",
      " 'model all-mpnet-base-v2, which has the\\n'\n",
      " '16A grid search with batch sizes 64, 128, 256, and 512\\n'\n",
      " 'was performed, training models for one epoch and evaluating\\n'\n",
      " 'the similarity-based 5-shot F 1 score on our evaluation set.\\n'\n",
      " 'Larger batch sizes consistently yielded lower scores across\\n'\n",
      " 'all models (both standard and soft supervised contrastive loss\\n'\n",
      " 'models). For instance, DFDjoint scored 63.23, 61.64, 58.77,\\n'\n",
      " 'and 56.30 for batch sizes 64, 128, 256, and 512, respectively.\\n'\n",
      " '5436 Figure A3: Change in F1 score (top) and ∆ Anisotropy\\n'\n",
      " '(bottom) with respect to the label temperature τ′ (x-\\n'\n",
      " 'axis). The blue and orange curves represent D2Fsingle\\n'\n",
      " 'and D2Fjoint, respectively. Horizontal lines indicate\\n'\n",
      " 'the performance of their D2F-Hard counterparts using\\n'\n",
      " 'the standard hard supervised contrastive loss.\\n'\n",
      " 'best reported average performance for seman-\\n'\n",
      " 'tic similarity.\\n'\n",
      " '• Self-Supervision: Adding the self-supervised\\n'\n",
      " 'loss from DSE (Lself) trained jointly with our\\n'\n",
      " 'targets (L+ Lself) on the same data as DSE.\\n'\n",
      " 'This was done to evaluate whether jointly\\n'\n",
      " 'training as DSE would yield better perfor-\\n'\n",
      " 'mance than using the pre-trained DSE encoder\\n'\n",
      " 'directly as the backbone.\\n'\n",
      " '• Contrastive Head Removal: Removing the\\n'\n",
      " 'contrastive head used during training.\\n'\n",
      " 'The results of these variations are summarized in\\n'\n",
      " 'Table A2. The only configuration that consistently\\n'\n",
      " 'improved performance was the replacement of the\\n'\n",
      " 'backbone model with the pre-trained DSE model,\\n'\n",
      " 'increasing the F1 score and anisotropy across all\\n'\n",
      " 'variations.\\n'\n",
      " 'In contrast, adding self-supervision generally de-\\n'\n",
      " 'graded performance, indicating that the additional\\n'\n",
      " 'DSE self-supervised loss Lself may not comple-\\n'\n",
      " 'ment our targets effectively when trained jointly.\\n'\n",
      " 'Similarly, removing the contrastive head during\\n'\n",
      " 'training resulted in a notable performance drop,\\n'\n",
      " 'highlighting its importance.17\\n'\n",
      " '17Each different configuration required re-training the\\n'\n",
      " 'model for 15 epochs, a process that takes approximately 5\\n'\n",
      " 'days on a single GeForce RTX 3090 GPU.\\n'\n",
      " 'D Supervised Soft Contrastive Loss\\n'\n",
      " 'Explanation\\n'\n",
      " 'Let p(pos=j|xi) be the probability of j-th sample\\n'\n",
      " 'in the batch being positive given the i-th anchor.\\n'\n",
      " 'Then, the loss in Equation 1 is equivalent to the\\n'\n",
      " 'categorical cross-entropy of correctly classifying\\n'\n",
      " 'the positions in the batch with positive samples for\\n'\n",
      " 'the given xi anchor:\\n'\n",
      " '−\\n'\n",
      " 'N∑\\n'\n",
      " 'j=1\\n'\n",
      " 'p(pos=j|xi)log ˆp(pos=j|xi) (2)\\n'\n",
      " 'where the true/target distribution pis defined as\\n'\n",
      " 'p(pos=j|xi) =\\n'\n",
      " '{ 1\\n'\n",
      " '|Pi|, if yi = yj\\n'\n",
      " '0, if yi ̸= yj\\n'\n",
      " '(3)\\n'\n",
      " 'and the predicted distribution ˆp is an N-way\\n'\n",
      " 'softmax-based distribution proportional to the\\n'\n",
      " 'alignment/similarity between (the vectors of) the\\n'\n",
      " 'given xi anchor and each x+\\n'\n",
      " 'j sample:\\n'\n",
      " 'ˆp(pos=j|xi) = ezi·z+\\n'\n",
      " 'j /τ\\n'\n",
      " '∑N\\n'\n",
      " 'k=1 ezi·z+\\n'\n",
      " 'k/τ\\n'\n",
      " 'Note that the target distribution in Equation 3 treats\\n'\n",
      " 'all samples with different labels as equally negative,\\n'\n",
      " 'independently of the semantics of the labels. How-\\n'\n",
      " 'ever, we hypothesize that better representations can\\n'\n",
      " 'be obtained by taking advantage of the semantics\\n'\n",
      " 'of the labels to model more nuanced relationships.\\n'\n",
      " 'More precisely, let δ(yi,yj) be a semantic similar-\\n'\n",
      " 'ity measure between both labels, we define a new\\n'\n",
      " 'target distribution p(pos=j|xi) ∝δ(yi,yj) as:\\n'\n",
      " 'p(pos=j|xi) = eδ(yi,yj)/τ′\\n'\n",
      " '∑N\\n'\n",
      " 'k=1 eδ(yi,yk)/τ′ (4)\\n'\n",
      " 'where τ′ is the temperature parameter to con-\\n'\n",
      " 'trol how soft/hard the negative labels are (Ap-\\n'\n",
      " 'pendix E).18 Note that unlike Equation 3, 19 this\\n'\n",
      " 'equation allows searching for an encoder that tries\\n'\n",
      " 'to separate anchors and negatives by degrees pro-\\n'\n",
      " 'portional to how semantically similar their labels\\n'\n",
      " 'are. Therefore, by replacing Equation 4 in Equa-\\n'\n",
      " 'tion 2, our soft contrastive loss is finally defined\\n'\n",
      " 'as:\\n'\n",
      " '18On both extremes, sufficiently small τ′will resemble the\\n'\n",
      " 'original distribution in Equation 3 while sufficiently large τ′\\n'\n",
      " 'will resemble a uniform distribution leading to no contrast\\n'\n",
      " 'between positive and negative samples.\\n'\n",
      " '19Equation 3 encourages the encoder to separate all nega-\\n'\n",
      " 'tives 180◦away from their anchors: if yi ̸= yj, ˆp(pos= j|\\n'\n",
      " 'xi) →0 ⇒e(·) →0 ⇒zi ·z+\\n'\n",
      " 'j →−1.\\n'\n",
      " '5437 (a) Sentence-BERT\\n'\n",
      " ' (b) D2Fjoint\\n'\n",
      " 'Figure A4: Dendrograms obtained by hierarchically clustering all user '\n",
      " 'utterances in the hospital domain using\\n'\n",
      " 'Sentence-BERT embeddings (left) and D2Fjoint embeddings (right). The '\n",
      " 'clustering and the plots were obtained\\n'\n",
      " 'using the AgglomerativeClustering class from scikit-learn, with the number '\n",
      " 'of clusters set to 4 (indicated by\\n'\n",
      " 'different colors).\\n'\n",
      " 'ℓsoft\\n'\n",
      " 'i =−\\n'\n",
      " 'N∑\\n'\n",
      " 'j=1\\n'\n",
      " 'eδ(yi,yj)/τ′\\n'\n",
      " '∑N\\n'\n",
      " 'k=1 e\\n'\n",
      " 'δ(yi,yk)\\n'\n",
      " 'τ′\\n'\n",
      " 'log ezi·z+\\n'\n",
      " 'j /τ\\n'\n",
      " '∑N\\n'\n",
      " 'k=1 e\\n'\n",
      " 'zi·z+\\n'\n",
      " 'k\\n'\n",
      " 'τ\\n'\n",
      " 'E Soft Contrastive Loss Temperature\\n'\n",
      " 'To understand the benefits of the \"softness\" intro-\\n'\n",
      " 'duced by our proposed contrastive loss compared\\n'\n",
      " 'to the conventional hard supervised contrastive loss,\\n'\n",
      " 'we conducted a preliminary study examining the\\n'\n",
      " 'impact of the label temperature parameter τ′. We\\n'\n",
      " 'trained models over three epochs, varying the tem-\\n'\n",
      " 'perature τ′across a range of values from 0.05 to\\n'\n",
      " '1.0 in increments of 0.05. This resulted in 42 dif-\\n'\n",
      " 'ferent model variants: 20 each for D2F single and\\n'\n",
      " 'D2Fjoint, and one for each D2F-Hard counterpart.\\n'\n",
      " 'For each τ′value, we recorded the 5-shot classifi-\\n'\n",
      " 'cation F1 score and ∆ anisotropy values as outlined\\n'\n",
      " 'in Section 6. The results are depicted in Figure A3.\\n'\n",
      " 'The plots reveal that as the temperature τ′in-\\n'\n",
      " 'creases from 0, indicating a transition from hard\\n'\n",
      " 'to softer negative labels, both F 1 scores and ∆\\n'\n",
      " 'anisotropy values improve beyond those obtained\\n'\n",
      " 'with the standard supervised contrastive loss. For\\n'\n",
      " 'both D2Fsingle and D2Fjoint models, increasing\\n'\n",
      " 'the temperature leads to greater separation between\\n'\n",
      " 'intra-class and inter-class embeddings, as indicated\\n'\n",
      " 'by higher ∆ anisotropy values.\\n'\n",
      " 'The performance metrics exhibit a steady rise\\n'\n",
      " 'up to a temperature around between 0.35 and 0.4,\\n'\n",
      " 'beyond which ∆ anisotropy values begin to plateau\\n'\n",
      " 'and F1 scores become less stable. The advantage\\n'\n",
      " 'of using softer contrast is more pronounced for the\\n'\n",
      " 'joint target (D2Fjoint, represented by the orange\\n'\n",
      " 'line), as evidenced by the larger gap between the\\n'\n",
      " 'orange curve and its corresponding horizontal line\\n'\n",
      " '(D2F-Hardjoint).\\n'\n",
      " 'However, it’s important to note that these\\n'\n",
      " 'improvements diminish with additional training\\n'\n",
      " 'epochs. The final difference in performance met-\\n'\n",
      " 'rics between soft and hard labels narrows after\\n'\n",
      " 'extended training, as reflected in the results re-\\n'\n",
      " 'ported in Table 2, where models were trained for\\n'\n",
      " '15 epochs.\\n'\n",
      " 'F How Many Actions to Cluster?\\n'\n",
      " 'In practice, determining the optimal number of clus-\\n'\n",
      " 'ters (actions) in dialog flow extraction is challeng-\\n'\n",
      " 'ing because it directly affects the granularity of the\\n'\n",
      " 'extracted flows. Hierarchical clustering algorithms,\\n'\n",
      " 'such as agglomerative clustering, are preferred over\\n'\n",
      " 'centroid-based methods like k-means because they\\n'\n",
      " 'provide a visual representation of the data’s hierar-\\n'\n",
      " 'chical structure, which can be examined to decide\\n'\n",
      " 'the number of clusters or set a distance threshold.\\n'\n",
      " 'Figure A4 illustrates dendrograms obtained\\n'\n",
      " 'by hierarchically clustering user utterances in\\n'\n",
      " 'the hospital domain using Sentence-BERT\\n'\n",
      " 'embeddings and D2F joint embeddings. The\\n'\n",
      " 'clustering and plotting were performed us-\\n'\n",
      " 'ing the AgglomerativeClustering class from\\n'\n",
      " 'scikit-learn, with the number of clusters set to\\n'\n",
      " '4, represented by different colors.\\n'\n",
      " 'The dendrograms reveal notable differences be-\\n'\n",
      " 'tween the embeddings. The Sentence-BERT den-\\n'\n",
      " 'drogram (left) shows a structure with two main\\n'\n",
      " '(semantic) groups with low variability in the dis-\\n'\n",
      " 'tances between child and parent nodes, resulting\\n'\n",
      " 'in a more stretched plot. In contrast, the D2Fjoint\\n'\n",
      " 'dendrogram (right) displays a clearer separation\\n'\n",
      " '5438 into four main groups, with larger gaps between\\n'\n",
      " 'child and parent nodes at a certain level of the\\n'\n",
      " 'hierarchy, indicating distinct clusters. D2F joint\\n'\n",
      " 'embeddings were trained to minimize intra-action\\n'\n",
      " 'distances (pushing them towards the bottom of the\\n'\n",
      " 'dendrogram) and maximize inter-action distances\\n'\n",
      " '(pushing parent nodes towards the top) facilitating\\n'\n",
      " 'easier identification of clusters. For instance, in\\n'\n",
      " 'the D2Fjoint dendrogram, the number of actions\\n'\n",
      " 'could be estimated to be between 4 and 7, or a dis-\\n'\n",
      " 'tance threshold around 0.4 could be used to form\\n'\n",
      " 'the clusters.\\n'\n",
      " 'In our experiments (Section 6), we used the\\n'\n",
      " 'ground truth number of clusters from annotations\\n'\n",
      " 'to ensure consistency in evaluation across the dif-\\n'\n",
      " 'ferent embeddings. However, agglomerative clus-\\n'\n",
      " 'tering was employed to mimic closer a realistic\\n'\n",
      " 'scenario where the number of actions is not prede-\\n'\n",
      " 'fined.\\n'\n",
      " 'Thus, hierarchical clustering methods provide a\\n'\n",
      " 'practical approach for approximating the number of\\n'\n",
      " 'actions in practice when such number is unknown.\\n'\n",
      " 'G Deriving Action Labels from Clusters\\n'\n",
      " 'In practice, as illustrated in Figures A1, A2, and 4,\\n'\n",
      " 'actions are identified by cluster IDs after clustering.\\n'\n",
      " 'However, for certain tasks, such as manual analysis\\n'\n",
      " 'of the extracted dialogue flow, a descriptive action\\n'\n",
      " 'name representing the cluster may be necessary.\\n'\n",
      " 'Following a prompt-based approach similar to that\\n'\n",
      " 'of Sreedhar et al. (2024) for creating weak intent\\n'\n",
      " 'labels, we can leverage instruction-tuned LLMs to\\n'\n",
      " 'assign representative labels to each cluster based on\\n'\n",
      " 'its constituent utterances. For instance, using the\\n'\n",
      " 'latest OpenAI GPT-4 model (gpt-4o) with the fol-\\n'\n",
      " 'lowing prompt, where \"<CLUSTER_UTTERANCES>\"\\n'\n",
      " 'is replaced with the utterances of a given cluster:\\n'\n",
      " '[ { \" r o l e \" : \" system \" ,\\n'\n",
      " '\" c o n t e n t \" : \" \" \" Your t a s k i s t o a n n o t a t e c o n v e r s '\n",
      " 'a t i o n a l\\n'\n",
      " 'u t t e r a n c e s w i t h t h e i n t e n t e x p r e s s e d a s c a n o '\n",
      " 'n i c a l\\n'\n",
      " 'forms . A c a n o n i c a l form i s a s h o r t summary\\n'\n",
      " 'r e p r e s e n t i n g t h e i n t e n t o f a s e t o f u t t e r a n c e '\n",
      " 's − i t i s\\n'\n",
      " 'n e i t h e r t o o v e r b o s e nor t o o s h o r t .\\n'\n",
      " 'Be aware t h a t r e q u i r e d c a n o n i c a l forms s h o u l d a v o i '\n",
      " 'd\\n'\n",
      " 'c o n t a i n i n g s p e c i f i c names o r q u a n t i t i e s , o n l y '\n",
      " 'r e p r e s e n t\\n'\n",
      " 't h e i n t e n t i n a b s t r a c t t e r m s .\\n'\n",
      " 'For example , f o r :\\n'\n",
      " 'For t h e f o l l o w i n g u t t e r a n c e s :\\n'\n",
      " \"1 . Uh yes i ' m l o o k i n g f o r a p l a c e f o r e n t e r t a i n m e \"\n",
      " 'n t t h a t\\n'\n",
      " 'i s i n t h e c e n t e r o f t h e c i t y\\n'\n",
      " '2 . i would l i k e t o know where a p l a c e f o r e n t e r t a i n m e n '\n",
      " 't\\n'\n",
      " 't h a t i s n o t f a r away from my l o c a t i o n\\n'\n",
      " 'C a n o n i c a l form i s : \" r e q u e s t e n t e r t a i n m e n t p l a '\n",
      " 'c e and i n f o r m\\n'\n",
      " 'l o c a t i o n \"\\n'\n",
      " 'For t h e f o l l o w i n g u t t e r a n c e s :\\n'\n",
      " '1 . Okay so t h e phone number i s a 1223217297\\n'\n",
      " '2 . Sure , my phone number i s f o u r f o u r f i v e f i v e\\n'\n",
      " '3 . 2 3 4 5 6 i s h e r phone number\\n'\n",
      " 'C a n o n i c a l form i s : \" i n f o r m phone number \"\\n'\n",
      " 'For t h e f o l l o w i n g u t t e r a n c e s :\\n'\n",
      " '1 . 8 4 0\\n'\n",
      " 'Figure A5: ˆGhospital graph from Figure 4 with cluster\\n'\n",
      " 'labels generated with ChatGPT.\\n'\n",
      " '2 . yes f i v e f i v e t h r e e\\n'\n",
      " 'C a n o n i c a l form i s : \" i n f o r m number \"\\n'\n",
      " '\" \" \" } ,\\n'\n",
      " '{ \" r o l e \" : \" u s e r \" , \" c o n t e n t \" : \" \" \" Give t h e f o l l o '\n",
      " 'w i n g l i s t o f\\n'\n",
      " 'u t t e r a n c e p r o v i d e a s i n g l e c a n o n i c a l name t h a '\n",
      " 't\\n'\n",
      " 'r e p r e s e n t a l l o f them :\\n'\n",
      " '<CLUSTER_UTTERANCES} >\"\"\" } ,\\n'\n",
      " '{ \" r o l e \" : \" a s s i s t a n t \" , \" c o n t e n t \" : \\' The c a n o n '\n",
      " 'i c a l name t h a t\\n'\n",
      " 'r e p r e s e n t t h e above u t t e r a n c e s i s : \" \\' } ]\\n'\n",
      " 'Replacing the node labels in Figure 4 with those\\n'\n",
      " 'generated by the above prompt yields a more in-\\n'\n",
      " 'terpretable version of the graph, as shown in Fig-\\n'\n",
      " 'ure A5.\\n'\n",
      " '5439 Original Standardized Parent\\n'\n",
      " 'inform inform (slots)\\n'\n",
      " 'inform\\n'\n",
      " 'notify_fail\\n'\n",
      " 'inform_failure\\n'\n",
      " 'notify_failure\\n'\n",
      " 'no_result\\n'\n",
      " 'nobook\\n'\n",
      " 'nooffer\\n'\n",
      " 'sorry\\n'\n",
      " 'cant_understand\\n'\n",
      " 'canthelp\\n'\n",
      " 'reject\\n'\n",
      " 'book\\n'\n",
      " 'inform_successofferbooked\\n'\n",
      " 'notify_success\\n'\n",
      " 'request request (slots)\\n'\n",
      " 'request\\n'\n",
      " 'request_alt request_alternative\\n'\n",
      " 'request_compare request_compare\\n'\n",
      " 'request_update request_update\\n'\n",
      " 'req_more\\n'\n",
      " 'request_morerequest_more\\n'\n",
      " 'moreinfo\\n'\n",
      " 'hearmore\\n'\n",
      " 'confirm confirm (slots)\\n'\n",
      " 'confirmation confirm_answerconfirm_answer\\n'\n",
      " 'confirm_question confirm_question\\n'\n",
      " 'affirm agreement agreementaffirm_intent\\n'\n",
      " 'negate\\n'\n",
      " 'disagreement disagreement negate_intent\\n'\n",
      " 'deny\\n'\n",
      " 'offer\\n'\n",
      " 'offer offerselect\\n'\n",
      " 'multiple_choice\\n'\n",
      " 'offerbook\\n'\n",
      " 'suggest recommendation recommendationrecommend\\n'\n",
      " 'greeting greeting greetingwelcome\\n'\n",
      " 'thank_you\\n'\n",
      " 'thank_you thank_you thanks\\n'\n",
      " 'thankyou\\n'\n",
      " 'good_bye\\n'\n",
      " 'good_bye good_bye goodbye\\n'\n",
      " 'closing\\n'\n",
      " 'Table A3: The original 44 dialog acts with their respective 18 standardized '\n",
      " 'names used to unify all the datasets,\\n'\n",
      " 'along with a parent category grouping them further into 10 parent acts.\\n'\n",
      " '5440')\n"
     ]
    }
   ],
   "source": [
    "# read csv\n",
    "data = pd.read_csv(f'./data/EMNLP/emnlp_{year}_main.csv')\n",
    "pprint.pp(data.iloc[0]['text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
