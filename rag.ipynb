{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9596a54-3810-4b18-ab2d-7dde9cf08e69",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from langchain_community.document_loaders.dataframe import DataFrameLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain import hub\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, ChatHuggingFace, HuggingFacePipeline\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain.chains.history_aware_retriever import create_history_aware_retriever\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,  BitsAndBytesConfig, TextStreamer\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f00408f4-e503-485e-92b3-8b674a11114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings\n",
    "model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    #model_kwargs=model_kwargs,\n",
    "    #encode_kwargs=encode_kwargs,\n",
    "    cache_folder = './hf'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf387d01-c358-474b-a72f-c657643940d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = FAISS.load_local(\"faiss_index\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ab62286-808b-40d6-bf15-405eab2143d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rephrase prompt:  input_variables=['chat_history', 'input'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'langchain-ai', 'lc_hub_repo': 'chat-langchain-rephrase', 'lc_hub_commit_hash': 'fb7ddb56be11b2ab10d176174dae36faa2a9a6ba13187c8b2b98315f6ca7d136'} template='Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\\n\\nChat History:\\n{chat_history}\\nFollow Up Input: {input}\\nStandalone Question:'\n",
      "\n",
      "\n",
      "Retrieval QA prompt:  input_variables=['context', 'input'] optional_variables=['chat_history'] input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7f8baf711990>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]} partial_variables={'chat_history': []} metadata={'lc_hub_owner': 'langchain-ai', 'lc_hub_repo': 'retrieval-qa-chat', 'lc_hub_commit_hash': 'b60afb6297176b022244feb83066e10ecadcda7b90423654c4a9d45e7a73cebc'} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='Answer any use questions based solely on the context below:\\n\\n<context>\\n{context}\\n</context>'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "rephrase_prompt = hub.pull(\"langchain-ai/chat-langchain-rephrase\", api_key=os.environ['LANGSMITH_API_KEY'], api_url=\"https://api.smith.langchain.com/\")\n",
    "retrieval_qa_chat_prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\", api_key=os.environ['LANGSMITH_API_KEY'], api_url=\"https://api.smith.langchain.com/\")\n",
    "print(\"Rephrase prompt: \", rephrase_prompt)\n",
    "print(\"\\n\")\n",
    "print(\"Retrieval QA prompt: \", retrieval_qa_chat_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6741d91-5dc8-458f-90fa-02bfd97513e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887c26f0a2eb4b93a1b9d6d64c79b3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:1\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=config, device_map=\"auto\", token=os.environ['hf_token'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=os.environ['hf_token'])\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "pipe = transformers.pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200, streamer=streamer)\n",
    "\n",
    "llm = HuggingFacePipeline(model_id=model_name, pipeline=pipe)\n",
    "llm = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38fe2331-714b-44df-a4c0-abfcaa7928f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Chain\n",
    "# Create Document Stuff Chain\n",
    "stuff_documents_chain = create_stuff_documents_chain(llm, retrieval_qa_chat_prompt)\n",
    "# Create History aware Chain\n",
    "history_aware_retriever = create_history_aware_retriever(llm=llm, retriever=retriever, prompt=rephrase_prompt)\n",
    "# Create Retrieval Chain\n",
    "qa = create_retrieval_chain(retriever=history_aware_retriever, combine_docs_chain=stuff_documents_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7400fd1f-f7f9-465f-98e0-9a12798f667a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain-of-Thought (CoT) is a problem-solving approach where a large language model is prompted to generate its answer following a step-by-step explanation. This approach encourages the model to articulate a reasoning process leading to the final answer, supported by in-context demonstrations. It involves decomposing intricate problems into manageable steps, simplifying the overall reasoning process, and creating a linkage among the reasoning steps to ensure no important conditions are overlooked.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Chain of Thought?\"\n",
    "chat_history = []\n",
    "\n",
    "res = qa.invoke(input={'input': query, 'chat_history': chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c184e580-3141-4476-beae-28e02e6b0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(response):\n",
    "    parts = response.split(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
    "    return parts[-1].strip() if len(parts) > 1 else response.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34900f4d-2579-42e9-8c6c-87ee0442928d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is Chain of Thought?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Chain-of-Thought (CoT) is a problem-solving approach where a large language model is prompted to generate its answer following a step-by-step explanation. This approach encourages the model to articulate a reasoning process leading to the final answer, supported by in-context demonstrations. It involves decomposing intricate problems into manageable steps, simplifying the overall reasoning process, and creating a linkage among the reasoning steps to ensure no important conditions are overlooked.', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history.append(HumanMessage(content=query)) \n",
    "chat_history.append(AIMessage(content=extract_answer(res['answer']))) \n",
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ad231e1-6c43-4071-a1e0-6816b9538d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the benefits of Chain of Thought (CoT)?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, the benefits of Chain-of-Thought (CoT) prompting include:\n",
      "\n",
      "1. Dramatically improved performance on reasoning-heavy tasks.\n",
      "2. Ability to elicit superior reasoning abilities in a few-shot setting.\n",
      "3. Capable of performing well in zero-shot setting with minimal modifications to the prompt.\n",
      "4. Enables the model to articulate a step-by-step reasoning process, providing a clear explanation for the predicted answer.\n",
      "5. Improves the interpretability of the model's reasoning process.\n",
      "\n",
      "However, the text also mentions that the intermediate steps in traditional CoT prompting hardly provide meaningful information due to deficiencies in prompt design.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the benefits?\"\n",
    "res = qa.invoke(input={'input': query, 'chat_history': chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35bb7966-e76a-415d-831c-59369898ba08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What was the topic of the previous conversation?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You asked \"What is Chain of Thought?\"\n"
     ]
    }
   ],
   "source": [
    "query = \"What did i ask you before?\"\n",
    "res = qa.invoke(input={'input': query, 'chat_history': chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9048a2dd-5aa7-4b23-811d-7037a4f55957",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the difference between Chain of Thought and Tree of Thought approaches in problem-solving?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree-of-Thought (ToT) is an approach that enables large language models to explore multiple reasoning pathways through a decision-making tree structure. This structure incorporates backtracking via a search algorithm to identify the globally optimal reasoning path. It allows the model to explore different possibilities and select the best thinking path to arrive at the solution.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is Tree of Thought?\"\n",
    "res = qa.invoke(input={'input': query, 'chat_history': chat_history})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
